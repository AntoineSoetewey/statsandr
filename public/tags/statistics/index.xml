<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Stats and R</title>
    <link>/tags/statistics/</link>
    <description>Recent content in Statistics on Stats and R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 19 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to do a t-test or ANOVA for more than one variable at once in R and communicate the results in a better way</title>
      <link>/blog/how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way/</link>
      <pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#perform-multiple-tests-at-once&#34;&gt;Perform multiple tests at once&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concise-and-easily-interpretable-results&#34;&gt;Concise and easily interpretable results&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#t-test&#34;&gt;T-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#anova&#34;&gt;ANOVA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#to-go-even-further&#34;&gt;To go even further&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/How%20to%20do%20a%20t-test%20or%20ANOVA%20for%20many%20variables%20at%20once%20in%20R%20and%20communicate%20the%20results%20in%20a%20better%20way.jpeg&#34; alt=&#34;Photo by Teemu Paananen&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Teemu Paananen&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;As part of my teaching assistant position in a Belgian university, students often ask me for some help in their statistical analyses for their master’s thesis.&lt;/p&gt;
&lt;p&gt;A frequent question is how to compare groups of patients in terms of several &lt;a href=&#34;/blog/variable-types-and-examples/#continuous&#34;&gt;quantitative continuous&lt;/a&gt; variables. Most of us know that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To compare two groups, a &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/&#34;&gt;Student’s t-test&lt;/a&gt; should be used&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;To compare three groups or more, an ANOVA should be performed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two tests are quite basic and have been extensively documented online and in statistical textbooks so the difficulty is not in how to perform these tests.&lt;/p&gt;
&lt;p&gt;In the past, I used to do the analyses by following these 3 steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Draw boxplots illustrating the distributions by group (with the &lt;code&gt;boxplot()&lt;/code&gt; function or thanks to the &lt;a href=&#34;/blog/rstudio-addins-or-how-to-make-your-coding-life-easier/#esquisse&#34;&gt;&lt;code&gt;{esquisse}&lt;/code&gt; R Studio addin&lt;/a&gt; if I wanted to use the &lt;code&gt;{ggplot2}&lt;/code&gt; package)&lt;/li&gt;
&lt;li&gt;Perform a t-test or an ANOVA depending on the number of groups to compare (with the &lt;code&gt;t.test()&lt;/code&gt; and &lt;code&gt;oneway.test()&lt;/code&gt; functions for t-test and ANOVA, respectively)&lt;/li&gt;
&lt;li&gt;Repeat steps 1 and 2 for each variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This was feasible as long as there were only a couple of variables to test. Nonetheless, most students came to me asking to perform these kind of tests not on one or two variables, but on &lt;strong&gt;multiples&lt;/strong&gt; variables. So when there were more than one variable to test, I quickly realized that I was wasting my time and that there must be a more efficient way to do the job.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;: you must be very careful with the issue of multiple testing (also referred as multiplicity) which can arise when you perform multiple t-tests. In short, when a large number of statistical tests are performed, some will have p-values less than 0.05 purely by chance, even if all null hypotheses are in fact really true. This is known as multiplicity or multiple testing. You can tackle this problem by using the Bonferroni correction, among others. The Bonferroni correction is a simple method that allows many t-tests to be made while still assuring an overall confidence level is maintained. For this, instead of using the standard threshold of &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 5\)&lt;/span&gt;% for the significance level, you can use &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{0.05}{m}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of t-tests. For example, if you perform 20 t-tests with a desired &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;, the Bonferroni correction implies that you would reject the null hypothesis for each individual test when the p-value is smaller than &lt;span class=&#34;math inline&#34;&gt;\(\alpha = \frac{0.05}{20} = 0.0025\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;perform-multiple-tests-at-once&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Perform multiple tests at once&lt;/h1&gt;
&lt;p&gt;I thus wrote a piece of code that automated the process, by drawing boxplots and performing the tests on several variables at once. Below is the code I used, illustrating the process with the &lt;code&gt;iris&lt;/code&gt; dataset. The &lt;code&gt;Species&lt;/code&gt; variable has 3 levels, so let’s remove one, and then draw a boxplot and apply a t-test on all 4 continuous variables at once. Note that the continuous variables that we would like to test are variables 1 to 4 in the &lt;code&gt;iris&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- iris

# remove one level to have only two groups
dat &amp;lt;- subset(dat, Species != &amp;quot;setosa&amp;quot;)
dat$Species &amp;lt;- factor(dat$Species)

# boxplots and t-tests for the 4 variables at once
for (i in 1:4) { # variables to compare are variables 1 to 4
  boxplot(dat[, i] ~ dat$Species, # draw boxplots by group
    ylab = names(dat[i]), # rename y-axis with variable&amp;#39;s name
    xlab = &amp;quot;Species&amp;quot;
  )
  print(t.test(dat[, i] ~ dat$Species)) # print results of t-test
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  dat[, i] by dat$Species
## t = -5.6292, df = 94.025, p-value = 1.866e-07
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.8819731 -0.4220269
## sample estimates:
## mean in group versicolor  mean in group virginica 
##                    5.936                    6.588&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  dat[, i] by dat$Species
## t = -3.2058, df = 97.927, p-value = 0.001819
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.33028364 -0.07771636
## sample estimates:
## mean in group versicolor  mean in group virginica 
##                    2.770                    2.974&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  dat[, i] by dat$Species
## t = -12.604, df = 95.57, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1.49549 -1.08851
## sample estimates:
## mean in group versicolor  mean in group virginica 
##                    4.260                    5.552&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  dat[, i] by dat$Species
## t = -14.625, df = 89.043, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.7951002 -0.6048998
## sample estimates:
## mean in group versicolor  mean in group virginica 
##                    1.326                    2.026&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the above piece of code draws a boxplot and then prints results of the test for each continuous variable, all at once.&lt;/p&gt;
&lt;p&gt;At some point in the past, I even wrote code to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;draw a boxplot&lt;/li&gt;
&lt;li&gt;test for the equality of variances (thanks to the Levene’s test)&lt;/li&gt;
&lt;li&gt;depending on whether the variances were equal or unequal, the appropriate test was applied: the Welch test if the variances were unequal and the Student’s t-test in the case the variances were equal (see more details about the different versions of the &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/&#34;&gt;t-test for two samples&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;apply steps 1 to 3 for all continuous variables at once&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I had a similar code for ANOVA in case I needed to compare more than two groups.&lt;/p&gt;
&lt;p&gt;The code was doing the job relatively well. Indeed, thanks to this code I was able to test several variables in an automated way in the sense that it compared groups for all variables at once.&lt;/p&gt;
&lt;p&gt;The only thing I had to change from one project to another is that I needed to modify the name of the grouping variable and the numbering of the continuous variables to test (&lt;code&gt;Species&lt;/code&gt; and &lt;code&gt;1:4&lt;/code&gt; in the above code).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;concise-and-easily-interpretable-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concise and easily interpretable results&lt;/h1&gt;
&lt;div id=&#34;t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;T-test&lt;/h2&gt;
&lt;p&gt;Although it was working quite well and applicable to different projects with only minor changes, I was still unsatisfied with another point.&lt;/p&gt;
&lt;p&gt;Someone who is proficient in statistics and R can read and interpret the output of a t-test without any difficulty. However, as you may have noticed with your own statistical projects, most people do not know what to look for in the results and are sometimes a bit confused when they see so many graphs, code, output, results and numeric values in a document. They are quite easily overwhelmed by this mass of information.&lt;/p&gt;
&lt;p&gt;With my old R routine, the time I was saving by automating the process of t-tests and ANOVA was (partially) lost when I had to explain R outputs to my students so that they could interpret the results correctly. Although most of the time it simply boiled down to pointing out what to look for in the outputs (i.e., &lt;em&gt;p&lt;/em&gt;-values), I was still losing quite a lot of time because these outputs were, in my opinion, too detailed for most real-life applications. In other words, too much information seemed to be confusing for many people so I was still not convinced that it was the most optimal way to share statistical results to nonscientists.&lt;/p&gt;
&lt;p&gt;Of course, they came to me for statistical advices, so they expected to have these results and I needed to give them answers to their questions and hypotheses. Nonetheless, I wanted to find a better way to communicate these results to this type of audience, with the minimum of information required to arrive at a conclusion. No more and no less than that.&lt;/p&gt;
&lt;p&gt;After a long time spent online trying to figure out a way to present results in a more concise and readable way, I discovered the &lt;a href=&#34;https://cran.r-project.org/web/packages/ggpubr/index.html&#34;&gt;&lt;code&gt;{ggpubr}&lt;/code&gt; package&lt;/a&gt;. This package allows to indicate the test used and the &lt;em&gt;p&lt;/em&gt;-value of the test directly on a ggplot2-based graph. It also facilitates the creation of publication-ready plots for non-advanced statistical audiences.&lt;/p&gt;
&lt;p&gt;After many refinements and modifications of the initial code (available in this &lt;a href=&#34;http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/76-add-p-values-and-significance-levels-to-ggplots/&#34;&gt;article&lt;/a&gt;), I finally came up with a rather stable and robust process to perform t-tests and ANOVA for more than one variable at once, and more importantly, make the results concise and easily readable by anyone (statisticians or not).&lt;/p&gt;
&lt;p&gt;A graph is worth a thousand words, so here are the exact same tests than in the previous section, but this time with my new R routine:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggpubr)

# Edit from here #
x &amp;lt;- which(names(dat) == &amp;quot;Species&amp;quot;) # name of grouping variable
y &amp;lt;- which(names(dat) == &amp;quot;Sepal.Length&amp;quot; # names of variables to test
| names(dat) == &amp;quot;Sepal.Width&amp;quot;
| names(dat) == &amp;quot;Petal.Length&amp;quot;
| names(dat) == &amp;quot;Petal.Width&amp;quot;)
method &amp;lt;- &amp;quot;t.test&amp;quot; # one of &amp;quot;wilcox.test&amp;quot; or &amp;quot;t.test&amp;quot;
paired &amp;lt;- FALSE # if paired make sure that in the dataframe you have first all individuals at T1, then all individuals again at T2
# Edit until here


# Edit at your own risk
for (i in y) {
  for (j in x) {
    ifelse(paired == TRUE,
      p &amp;lt;- ggpaired(dat,
        x = colnames(dat[j]), y = colnames(dat[i]),
        color = colnames(dat[j]), line.color = &amp;quot;gray&amp;quot;, line.size = 0.4,
        palette = &amp;quot;npg&amp;quot;,
        legend = &amp;quot;none&amp;quot;,
        xlab = colnames(dat[j]),
        ylab = colnames(dat[i]),
        add = &amp;quot;jitter&amp;quot;
      ),
      p &amp;lt;- ggboxplot(dat,
        x = colnames(dat[j]), y = colnames(dat[i]),
        color = colnames(dat[j]),
        palette = &amp;quot;npg&amp;quot;,
        legend = &amp;quot;none&amp;quot;,
        add = &amp;quot;jitter&amp;quot;
      )
    )
    #  Add p-value
    print(p + stat_compare_means(aes(label = paste0(..method.., &amp;quot;, p-value = &amp;quot;, ..p.format.., &amp;quot; (&amp;quot;, ifelse(..p.adj.. &amp;gt;= 0.05, &amp;quot;not significant&amp;quot;, ..p.signif..), &amp;quot;)&amp;quot;)),
      method = method,
      paired = paired,
      # group.by = NULL,
      ref.group = NULL
    ))
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see from the graphs above, only the most important information is presented for each variable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a visual comparison of the groups thanks to boxplots&lt;/li&gt;
&lt;li&gt;the name of the statistical test&lt;/li&gt;
&lt;li&gt;the &lt;em&gt;p&lt;/em&gt;-value of the test&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on these graphs, it is easy, even for non-experts, to interpret the results and conclude that the &lt;code&gt;versicolor&lt;/code&gt; and &lt;code&gt;virginica&lt;/code&gt; species are significantly different in terms of all 4 variables (since all &lt;em&gt;p&lt;/em&gt;-values &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt; \frac{0.05}{4} = 0.0125\)&lt;/span&gt; (remind that the Bonferroni correction is applied to avoid the issue of multiple testing, so we divide the usual &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; level by 4 because there are 4 t-tests)).&lt;/p&gt;
&lt;p&gt;Of course, experts may be interested in more advanced results. However, this simple yet complete graph, which includes the name of the test and the &lt;em&gt;p&lt;/em&gt;-value, gives all the necessary information to answer the question: “Are the groups different?”.&lt;/p&gt;
&lt;p&gt;In my experience, I have noticed that students and professionals (especially those from a less scientific background) understand way better these results than the ones presented in the previous section.&lt;/p&gt;
&lt;p&gt;The only lines of code that need to be modified for your own project is the name of the grouping variable (&lt;code&gt;Species&lt;/code&gt; in the above code), the names of the variables you want to test (&lt;code&gt;Sepal.Length&lt;/code&gt;, &lt;code&gt;Sepal.Width&lt;/code&gt;, etc.),&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; whether you want to apply a t-test (&lt;code&gt;t.test&lt;/code&gt;) or Wilcoxon test (&lt;code&gt;wilcox.test&lt;/code&gt;) and whether the samples are paired or not (&lt;code&gt;FALSE&lt;/code&gt; if samples are independent, &lt;code&gt;TRUE&lt;/code&gt; if they are paired).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anova&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ANOVA&lt;/h2&gt;
&lt;p&gt;Below the same process with an ANOVA. Note that we reload the dataset &lt;code&gt;iris&lt;/code&gt; to include all three &lt;code&gt;Species&lt;/code&gt; this time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- iris

# Edit from here
x &amp;lt;- which(names(dat) == &amp;quot;Species&amp;quot;) # name of grouping variable
y &amp;lt;- which(names(dat) == &amp;quot;Sepal.Length&amp;quot; # names of variables to test
| names(dat) == &amp;quot;Sepal.Width&amp;quot;
| names(dat) == &amp;quot;Petal.Length&amp;quot;
| names(dat) == &amp;quot;Petal.Width&amp;quot;)
method1 &amp;lt;- &amp;quot;anova&amp;quot; # one of &amp;quot;anova&amp;quot; or &amp;quot;kruskal.test&amp;quot;
method2 &amp;lt;- &amp;quot;t.test&amp;quot; # one of &amp;quot;wilcox.test&amp;quot; or &amp;quot;t.test&amp;quot;
my_comparisons &amp;lt;- list(c(&amp;quot;setosa&amp;quot;, &amp;quot;versicolor&amp;quot;), c(&amp;quot;setosa&amp;quot;, &amp;quot;virginica&amp;quot;), c(&amp;quot;versicolor&amp;quot;, &amp;quot;virginica&amp;quot;)) # comparisons for post-hoc tests
# Edit until here


# Edit at your own risk
for (i in y) {
  for (j in x) {
    p &amp;lt;- ggboxplot(dat,
      x = colnames(dat[j]), y = colnames(dat[i]),
      color = colnames(dat[j]),
      legend = &amp;quot;none&amp;quot;,
      palette = &amp;quot;npg&amp;quot;,
      add = &amp;quot;jitter&amp;quot;
    )
    print(
      p + stat_compare_means(aes(label = paste0(..method.., &amp;quot;, p-value = &amp;quot;, ..p.format.., &amp;quot; (&amp;quot;, ifelse(..p.adj.. &amp;gt; 0.05, &amp;quot;not significant&amp;quot;, ..p.signif..), &amp;quot;)&amp;quot;)),
        method = method1, label.y = max(dat[, i], na.rm = TRUE)
      )
      + stat_compare_means(comparisons = my_comparisons, method = method2, label = &amp;quot;p.format&amp;quot;) # remove if p-value of ANOVA or Kruskal-Wallis test &amp;gt;= 0.05
    )
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/blog/2020-03-19-how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way_files/figure-html/unnamed-chunk-3-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Like the improved routine for the t-test, I have noticed that students and non-expert professionals understand ANOVA results presented this way much more easily compared to the default R outputs.&lt;/p&gt;
&lt;p&gt;With one graph for each variable, it is easy to see that all species are different from each other in terms of all 4 variables.&lt;/p&gt;
&lt;p&gt;If you want to apply the same automated process to your data, you will need to modify the name of the grouping variable (&lt;code&gt;Species&lt;/code&gt;), the names of the variables you want to test (&lt;code&gt;Sepal.Length&lt;/code&gt;, etc.), whether you want to perform an ANOVA (&lt;code&gt;anova&lt;/code&gt;) or Kruskal-Wallis test (&lt;code&gt;kruskal.test&lt;/code&gt;) and finally specify the comparisons for the post-hoc tests.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;to-go-even-further&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;To go even further&lt;/h1&gt;
&lt;p&gt;As we have seen, these two improved R routines allow to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Perform t-tests and ANOVA on a small or large number of variables with only minor changes to the code. I basically only have to replace the variable names and the name of the test I want to use. It takes almost the same time to test one or several variables so it is quite an improvement compared to testing one variable at a time&lt;/li&gt;
&lt;li&gt;Share test results in a much proper and cleaner way. This is possible thanks to a graph showing the observations by group and the &lt;em&gt;p&lt;/em&gt;-value of the appropriate test included on this graph. This is particularily important when communicating results to a wider audience or to people from diverse backgrounds.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, like most of my R routines, these two pieces of code are still a work in progress. Below are some additional features I have been thinking of and which could be added in the future to make the process of comparing two or more groups even more optimal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add the possibility to select variables by their numbering in the dataframe. For the moment it is only possible to do it via their names. This will allow to automate the process even further because instead of typing all variable names one by one, we could simply type &lt;code&gt;4:100&lt;/code&gt; (to test variables 4 to 100 for instance).&lt;/li&gt;
&lt;li&gt;When comparing more than two groups, it is only possible to apply an ANOVA or Kruskal-Wallis test at the moment. A major improvement would be to add the possibility to perform a repeated measures ANOVA (i.e., an ANOVA when the samples are dependent). It is currently already possible to do a t-test with two paired samples, but it is not yet possible to do the same with more than two groups.&lt;/li&gt;
&lt;li&gt;Another less important (yet still nice) feature when comparing more than 2 groups would be to automatically apply post-hoc tests only in the case where the null hypothesis of the ANOVA or Kruskal-Wallis test is rejected (so when there is at least one group different from the others, because if the null hypothesis of equal groups is not rejected we do not apply a post-hoc test). At the present time, I manually add or remove the code that displays the &lt;em&gt;p&lt;/em&gt;-values of post-hoc tests depending on the global &lt;em&gt;p&lt;/em&gt;-value of the ANOVA or Kruskal-Wallis test.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will try to add these features in the future, or I would be glad to help if the author of the {ggpubr} package needs help in including these features (I hope he will see this article!).&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article will help you to perform t-tests and ANOVA for multiple variables at once and make the results more easily readable and interpretable by nonscientists. Learn more about the t-test and how to compare two samples in &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/&#34;&gt;this article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In theory, an ANOVA can also be used to compare two groups as it will give the same results compared to a Student’s t-test, but in practice we use the Student’s t-test to compare two samples and the ANOVA to compare three samples or more.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Do not forget to separate the variables you want to test with &lt;code&gt;|&lt;/code&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Post-hoc test is only the name used to refer to a specific type of statistical tests. Post-hoc test includes, among others, the Tukey HSD test, the Bonferroni correction, Dunnett’s test. Even if an ANOVA or a Kruskal-Wallis test can determine whether there is at least one group that is different from the others, it does not allow us to conclude &lt;strong&gt;which&lt;/strong&gt; are different from each other. For this purpose, there are post-hoc tests that compare all groups two by two to determine which ones are different, after adjusting for multiple comparisons. Concretely, post-hoc tests are performed to each possible pair of groups &lt;strong&gt;after&lt;/strong&gt; an ANOVA or a Kruskal-Wallis test has shown that there is at least one group which is different (hence “post” in the name of this type of test). The null and alternative hypotheses and the interpretations of these tests are similar to a Student’s t-test for two samples.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to perform a one sample t-test by hand and in R: test on one mean</title>
      <link>/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r-test-on-one-mean/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r-test-on-one-mean/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#null-and-alternative-hypothesis&#34;&gt;Null and alternative hypothesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypothesis-testing&#34;&gt;Hypothesis testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-versions-of-the-one-sample-t-test&#34;&gt;Two versions of the one sample t-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-compute-the-one-sample-t-test-by-hand&#34;&gt;How to compute the one sample t-test by hand?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-1-variance-of-the-population-is-known&#34;&gt;Scenario 1: variance of the population is known&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-2-variance-of-the-population-is-unknown&#34;&gt;Scenario 2: variance of the population is unknown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#different-underlying-distributions-for-the-critical-value&#34;&gt;Different underlying distributions for the critical value&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-compute-the-one-sample-t-test-in-r&#34;&gt;How to compute the one sample t-test in R?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-1-variance-of-the-population-is-known-1&#34;&gt;Scenario 1: variance of the population is known&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-2-variance-of-the-population-is-unknown-1&#34;&gt;Scenario 2: variance of the population is unknown&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#confidence-interval&#34;&gt;Confidence interval&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions&#34;&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r_files/how-to-perform-a-one-sample-t-test-by-hand-and-in-r.jpeg&#34; alt=&#34;Photo by Helloquence&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Helloquence&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;After having written an article on the &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/&#34;&gt;Student’s t-test for two samples&lt;/a&gt; (independent and paired samples), I believe it is time to explain in details how to perform one sample t-tests by hand and in R.&lt;/p&gt;
&lt;p&gt;One sample t-test is an important part of inferential statistics (probably one of the first statistical test that students learn). Remind that, unlike &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;descriptive statistics&lt;/a&gt;, inferential statistics is a branch of statistics aiming at drawing conclusions about one or two &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;populations&lt;/a&gt;, based on a subset (or two) of that population (called samples). In other words, we first collect a random set of observations from a population, and then some measurements are calculated in order to generalize to the population the information found through the sample.&lt;/p&gt;
&lt;p&gt;In this context, the &lt;strong&gt;one sample t-test is used to determine whether the mean of a measurement variable is different from a specified value&lt;/strong&gt; (a belief or a theoretical expectation for example). It works as follows: if the mean of the sample is too distant from the specified value (the value under the null hypothesis), it is considered that the mean of the population is different from what is expected. On the contrary, if the mean of the sample is close to the specified value, we cannot reject the hypothesis that the population mean is equal to what is expected.&lt;/p&gt;
&lt;p&gt;Like the Student’s t-test for two samples, there are also different versions of the one sample t-test. Luckily, there are only two different versions for this test (the Student’s t-test for two samples has 5 versions!). The difference between the two versions of the one sample t-test lies in the fact that one version is used when the variance of the population (not the variance of the sample!) is known, the other version being used when the variance of the population is unknown.&lt;/p&gt;
&lt;p&gt;In this article, I will first detail step by step how to perform both versions of the one sample t-test by hand. The analyses will be done on a small set of observations for the sake of illustration and easiness. I will then show how to perform this test in R with the exact same data in order to verify the results found by hand. Reminders about the reasoning behind hypothesis testing, interpretations of the &lt;em&gt;p&lt;/em&gt;-value and the results, and assumptions of this test will also be presented.&lt;/p&gt;
&lt;p&gt;Note that the aim of this article is to show how to compute the one sample t-test by hand and in R, so we refrain from testing the assumptions and we assume all assumptions are met for this exercise. For completeness, we still mention the assumptions and how to test them. Interested readers are invited to have a look at the &lt;a href=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r-test-on-one-mean/#assumptions&#34;&gt;end of the article&lt;/a&gt; for more information about these assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;null-and-alternative-hypothesis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Null and alternative hypothesis&lt;/h1&gt;
&lt;p&gt;Before diving into the computations of the one sample t-test by hand, let’s recap the null and alternative hypotheses of this test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu = \mu_0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu \ne \mu_0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the population mean and &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; is the known or &lt;strong&gt;hypothesized&lt;/strong&gt; value of the mean in the population.&lt;/p&gt;
&lt;p&gt;This is in the general case where we simply want to determine whether the population mean is &lt;strong&gt;different&lt;/strong&gt; (in terms of the dependent variable) compared to the hypothesized value. In this sense, we have no prior belief about the population mean being larger or smaller than the hypothesized value. This type of test is referred as a &lt;strong&gt;two-sided&lt;/strong&gt; or bilateral test.&lt;/p&gt;
&lt;p&gt;If we have some prior beliefs about the population mean being larger or smaller than the hypothesized value, the one sample t-test also allows to test the following hypotheses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu = \mu_0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu &amp;gt; \mu_0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu = \mu_0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu &amp;lt; \mu_0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the first case, we want to test if the population mean is significantly larger than the hypothesized value, while in the latter case, we want to test if the population mean is significantly smaller than the hypothesized value. This type of test is referred as a &lt;strong&gt;one-sided&lt;/strong&gt; or unilateral test.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;In statistics, many statistical tests is in the form of hypothesis tests. Hypothesis tests are used to determine whether a certain belief can be deemed as true (plausible) or not, based on the data at hand (i.e., the sample(s)). Most hypothesis tests boil down to the following 4 steps:&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;State the null and alternative hypothesis.&lt;/li&gt;
&lt;li&gt;Compute the test statistic, denoted t-stat. Formulas to compute the test statistic differ among the different versions of the one sample t-test but they have the same structure. See scenarios 1 and 2 below to see the different formulas.&lt;/li&gt;
&lt;li&gt;Find the critical value given the theoretical statistical distribution of the test, the parameters of the distribution and the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. For the two versions of the one sample t-test, it is either the normal or the Student’s t distribution (&lt;em&gt;t&lt;/em&gt; denoting the Student distribution and &lt;em&gt;z&lt;/em&gt; denoting the normal distribution).&lt;/li&gt;
&lt;li&gt;Conclude by comparing the t-stat (found in step 2.) with the critical value (found in step. 3). If the t-stat lies in the rejection region (determined thanks to the critical value and the direction of the test), we reject the null hypothesis, otherwise we do not reject the null hypothesis. These two alternatives (reject or do not reject the null hypothesis) are the only two possible solutions, we never “accept” an hypothesis. It is also a good practice to always interpret the decision in the terms of the initial question.
&lt;!-- See why we do not accept an hypothesis in this article covering the reasoning behind [hypothesis tests](/blog/xxx). --&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;two-versions-of-the-one-sample-t-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two versions of the one sample t-test&lt;/h1&gt;
&lt;p&gt;There are two versions of the one sample t-test, depending on whether the variance of the population (not the variance of the sample!) is known or unknown. This criteria is rather straightforward, we either know the variance of the population or we do not. The variance of the population cannot be computed because if you can compute the variance of a population, it means you have the data for the whole population, then there is no need to do a hypothesis test anymore…&lt;/p&gt;
&lt;p&gt;So the variance of the population is either given in the statement (use them in that case), or there is no information about the variance and in that case, it is assumed that the variance is unknown. In practice, the variance of the population is most of the time unknown. However, we still illustrate how to do both versions of this test by hand and in R in the next sections following the 4 steps of hypothesis testing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-compute-the-one-sample-t-test-by-hand&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to compute the one sample t-test by hand?&lt;/h1&gt;
&lt;p&gt;Note that the data are artificial and do not represent any real variable. Furthermore, remind that the assumptions may or may not be met. The point of the article is to detail how to compute the different versions of the test by hand and in R, so all assumptions are assumed to be met. Moreover, assume that the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 5\)&lt;/span&gt;% for all tests.&lt;/p&gt;
&lt;p&gt;If you are interested in applying these tests by hand without having to do the computations yourself, here is a &lt;a href=&#34;/blog/a-shiny-app-for-inferential-statistics-by-hand&#34;&gt;Shiny app&lt;/a&gt; which does it for you. You just need to enter the data and choose the appropriate version of the test thanks to the sidebar menu. There is also a graphical representation that helps you to visualize the test statistic and the rejection region. I hope you will find it useful!&lt;/p&gt;
&lt;div id=&#34;scenario-1-variance-of-the-population-is-known&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 1: variance of the population is known&lt;/h2&gt;
&lt;p&gt;For the first scenario, suppose the data below. Moreover, suppose that the population variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1\)&lt;/span&gt; and that we would like to test whether the population mean is different from 0.&lt;/p&gt;
&lt;table style=&#34;width:11%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5 observations: &lt;span class=&#34;math inline&#34;&gt;\(n = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of the sample: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x} = 0.56\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variance of the population: &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_0 = 0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the 4 steps of hypothesis testing we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu \ne 0\)&lt;/span&gt;. (&lt;span class=&#34;math inline&#34;&gt;\(\ne\)&lt;/span&gt; because we want to test whether the population mean is different from 0, we do not impose a direction in the test.)&lt;/li&gt;
&lt;li&gt;Test statistic: &lt;span class=&#34;math display&#34;&gt;\[z_{obs} = \frac{\bar{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}} = \frac{0.56-0}{0.447} = 1.252\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Critical value: &lt;span class=&#34;math inline&#34;&gt;\(\pm z_{\alpha / 2} = \pm z_{0.025} = \pm 1.96\)&lt;/span&gt; (see a guide on &lt;a href=&#34;/blog/a-guide-on-how-to-read-statistical-tables&#34;&gt;how to read statistical tables&lt;/a&gt; if you struggle to find the critical value)&lt;/li&gt;
&lt;li&gt;Conclusion: The rejection regions are thus from &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; to -1.96 and from 1.96 to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;. The test statistic is outside the rejection regions so we do not reject the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. In terms of the initial question: At the 5% significance level, we do not reject the hypothesis that the population mean is equal to 0, or there is no sufficient evidence in the data to conclude that the population mean is different from 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-2-variance-of-the-population-is-unknown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 2: variance of the population is unknown&lt;/h2&gt;
&lt;p&gt;For the second scenario, suppose the data below. Moreover, suppose that the variance in the population is unknown and that we would like to test whether the population mean is larger than 5.&lt;/p&gt;
&lt;table style=&#34;width:11%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;7.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;7.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;6.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5 observations: &lt;span class=&#34;math inline&#34;&gt;\(n = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of the sample: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x} = 6.8\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;standard deviation of the sample: &lt;span class=&#34;math inline&#34;&gt;\(s = 0.825\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu_0 = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the 4 steps of hypothesis testing we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu = 5\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu &amp;gt; 5\)&lt;/span&gt;. (&amp;gt; because we want to test whether the population mean is larger than 5.)&lt;/li&gt;
&lt;li&gt;Test statistic: &lt;span class=&#34;math display&#34;&gt;\[t_{obs} = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}} = \frac{6.8-5}{0.369} = 4.881\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Critical value: &lt;span class=&#34;math inline&#34;&gt;\(t_{\alpha, n - 1} = t_{0.05, 4} = 2.132\)&lt;/span&gt; (see a guide on &lt;a href=&#34;/blog/a-guide-on-how-to-read-statistical-tables&#34;&gt;how to read statistical tables&lt;/a&gt; if you struggle to find the critical value)&lt;/li&gt;
&lt;li&gt;Conclusion: The rejection region is thus from 2.132 to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;. The test statistic lies within the rejection region so we reject the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. In terms of the initial question: At the 5% significance level, we conclude that the population mean is larger than 5.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This concludes how to perform the two versions of the one sample t-test by hand. In the next sections, we detail how to perform the exact same tests in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;different-underlying-distributions-for-the-critical-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different underlying distributions for the critical value&lt;/h2&gt;
&lt;p&gt;As you may have noticed, the underlying probability distributions used to find the critical value are different depending on whether the variance of the population is known or unknown.&lt;/p&gt;
&lt;p&gt;The underlying probability distribution when the variance is known (scenario 1) is the normal distribution, while the probability distribution in the case where the variance is unknown (scenario 2) is the Student’s t distribution. This difference is partially explained by the fact that when the variance of the population is unknown, there is more “uncertainty” in the data, so we need to use the Student’s t distribution instead of the normal distribution.&lt;/p&gt;
&lt;p&gt;Note that when the sample size is large (usually when &lt;em&gt;n &amp;gt; 30&lt;/em&gt;), the Student’s t distribution tends to a normal distribution. Using a normal distribution when the variance is known and a Student’s t distribution when the variance is unknown also applies to a &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios&#34;&gt;t-test for two samples&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-compute-the-one-sample-t-test-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to compute the one sample t-test in R?&lt;/h1&gt;
&lt;p&gt;A good practice before doing t-tests in R is to visualize the data thanks to a &lt;a href=&#34;/blog/descriptive-statistics-in-r/#boxplot&#34;&gt;boxplot&lt;/a&gt; (or a &lt;a href=&#34;/blog/descriptive-statistics-in-r/#density-plot&#34;&gt;density plot&lt;/a&gt;, or eventually both). A boxplot gives a first indication on the location of the sample, and thus, a first indication on whether the null hypothesis is likely to be rejected or not. However, even if a boxplot or a density plot is great in showing the distribution of a sample, only a sound statistical test will confirm our first impression.&lt;/p&gt;
&lt;p&gt;After a visualization of the data, we replicate in R the results found by hand. We will see that for the version of the t-test with a known population variance, there is no default function built in R (at least to my knowledge, do not hesitate to let me know if I’m mistaken). In this case, a function is written to replicate the results by hand.&lt;/p&gt;
&lt;p&gt;Note that we use the same data, the same assumptions and the same question for both scenarios to facilitate the comparison between the tests performed by hand and in R.&lt;/p&gt;
&lt;div id=&#34;scenario-1-variance-of-the-population-is-known-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 1: variance of the population is known&lt;/h2&gt;
&lt;p&gt;For the first scenario, suppose the data below. Moreover, suppose that the population variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1\)&lt;/span&gt; and that we would like to test whether the population mean is different from 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat1 &amp;lt;- data.frame(
  value = c(0.9, -0.8, 1.3, -0.3, 1.7)
)

dat1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   value
## 1   0.9
## 2  -0.8
## 3   1.3
## 4  -0.3
## 5   1.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

ggplot(dat1) +
  aes(y = value) +
  geom_boxplot() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that you can use the &lt;a href=&#34;/blog/rstudio-addins-or-how-to-make-your-coding-life-easier/#esquisse&#34;&gt;&lt;code&gt;{esquisse}&lt;/code&gt; RStudio addin&lt;/a&gt; if you want to draw a boxplot with the package &lt;code&gt;{ggplot2}&lt;/code&gt; without writing the code yourself. If you prefer the default graphics, use the &lt;code&gt;boxplot()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(dat1$value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The boxplot shows that the distribution of the sample is not distant from 0 (the hypothesized value), so we tend to believe that we will not be able to reject the null hypothesis that the population mean is equal to 0. However, only a formal statistical test will confirm this belief.&lt;/p&gt;
&lt;p&gt;Since there is no function in R to perform a t-test with a known population variance, here is one with arguments accepting the sample (&lt;code&gt;x&lt;/code&gt;), the variance of the population (&lt;code&gt;V&lt;/code&gt;), the mean under the null hypothesis (&lt;code&gt;m0&lt;/code&gt;, default is &lt;code&gt;0&lt;/code&gt;), the significance level (&lt;code&gt;alpha&lt;/code&gt;, default is &lt;code&gt;0.05&lt;/code&gt;) and the alternative (&lt;code&gt;alternative&lt;/code&gt;, one of &lt;code&gt;&amp;quot;two.sided&amp;quot;&lt;/code&gt; (default), &lt;code&gt;&amp;quot;less&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;greater&amp;quot;&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test2 &amp;lt;- function(x, V, m0 = 0, alpha = 0.05, alternative = &amp;quot;two.sided&amp;quot;) {
  M &amp;lt;- mean(x)
  n &amp;lt;- length(x)
  sigma &amp;lt;- sqrt(V)
  S &amp;lt;- sqrt(V / n)
  statistic &amp;lt;- (M - m0) / S
  p &amp;lt;- if (alternative == &amp;quot;two.sided&amp;quot;) {
    2 * pnorm(abs(statistic), lower.tail = FALSE)
  } else if (alternative == &amp;quot;less&amp;quot;) {
    pnorm(statistic, lower.tail = TRUE)
  } else {
    pnorm(statistic, lower.tail = FALSE)
  }
  LCL &amp;lt;- (M - S * qnorm(1 - alpha / 2))
  UCL &amp;lt;- (M + S * qnorm(1 - alpha / 2))
  value &amp;lt;- list(mean = M, m0 = m0, sigma = sigma, statistic = statistic, p.value = p, LCL = LCL, UCL = UCL, alternative = alternative)
  # print(sprintf(&amp;quot;P-value = %g&amp;quot;,p))
  # print(sprintf(&amp;quot;Lower %.2f%% Confidence Limit = %g&amp;quot;,
  #               alpha, LCL))
  # print(sprintf(&amp;quot;Upper %.2f%% Confidence Limit = %g&amp;quot;,
  #               alpha, UCL))
  return(value)
}

test &amp;lt;- t.test2(dat1$value,
  V = 1
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $mean
## [1] 0.56
## 
## $m0
## [1] 0
## 
## $sigma
## [1] 1
## 
## $statistic
## [1] 1.252198
## 
## $p.value
## [1] 0.2104977
## 
## $LCL
## [1] -0.3165225
## 
## $UCL
## [1] 1.436523
## 
## $alternative
## [1] &amp;quot;two.sided&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above recaps all the information needed to perform the test: the test statistic, the &lt;em&gt;p&lt;/em&gt;-value, the alternative used, the sample mean, the hypothesized value and the population variance (compare these results found in R with the results found by hand).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value can be extracted as usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2104977&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value is 0.21 so at the 5% significance level we do not reject the null hypothesis. There is no sufficient evidence in the data to reject the hypothesis that the population mean is equal to 0. This result confirms what we found by hand.&lt;/p&gt;
&lt;p&gt;If you are unfamiliar with the concept of &lt;em&gt;p&lt;/em&gt;-value, I invite you to read my &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/#a-note-on-p-value-and-significance-level-alpha&#34;&gt;note on &lt;em&gt;p&lt;/em&gt;-value and significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To sum up&lt;/strong&gt; what have been said in that article about &lt;em&gt;p&lt;/em&gt;-value and significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the &lt;em&gt;p&lt;/em&gt;-value is smaller than the predetermined significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (usually 5%) so if &lt;em&gt;p&lt;/em&gt;-value &amp;lt; 0.05, we reject the null hypothesis&lt;/li&gt;
&lt;li&gt;If the &lt;em&gt;p&lt;/em&gt;-value is greater than or equal to the predetermined significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (usually 5%) so if &lt;em&gt;p&lt;/em&gt;-value &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; 0.05, we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This applies to all statistical tests without exception. Of course, the null and alternative hypotheses change depending on the test.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-2-variance-of-the-population-is-unknown-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 2: variance of the population is unknown&lt;/h2&gt;
&lt;p&gt;For the second scenario, suppose the data below. Moreover, suppose that the variance in the population is unknown and that we would like to test whether the population mean is larger than 5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat2 &amp;lt;- data.frame(
  value = c(7.9, 5.8, 6.3, 7.3, 6.7)
)

dat2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   value
## 1   7.9
## 2   5.8
## 3   6.3
## 4   7.3
## 5   6.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat2) +
  aes(y = value) +
  geom_boxplot() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unlike the previous scenario, the box is quite distant from the hypothesized value of 5. From this boxplot, we can expect the test to reject the null hypothesis of the population mean being equal to 5. Nonetheless, only a formal statistical test will confirm this expectation.&lt;/p&gt;
&lt;p&gt;There is a function in R, and it is simply the &lt;code&gt;t.test()&lt;/code&gt; function. This version of the test is actually the “standard” t-test for one sample. Note that in our case the alternative hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu &amp;gt; 5\)&lt;/span&gt; so we need to add the arguments &lt;code&gt;mu = 5&lt;/code&gt; and &lt;code&gt;alternative = &amp;quot;greater&amp;quot;&lt;/code&gt; to the function because the default arguments are &lt;code&gt;mu = 0&lt;/code&gt; and the two-sided test:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- t.test(dat2$value,
  mu = 5,
  alternative = &amp;quot;greater&amp;quot;
)

test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  dat2$value
## t = 4.8809, df = 4, p-value = 0.004078
## alternative hypothesis: true mean is greater than 5
## 95 percent confidence interval:
##  6.013814      Inf
## sample estimates:
## mean of x 
##       6.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above recaps all the information needed to perform the test: the name of the test, the test statistic, the degrees of freedom, the &lt;em&gt;p&lt;/em&gt;-value, the alternative used, the hypothesized value and the sample mean (compare these results found in R with the results found by hand).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value can be extracted as usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.004077555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value is 0.004 so at the 5% significance level we reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;Unlike the first scenario, the &lt;em&gt;p&lt;/em&gt;-value in this scenario is below 5% so we reject the null hypothesis. At the 5% significance level, we can conclude that the population mean is significantly larger than 5. This result confirms what we found by hand.&lt;/p&gt;
&lt;div id=&#34;confidence-interval&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confidence interval&lt;/h3&gt;
&lt;p&gt;Note that the confidence interval can be extracted with &lt;code&gt;$conf.int&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$conf.int&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.013814      Inf
## attr(,&amp;quot;conf.level&amp;quot;)
## [1] 0.95&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that the 95% confidence interval for the population mean is &lt;span class=&#34;math inline&#34;&gt;\([6.01; \infty]\)&lt;/span&gt;, meaning that, at the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 5\)&lt;/span&gt;%, we reject the null hypothesis as long as the hypothesized value &lt;span class=&#34;math inline&#34;&gt;\(\mu_0\)&lt;/span&gt; is below 6.01, otherwise the null hypothesis cannot be rejected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions&lt;/h1&gt;
&lt;p&gt;As for many statistical tests, there are some assumptions that need to be met in order to be able to interpret the results. When one or several assumptions are not met, although it is technically possible to perform these tests, it would be incorrect to interpret the results. Below are the assumptions of the one sample t-test and how to test them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The data, collected from a representative and randomly selected portion of the total population, should be independent of one another.&lt;/li&gt;
&lt;li&gt;The dependent variable (i.e., the measured variable) must be continuous.&lt;/li&gt;
&lt;li&gt;Observations should follow a &lt;a href=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r&#34;&gt;&lt;strong&gt;normal distribution&lt;/strong&gt;&lt;/a&gt;. The normality assumption can be tested visually thanks to a histogram and a QQ-plot, and/or formally via a normality test such as the Shapiro-Wilk or Kolmogorov-Smirnov test (see more information about the normality assumption and how to test it &lt;a href=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/#how-to-test-the-normality-assumption&#34;&gt;here&lt;/a&gt;). Some transformations, such as, among others, taking the logarithm, the square root or the Box-Cox transformation can be applied on the observations to transform you data to better fit the normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you to understand how the different versions of the one sample t-test work and how to perform them by hand and in R. If you are interested, here is a &lt;a href=&#34;/blog/a-shiny-app-for-inferential-statistics-by-hand&#34;&gt;Shiny app&lt;/a&gt; to perform these tests by hand easily (you just need to enter your data and select the appropriate version of the test thanks to the sidebar menu). Moreover, read &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios&#34;&gt;this article&lt;/a&gt; if you would like to know how to compute the Student’s t-test but this time, for two samples, in order to compare two dependent or independent groups.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;It is a least the case regarding parametric hypothesis tests. A parametric test means that it is based on a theoretical statistical distribution, which depends on some defined parameters. In the case of the one sample t-test, it is based on the Student’s t distribution with a single parameter, the degrees of freedom (&lt;span class=&#34;math inline&#34;&gt;\(df = n - 1\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the sample size), or the normal distribution.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The 9 concepts and formulas in probability that every data scientist should know</title>
      <link>/blog/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/</link>
      <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-probability&#34;&gt;What is probability?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-probability-is-always-between-0-and-1&#34;&gt;1. A probability is always between 0 and 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compute-a-probability&#34;&gt;2. Compute a probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complement-of-an-event&#34;&gt;3. Complement of an event&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#union-of-two-events&#34;&gt;4. Union of two events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#intersection-of-two-events&#34;&gt;5. Intersection of two events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#independence-of-two-events&#34;&gt;6. Independence of two events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-probability&#34;&gt;7. Conditional probability&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bayes-theorem&#34;&gt;Bayes’ theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accuracy-measures&#34;&gt;8. Accuracy measures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#false-negatives&#34;&gt;False negatives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#false-positives&#34;&gt;False positives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sensitivity&#34;&gt;Sensitivity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#specificity&#34;&gt;Specificity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#positive-predictive-value&#34;&gt;Positive predictive value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#negative-predictive-value&#34;&gt;Negative predictive value&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#counting-techniques&#34;&gt;9. Counting techniques&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#multiplication&#34;&gt;Multiplication&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-1&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#permutation&#34;&gt;Permutation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-2&#34;&gt;Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in-r&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#combination&#34;&gt;Combination&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-3&#34;&gt;Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand-1&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in-r-1&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know.jpeg&#34; alt=&#34;Photo by Josh Appel&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Josh Appel&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is probability?&lt;/h1&gt;
&lt;p&gt;Probability is the likelihood of an event occurring; it is a mathematical model to describe random phenomena. In other words, probability is a branch of mathematics that provides models to describe random processes. These mathematical tools allow to establish theoretical models for random phenomena and to use them to make predictions. Like every model, the probabilistic model is a simplification of the world. However, the model is useful as soon as it captures the essential features.&lt;/p&gt;
&lt;p&gt;In this article, we present 9 fundamental formulas and concepts in probability that every data scientist should understand and master in order to appropriately handle any project in probability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-probability-is-always-between-0-and-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. A probability is always between 0 and 1&lt;/h1&gt;
&lt;p&gt;The probability of an event is always between 0 and 1,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0 \le P(A) \le 1\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If an event is impossible: &lt;span class=&#34;math inline&#34;&gt;\(P(A) = 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If an event is certain: &lt;span class=&#34;math inline&#34;&gt;\(P(A) = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, throwing a 7 with a standard six-sided dice (with faces ranging from 1 to 6) is impossible so its probability is equal to 0. Throwing head or tail with a coin is certain, so its probability is equal to 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-a-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Compute a probability&lt;/h1&gt;
&lt;p&gt;If the elements of a sample space (the set of all possible results of a randomized experiment) are equiprobable (= all elements have the same probability), then the probability of an event occurring is equal to the number of favourable cases (number of ways it can happen) divided by the number of possible cases (total number of outcomes):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A) = \frac{\text{number of favourable cases}}{\text{number of possible cases}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For example, all numbers of a six-sided dice are equiprobable since they all have the same probability of occurring. The probability of rolling a 3 with a dice is thus&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(3) = \frac{\text{number of favourable cases}}{\text{number of possible cases}} = \frac{1}{6}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;because there is only one favourable case (there is only one face with a 3 on it), and there are 6 possible cases (because there are 6 faces altogether).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complement-of-an-event&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Complement of an event&lt;/h1&gt;
&lt;p&gt;The probability of the complement (or opposite) of an event is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{not A}) = P(\bar{A}) = 1 - P(A)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For instance, the probability of not throwing a 3 with a dice is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\bar{A}) = 1 - P(A) = 1 - \frac{1}{6} = \frac{5}{6}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;union-of-two-events&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Union of two events&lt;/h1&gt;
&lt;p&gt;The probability of the union of two events is the probability of either occurring:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{A or B)} = P(A \cup B) = P(A) + P(B) - P(A \cap B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose that the probability of a fire breaking out in two houses in a given year is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in house A: 60%, so &lt;span class=&#34;math inline&#34;&gt;\(P(A) = 0.6\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in house B: 45%, so &lt;span class=&#34;math inline&#34;&gt;\(P(B) = 0.45\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in at least one of the two houses: 80%, so &lt;span class=&#34;math inline&#34;&gt;\(P(A \cup B) = 0.8\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Graphically we have&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The probability of a fire breaking out in house A &lt;strong&gt;or&lt;/strong&gt; house B is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cup B) = P(A) + P(B) - P(A \cap B)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= 0.6 + 0.45 - 0.25 = 0.8\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By summing &lt;span class=&#34;math inline&#34;&gt;\(P(A)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(B)\)&lt;/span&gt;, the intersection of A and B, i.e. &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B)\)&lt;/span&gt;, is counted twice. This is the reason we subtract it to count it only once.&lt;/p&gt;
&lt;p&gt;If two events are mutually exclusive (i.e., two events that cannot occur simultaneously), the probability of both events occurring is equal to 0, so the above formula becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cup B) = P(A) + P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For example, the event “rolling a 3” and the event “rolling a 6” on a six-sided dice are two mutually exclusive events since they cannot both occur at the same time. Since their joint probability is equal to 0, the probability of rolling a 3 or 6 on a six-sided dice is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(3 \cup 6) = P(3) + P(6) = \frac{1}{6} + \frac{1}{6} = \frac{1}{3}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intersection-of-two-events&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Intersection of two events&lt;/h1&gt;
&lt;p&gt;If two events are independent, the probability of the intersection of the two events (i.e., the joint probability) is the probability of the two events occurring:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{A and B)} = P(A \cap B) = P(A) \cdot P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For instance, if two coins are flipped, the probability of both coins being tails is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]&lt;/span&gt;
Note that &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B) = P(B \cap A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If two events are mutually exclusive, their joint probability is equal to 0:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) = 0\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;independence-of-two-events&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Independence of two events&lt;/h1&gt;
&lt;p&gt;The independence of two events can be verified thanks to the above formula. If the equality holds, the two events are said to be independent, otherwise the two events are said to be dependent. Formally, the events A and B are independent if and only if&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) = P(A) \cdot P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the example of the two coins:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1 \cap T_2) = \frac{1}{4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so the following equality holds&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two events are thus independent, denoted &lt;span class=&#34;math inline&#34;&gt;\(T_1{\perp\!\!\!\perp}T_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the example of the fire breaking out in two houses (see &lt;a href=&#34;/blog/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/#union-of-two-events&#34;&gt;section 4&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) = 0.25\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A) \cdot P(B) = 0.6 \cdot 0.45 = 0.27\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so the following equality does not hold&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) \ne P(A) \cdot P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two events are thus dependent (or not independent), denoted &lt;span class=&#34;math inline&#34;&gt;\(A \not\!\perp\!\!\!\perp B\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;7. Conditional probability&lt;/h1&gt;
&lt;p&gt;Suppose two events A and B and &lt;span class=&#34;math inline&#34;&gt;\(P(B) &amp;gt; 0\)&lt;/span&gt;. The conditional probability of A given (knowing) B is the likelihood of event A occurring given that event B has occurred:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \frac{P(B \cap A)}{P(B)} \text{ (since } P(A \cap B) = P(B \cap A))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that, in general, the probability of A given B is not equal to the probability of B given A, that is, &lt;span class=&#34;math inline&#34;&gt;\(P(A | B) \ne P(B | A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From the formula of the conditional probability, we can derive the multiplicative law:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)} \text{ (Eq. 1)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) \cdot P(B) = \frac{P(A \cap B)}{P(B)} \cdot P(B)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) \cdot P(B) = P(A \cap B) \text{ (multiplicative law)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If two events are independent, &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B) = P(A) \cdot P(B)\)&lt;/span&gt;, and:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(B) &amp;gt; 0\)&lt;/span&gt;, the conditional probability becomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A) \cdot P(B)}{P(B)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = P(A) \text{ (Eq. 2)}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(A) &amp;gt; 0\)&lt;/span&gt;, the conditional probability becomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(B \cap A)}{P(A)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(B) \cdot P(A)}{P(A)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = P(B) \text{ (Eq. 3)}\]&lt;/span&gt;
Equations 2 and 3 mean that knowing that one event occurred does not influence the probability of the outcome of the other event. This is in fact the definition of the independence: if knowing that one event occurred does not help to predict (does not influence) the outcome of the other event, the two events are by essence independent.&lt;/p&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes’ theorem&lt;/h2&gt;
&lt;p&gt;From the formulas of the conditional probability and the multiplicative law, we can derive the Bayes’ theorem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(B \cap A)}{P(A)} \text{ (from the conditional probability)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(A \cap B)}{P(A)} \text{ (since } P(A \cap B) = P(B \cap A))\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \text{ (from the multiplicative law)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is equivalent to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)} \text{ (Bayes&amp;#39; theorem)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;In order to illustrate the conditional probability and the Bayes’ theorem, suppose the following problem:&lt;/p&gt;
&lt;p&gt;In order to determine the presence of a disease in a person, a blood test is performed. When a person has the disease, the test can reveal the disease in 80% of cases. When the disease is not present, the test is negative in 90% of cases. Experience has shown that the probability of the disease being present is 10%. A researcher would like to know the probability that an individual has the disease given that the result of the test is positive.&lt;/p&gt;
&lt;p&gt;To answer this question, the following events are defined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P: the test result is positive&lt;/li&gt;
&lt;li&gt;D: the person has the disease&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, we use a tree diagram to illustrate the statement:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/Screenshot%202020-03-03%20at%2013.54.24.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(The sum of all 4 scenarios must be equal to 1 since these 4 scenarios include all possible cases.)&lt;/p&gt;
&lt;p&gt;We are looking for the probability that an individual has the disease given that the result of the test is positive, &lt;span class=&#34;math inline&#34;&gt;\(P(D | P)\)&lt;/span&gt;. Following the formula of the conditional probability (Eq. 1) we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In terms of our problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(D | P) = \frac{P(D \cap P)}{P(P)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(D | P) = \frac{0.08}{P(P)} \text{ (Eq. 4)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the tree diagram, we can see that a positive test result is possible under two scenarios: (i) when a person has the disease, or (ii) when the person does not actually have the disease (because the test is not always correct). In order to find the probability of a positive test result, &lt;span class=&#34;math inline&#34;&gt;\(P(P)\)&lt;/span&gt;, we need to sum up those two scenarios:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(P) = P(D \cap P) + P(\bar{D} \cap P) = 0.08+0.09=0.17\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Eq. 4 then becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(D | P) = \frac{0.08}{0.17} = 0.4706\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The probability of having the disease given that the result of the test is positive is only 47.06%. This means that in this specific case (with the same percentages), an individual has less than 1 chance out of 2 of having the disease knowing that his test is positive!&lt;/p&gt;
&lt;p&gt;This relatively small percentage is due to the facts that the disease is quite rare (only 10% of the population is affected) and that the test is not always correct (sometimes it detects the disease although it is not present, and sometimes it does not detect it although it is present). As a consequence, a higher percentage of healthy people have a positive result (9%) compared to the percentage of people who have a positive result and who actually have the disease (8%). This explains why several diagnostic tests are often performed before announcing the result of the test, especially for rare diseases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;accuracy-measures&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;8. Accuracy measures&lt;/h1&gt;
&lt;p&gt;Based on the example of the disease and the diagnostic test presented above, we explain the most common accuracy measures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;False negatives&lt;/li&gt;
&lt;li&gt;False positives&lt;/li&gt;
&lt;li&gt;Sensitivity&lt;/li&gt;
&lt;li&gt;Specificity&lt;/li&gt;
&lt;li&gt;Positive predictive value&lt;/li&gt;
&lt;li&gt;Negative predictive value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before diving into the details of these accuracy measures, here is an overview of the measures and the tree diagram with the labels added for each of the 4 scenarios:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know.png&#34; alt=&#34;Adapted from Wikipedia&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Adapted from Wikipedia&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/Screenshot%202020-03-03%20at%2015.53.19.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;false-negatives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;False negatives&lt;/h2&gt;
&lt;p&gt;The false negatives (FN) are the number of people incorrectly labeled as &lt;strong&gt;not&lt;/strong&gt; having the disease or the condition, when in reality it is present. It is like telling a women who is 7 months pregnant that she is not pregnant.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[FN = P(D \cap \bar{P}) = 0.02\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;false-positives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;False positives&lt;/h2&gt;
&lt;p&gt;The false positives (FP) are the number of people incorrectly labeled as having the disease or the condition, when in reality it is &lt;strong&gt;not&lt;/strong&gt; present. It is like telling a man he is pregnant.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[FP = P(\bar{D} \cap P) = 0.09\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sensitivity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sensitivity&lt;/h2&gt;
&lt;p&gt;The sensitivity of a test, also referred as the recall, measures the ability of a test to detect the condition when the condition is present (the percentage of sick people who are correctly identified as having the disease):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Sensitivity = \frac{TP}{TP + FN}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;TP&lt;/em&gt; is the true positives.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Sensitivity = \frac{TP}{TP + FN} = P(P|D) = 0.8\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specificity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specificity&lt;/h2&gt;
&lt;p&gt;The specificity of a test measures the ability of a test to correctly exclude the condition when the condition is absent (the percentage of healthy people who are correctly identified as not having the disease):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Specificity = \frac{TN}{TN + FP}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;TN&lt;/em&gt; is the true negatives.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Specificity = \frac{TN}{TN + FP} = P(\bar{P} | \bar{D}) = 0.9\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;positive-predictive-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Positive predictive value&lt;/h2&gt;
&lt;p&gt;The positive predictive value, also referred as the precision, is the proportion of positives that correspond to the presence of the condition, so the proportions of positive results that are true positive results:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[PPV = \frac{TP}{TP+FP}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[PPV = \frac{TP}{TP+FP} = P(D | P) = \frac{P(D \cap P)}{P(P)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \frac{0.08}{0.08+0.09} = 0.4706\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;negative-predictive-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Negative predictive value&lt;/h2&gt;
&lt;p&gt;The negative predictive value is the proportion of negatives that correspond to the absence of the condition, so the proportions of negative results that are true negative results:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[NPV = \frac{TN}{TN + FN}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[NPV = \frac{TN}{TN + FN} = P(\bar{D} | \bar{P}) = \frac{P(\bar{D} \cap \bar{P})}{P(\bar{P})}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \frac{0.81}{0.81+0.02} = 0.9759\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;counting-techniques&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;9. Counting techniques&lt;/h1&gt;
&lt;p&gt;In order to use the formula in &lt;a href=&#34;/blog/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/#compute-a-probability&#34;&gt;section 2&lt;/a&gt;, one must know how to count the number of possible elements.&lt;/p&gt;
&lt;p&gt;There are 3 main counting techniques in probability:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Multiplication&lt;/li&gt;
&lt;li&gt;Permutation&lt;/li&gt;
&lt;li&gt;Combination&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;See below how to count the number of possible elements in case of equiprobable results.&lt;/p&gt;
&lt;div id=&#34;multiplication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiplication&lt;/h2&gt;
&lt;p&gt;The multiplication rule is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\#(A \times B) = (\#A) \times (\#B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\#\)&lt;/span&gt; is the number of elements.&lt;/p&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;In a restaurant, a customer has to choose a starter, a main course and a dessert. The restaurant offers 2 starters, 3 main courses and 2 desserts. How many different choices are possible?&lt;/p&gt;
&lt;p&gt;There are 12 different possible choices (i.e., &lt;span class=&#34;math inline&#34;&gt;\(2 \cdot 3 \cdot 2\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Permutation&lt;/h2&gt;
&lt;p&gt;The number of permutations is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P^r_n = n \times (n - 1) \times \cdots \times (n - r + 1) = \frac{n !}{(n - r)!}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; the length, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; the number of elements and &lt;span class=&#34;math inline&#34;&gt;\(r \le n\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(0! = 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k! = k \times (k - 1) \times (k - 2) \times \cdots \times 2 \times 1\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(k = 1, 2, \dots\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The order is important in permutations!&lt;/p&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Count the permutations of length 2 of the set &lt;span class=&#34;math inline&#34;&gt;\(A = \{a, b, c, d\}\)&lt;/span&gt;, without a letter being repeated. How many permutations do you find?&lt;/p&gt;
&lt;div id=&#34;by-hand&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P^4_2 = \frac{4!}{(4-2)!} = \frac{4\cdot3\cdot2\cdot1}{2\cdot1} = 12\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gtools)

x &amp;lt;- c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;)

# See all different permutations
perms &amp;lt;- permutations(
  n = 4, r = 2, v = x,
  repeats.allowed = FALSE
)
perms&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1] [,2]
##  [1,] &amp;quot;a&amp;quot;  &amp;quot;b&amp;quot; 
##  [2,] &amp;quot;a&amp;quot;  &amp;quot;c&amp;quot; 
##  [3,] &amp;quot;a&amp;quot;  &amp;quot;d&amp;quot; 
##  [4,] &amp;quot;b&amp;quot;  &amp;quot;a&amp;quot; 
##  [5,] &amp;quot;b&amp;quot;  &amp;quot;c&amp;quot; 
##  [6,] &amp;quot;b&amp;quot;  &amp;quot;d&amp;quot; 
##  [7,] &amp;quot;c&amp;quot;  &amp;quot;a&amp;quot; 
##  [8,] &amp;quot;c&amp;quot;  &amp;quot;b&amp;quot; 
##  [9,] &amp;quot;c&amp;quot;  &amp;quot;d&amp;quot; 
## [10,] &amp;quot;d&amp;quot;  &amp;quot;a&amp;quot; 
## [11,] &amp;quot;d&amp;quot;  &amp;quot;b&amp;quot; 
## [12,] &amp;quot;d&amp;quot;  &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Count the number of permutations
nrow(perms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;combination&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Combination&lt;/h2&gt;
&lt;p&gt;The number of combinations is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[C^r_n = \frac{P^r_n}{r!} = \frac{n !}{r!(n - r)!} = {n \choose r}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \frac{n}{r} \times \frac{n - 1}{r - 1} \times \dots \times \frac{n - r + 1}{1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; the length, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; the number of elements and &lt;span class=&#34;math inline&#34;&gt;\(r \le n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The order is &lt;strong&gt;not&lt;/strong&gt; important in combinations!&lt;/p&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;In a family of 5 children, what is the probability that there are 3 girls and 2 boys? Assume that the probabilities of giving birth to a girl and a boy are equal.&lt;/p&gt;
&lt;div id=&#34;by-hand-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Count of 3 girls and 2 boys (favourable cases): &lt;span class=&#34;math inline&#34;&gt;\(C^3_5 = {5 \choose 3} = \frac{5!}{3!(5-3)!} = 10\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Count of possible cases: &lt;span class=&#34;math inline&#34;&gt;\(2^5 = 32\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow P(3 \text{ girls and 2 boys}) = \frac{\text{# of favourable cases}}{\text{# of possible cases}}\)&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[= \frac{10}{32} = 0.3125\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Count of 3 girls and 2 boys:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;choose(n = 5, k = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Count of possible cases:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2^5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Probability of 3 girls and 2 boys:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;choose(n = 5, k = 3) / 2^5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3125&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you to understand the most important formulas and concepts from probability theory.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Student&#39;s t-test in R and by hand: how to compare two groups under different scenarios</title>
      <link>/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#null-and-alternative-hypothesis&#34;&gt;Null and alternative hypothesis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypothesis-testing&#34;&gt;Hypothesis testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#different-versions-of-the-students-t-test&#34;&gt;Different versions of the Student’s t-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-compute-students-t-test-by-hand&#34;&gt;How to compute Student’s t-test by hand?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-1-independent-samples-with-2-known-variances&#34;&gt;Scenario 1: Independent samples with 2 known variances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-2-independent-samples-with-2-equal-but-unknown-variances&#34;&gt;Scenario 2: Independent samples with 2 equal but unknown variances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-3-independent-samples-with-2-unequal-and-unknown-variances&#34;&gt;Scenario 3: Independent samples with 2 unequal and unknown variances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-4-paired-samples-where-the-variance-of-the-differences-is-known&#34;&gt;Scenario 4: Paired samples where the variance of the differences is known&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-5-paired-samples-where-the-variance-of-the-differences-is-unknown&#34;&gt;Scenario 5: Paired samples where the variance of the differences is unknown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-compute-students-t-test-in-r&#34;&gt;How to compute Student’s t-test in R?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-1-independent-samples-with-2-known-variances-1&#34;&gt;Scenario 1: Independent samples with 2 known variances&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-note-on-p-value-and-significance-level-alpha&#34;&gt;A note on &lt;em&gt;p&lt;/em&gt;-value and significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-2-independent-samples-with-2-equal-but-unknown-variances-1&#34;&gt;Scenario 2: Independent samples with 2 equal but unknown variances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-3-independent-samples-with-2-unequal-and-unknown-variances-1&#34;&gt;Scenario 3: Independent samples with 2 unequal and unknown variances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-4-paired-samples-where-the-variance-of-the-differences-is-known-1&#34;&gt;Scenario 4: Paired samples where the variance of the differences is known&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scenario-5-paired-samples-where-the-variance-of-the-differences-is-unknown-1&#34;&gt;Scenario 5: Paired samples where the variance of the differences is unknown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assumptions&#34;&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios_files/Student-t-test-in-R-and-by-hand-how-to-compare-two-groups-under-different-scenarios.jpeg&#34; alt=&#34;Photo by Jason Dent&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Jason Dent&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;One of the most important test within the branch of inferential statistics is the &lt;strong&gt;Student’s t-test&lt;/strong&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The Student’s t-test for two samples is used to &lt;strong&gt;test whether two groups (two populations) are different&lt;/strong&gt; in terms of a quantitative variable, &lt;strong&gt;based on the comparison of two samples&lt;/strong&gt; drawn from these two groups. In other words, a Student’s t-test for two samples allows to determine whether the two populations from which your two samples are drawn are different (with the two samples being measured on a &lt;a href=&#34;/blog/variable-types-and-examples/#continuous&#34;&gt;quantitative continuous&lt;/a&gt; variable).&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The reasoning behind this statistical test is that if your two samples are markedly different from each other, it can be assumed that the two populations from which the samples are drawn are different. On the contrary, if the two samples are rather similar, we cannot reject the hypothesis that the two populations are similar, so there is no sufficient evidence in the data at hand to conclude that the two populations from which the samples are drawn are different. Note that this statistical tool belongs to the branch of inferential statistics because conclusions drawn from the study of the samples are generalized to the population, even though we do not have the data on the entire population.&lt;/p&gt;
&lt;p&gt;To compare two samples, it is usual to compare a measure of central tendency computed for each sample. In the case of the Student’s t-test, the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#mean&#34;&gt;mean&lt;/a&gt; is used to compare the two samples. However, in some cases, the mean is not appropriate to compare two samples so the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#median&#34;&gt;median&lt;/a&gt; is used to compare them via the Wilcoxon test. This article being already quite long and complete, the Wilcoxon test will be covered in a separate article, together with some illustrations on when to use one test or the other.&lt;/p&gt;
&lt;p&gt;These two tests (Student’s t-test and Wilcoxon test) have the same final goal, that is, compare two samples in order to determine whether the two populations from which they were drawn are different or not. Note that the Student’s t-test is more powerful than the Wilcoxon test (it more often detects a significant difference if there is a true difference, so a smaller difference can be detected with the Student’s t-test) but the Student’s t-test is sensitive to outliers and data asymmetry. Furthermore, within each of these two tests, several versions exist, with each version using different formulas to arrive at the final result. It is thus necessary to understand the difference between the two tests and which version to use in order to carry out the appropriate analyses depending on the question and the data at hand.&lt;/p&gt;
&lt;p&gt;In this article, I will first detail step by step how to perform all versions of the Student’s t-test for independent and paired samples by hand. The analyses will be done on a small set of observations for the sake of illustration and easiness. I will then show how to perform this test in R with the exact same data in order to verify the results found by hand. Reminders about the reasoning behind hypothesis testing, interpretations of the &lt;em&gt;p&lt;/em&gt;-value and the results, and assumptions of this test will also be presented.&lt;/p&gt;
&lt;p&gt;Note that the aim of this article is to show how to compute the Student’s t-test by hand and in R, so we refrain from testing the assumptions and we assume all assumptions are met for this exercise. For completeness, we still mention the assumptions, how to test them and what other tests exist if an assumption is not met. Interested readers are invited to have a look at the &lt;a href=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/#assumptions&#34;&gt;end of the article&lt;/a&gt; for more information about these assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;null-and-alternative-hypothesis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Null and alternative hypothesis&lt;/h1&gt;
&lt;p&gt;Before diving into the computations of the Student’s t-test by hand, let’s recap the null and alternative hypotheses of this test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_1 = \mu_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_1 \ne \mu_2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_2\)&lt;/span&gt; are the means of the two populations from which the samples were drawn.&lt;/p&gt;
&lt;p&gt;As mentioned in the introduction, although technically the Student’s t-test is based on the comparison of the means of the two samples, the final goal of this test is actually to test the following hypotheses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: the two populations are similar&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: the two populations are different&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is in the general case where we simply want to determine whether the two populations are &lt;strong&gt;different&lt;/strong&gt; or not (in terms of the dependent variable). In this sense, we have no prior belief about a particular population being larger or smaller than the other. This type of test is referred as a &lt;strong&gt;two-sided&lt;/strong&gt; or bilateral test.&lt;/p&gt;
&lt;p&gt;If we have some prior beliefs about one population being larger or smaller than the other, the Student’s t-test also allows to test the following hypotheses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_1 = \mu_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_1 &amp;gt; \mu_2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_1 = \mu_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\mu_1 &amp;lt; \mu_2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the first case, we want to test if the first population is significantly larger than the second, while in the latter case, we want to test if the first population is significantly smaller than the second. This type of test is referred as a &lt;strong&gt;one-sided&lt;/strong&gt; or unilateral test.&lt;/p&gt;
&lt;p&gt;Some authors argue that one-sided tests should not be used in practice for the simple reason that, if a researcher is so sure that one population is larger (smaller) than the other and would never be smaller (larger) than the other, why would she needs to test for significance at all? This a rather philosophical question and it is beyond the scope of this article. Interested readers are invited to see part of the discussion in &lt;span class=&#34;citation&#34;&gt;Rowntree (2000)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis testing&lt;/h1&gt;
&lt;p&gt;In statistics, many statistical tests is in the form of hypothesis tests. Hypothesis tests are used to determine whether a certain belief can be deemed as true (plausible) or not, based on the data at hand (i.e., the sample(s)). Most hypothesis tests boil down to the following 4 steps:&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;State the null and alternative hypothesis.&lt;/li&gt;
&lt;li&gt;Compute the test statistic, denoted t-stat. Formulas to compute the test statistic differ among the different versions of the Student’s t-test but they have the same structure. See scenarios 1 to 5 below to see the different formulas.&lt;/li&gt;
&lt;li&gt;Find the critical value given the theoretical statistical distribution of the test, the parameters of the distribution and the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. For a Student’s t-test and its extended version, it is either the normal or the Student’s t distribution (&lt;em&gt;t&lt;/em&gt; denoting the Student distribution and &lt;em&gt;z&lt;/em&gt; denoting the normal distribution).&lt;/li&gt;
&lt;li&gt;Conclude by comparing the t-stat (found in step 2.) with the critical value (found in step. 3). If the t-stat lies in the rejection region (determined thanks to the critical value and the direction of the test), we reject the null hypothesis, otherwise we do not reject the null hypothesis. These two alternatives (reject or do not reject the null hypothesis) are the only two possible solutions, we never “accept” an hypothesis. It is also a good practice to always interpret the decision in the terms of the initial question.
&lt;!-- See why we do not accept an hypothesis in this article covering the reasoning behind [hypothesis tests](/blog/xxx). --&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;different-versions-of-the-students-t-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Different versions of the Student’s t-test&lt;/h1&gt;
&lt;p&gt;There are several versions of the Student’s t-test for two samples, depending on whether the samples are independent or paired and depending on the variances of the populations:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph {\n\ngraph [rankdir = \&#34;LR\&#34;]\n\n\n\n  \&#34;1\&#34; [label = \&#34;Test on 2 means\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Test on 2 means\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;2\&#34; [label = \&#34;2 independepent samples\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: 2 independepent samples\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;3\&#34; [label = \&#34;2 variances are known (scenario 1)\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: 2 variances are known (scenario 1)\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;4\&#34; [label = \&#34;2 variances are equal but unknown (scenario 2)\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: 2 variances are equal but unknown (scenario 2)\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;5\&#34; [label = \&#34;2 variances are unequal and unknown (scenario 3)\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: 2 variances are unequal and unknown (scenario 3)\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;6\&#34; [label = \&#34;2 paired samples\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: 2 paired samples\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;7\&#34; [label = \&#34;Variance of the differences is known (scenario 4)\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Variance of the differences is known (scenario 4)\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;8\&#34; [label = \&#34;Variance of the differences is unknown (scenario 5)\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Variance of the differences is unknown (scenario 5)\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;1\&#34;-&gt;\&#34;2\&#34; \n  \&#34;1\&#34;-&gt;\&#34;6\&#34; \n  \&#34;2\&#34;-&gt;\&#34;3\&#34; \n  \&#34;2\&#34;-&gt;\&#34;4\&#34; \n  \&#34;2\&#34;-&gt;\&#34;5\&#34; \n  \&#34;6\&#34;-&gt;\&#34;7\&#34; \n  \&#34;6\&#34;-&gt;\&#34;8\&#34; \n}&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;On the one hand, &lt;strong&gt;independent&lt;/strong&gt; samples means that the two samples are collected on &lt;strong&gt;different&lt;/strong&gt; experimental units or different individuals, for instance when we are working on women and men separately, or working on patients who have been randomly assigned to a control and a treatment group (and a patient belongs to only one group). On the other hand, we face &lt;strong&gt;paired&lt;/strong&gt; samples when measurements are collected on the &lt;strong&gt;same&lt;/strong&gt; experimental units, same individuals. This is often the case, for example in medical studies, when testing the efficiency of a treatment at two different times. The same patients are measured twice, before and after the treatment, and the dependency between the two samples must be taken into account in the computation of the test statistic by working on the &lt;strong&gt;differences&lt;/strong&gt; of measurements for each subject. Paired samples are usually the result of measurements at two different times, but not exclusively. Suppose we want to test the difference in vision between the left and right eyes of 50 athletes. Although the measurements are not made at two different time (before-after), it is clear that both eyes are dependent within each subject. Therefore, the Student’s t-test for paired samples should be used to account for the dependency between the two samples instead of the standard Student’s t-test for independent samples.&lt;/p&gt;
&lt;p&gt;Another criteria for choosing the appropriate version of the Student’s t-test is whether the variances of the populations (not the variances of the samples!) are known or unknown and equal or unequal. This criteria is rather straightforward, we either know the variances of the populations or we do not. The variances of the populations cannot be computed because if you can compute the variance of a population, it means you have the data for the whole population, then there is no need to do a hypothesis test anymore… So the variances of the populations are either given in the statement (use them in that case), or there is no information about these variances and in this case, it is assumed that the variances are unknown. In practice, the variances of the populations are most of the time unknown and the only thing to do in order to choose the appropriate version of the test is to check whether the variances are equal or not. However, we still illustrate how to do all versions of this test by hand and in R in the next sections following the 4 steps of hypothesis testing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-compute-students-t-test-by-hand&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to compute Student’s t-test by hand?&lt;/h1&gt;
&lt;p&gt;Note that the data are artificial and do not represent any real variable. Furthermore, remind that the assumptions may or may not be met. The point of the article is to detail how to compute the different versions of the test by hand and in R, so all assumptions are assumed to be met. Moreover, assume that the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 5\)&lt;/span&gt;% for all tests.&lt;/p&gt;
&lt;p&gt;If you are interested in applying these tests by hand without having to do the computations yourself, here is a &lt;a href=&#34;/blog/a-shiny-app-for-inferential-statistics-by-hand&#34;&gt;Shiny app&lt;/a&gt; which does it for you. You just need to enter the data and choose the appropriate version of the test thanks to the sidebar menu. There is also a graphical representation that helps you to visualize the test statistic and the rejection region. I hope you will find it useful!&lt;/p&gt;
&lt;div id=&#34;scenario-1-independent-samples-with-2-known-variances&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 1: Independent samples with 2 known variances&lt;/h2&gt;
&lt;p&gt;For the first scenario, suppose the data below. Moreover, suppose that the two samples are independent, that the variances &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1\)&lt;/span&gt; in both populations and that we would like to test whether the two populations are different.&lt;/p&gt;
&lt;table style=&#34;width:24%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;value&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;sample&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5 observations in each sample: &lt;span class=&#34;math inline&#34;&gt;\(n_1 = n_2 = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of sample 1: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_1 = 0.02\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of sample 2: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_2 = 0.06\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variances of both populations: &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_1 = \sigma^2_2 = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the 4 steps of hypothesis testing we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu_1 = \mu_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_1 - \mu_2 \ne 0\)&lt;/span&gt;. (&lt;span class=&#34;math inline&#34;&gt;\(\ne\)&lt;/span&gt; because we want to test whether the two means are different, we do not impose a direction in the test.)&lt;/li&gt;
&lt;li&gt;Test statistic: &lt;span class=&#34;math display&#34;&gt;\[z_{obs} = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}}}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \frac{0.02-0.06-0}{0.632} = -0.063\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Critical value: &lt;span class=&#34;math inline&#34;&gt;\(\pm z_{\alpha / 2} = \pm z_{0.025} = \pm 1.96\)&lt;/span&gt; (see a guide on &lt;a href=&#34;/blog/a-guide-on-how-to-read-statistical-tables&#34;&gt;how to read statistical tables&lt;/a&gt; if you struggle to find the critical value)&lt;/li&gt;
&lt;li&gt;Conclusion: The rejection regions are thus from &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; to -1.96 and from 1.96 to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;. The test statistic is outside the rejection regions so we do not reject the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. In terms of the initial question: At the 5% significance level, we do not reject the hypothesis that the two populations are the same, or there is no sufficient evidence in the data to conclude that the two populations considered are different.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-2-independent-samples-with-2-equal-but-unknown-variances&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 2: Independent samples with 2 equal but unknown variances&lt;/h2&gt;
&lt;p&gt;For the second scenario, suppose the data below. Moreover, suppose that the two samples are independent, that the variances in both populations are unknown but equal (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_1 = \sigma^2_1\)&lt;/span&gt;) and that we would like to test whether population 1 is larger than population 2.&lt;/p&gt;
&lt;table style=&#34;width:24%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;value&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;sample&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.78&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;6 observations in sample 1: &lt;span class=&#34;math inline&#34;&gt;\(n_1 = 6\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;5 observations in sample 2: &lt;span class=&#34;math inline&#34;&gt;\(n_2 = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of sample 1: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_1 = 1.247\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of sample 2: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_2 = 0.1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variance of sample 1: &lt;span class=&#34;math inline&#34;&gt;\(s^2_1 = 0.303\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variance of sample 2: &lt;span class=&#34;math inline&#34;&gt;\(s^2_1 = 0.315\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the 4 steps of hypothesis testing we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu_1 = \mu_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_1 - \mu_2 &amp;gt; 0\)&lt;/span&gt;. (&amp;gt; because we want to test if the mean of the first population is larger than the mean of the second population.)&lt;/li&gt;
&lt;li&gt;Test statistic: &lt;span class=&#34;math display&#34;&gt;\[t_{obs} = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[s_p = \sqrt{\frac{(n_1-1)s^2_1+ (n_2 - 1)s^2_2}{n_1 + n_2 - 2}} = 0.555\]&lt;/span&gt; so &lt;span class=&#34;math display&#34;&gt;\[t_{obs} = \frac{1.247-0.1-0}{0.555 * 0.606} = 3.411\]&lt;/span&gt;
(Note that as it is assumed the variances of the two populations are equal, a pooled (common) variance, denoted &lt;span class=&#34;math inline&#34;&gt;\(s_p\)&lt;/span&gt;, is computed.)&lt;/li&gt;
&lt;li&gt;Critical value: &lt;span class=&#34;math inline&#34;&gt;\(t_{\alpha, n_1 + n_2 - 2} = t_{0.05, 9} = 1.833\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Conclusion: The rejection region is thus from 1.833 to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt; (there is only one rejection region because it is a one-sided test). The test statistic lies within the rejection region so we reject the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. In terms of the initial question: At the 5% significance level, we conclude that the population 1 is larger than the population 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-3-independent-samples-with-2-unequal-and-unknown-variances&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 3: Independent samples with 2 unequal and unknown variances&lt;/h2&gt;
&lt;p&gt;For the third scenario, suppose the data below. Moreover, suppose that the two samples are independent, that the variances in both populations are unknown and unequal (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_1 \ne \sigma^2_1\)&lt;/span&gt;) and that we would like to test whether population 1 is smaller than population 2.&lt;/p&gt;
&lt;table style=&#34;width:24%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;value&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;sample&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.78&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;5 observations in sample 1: &lt;span class=&#34;math inline&#34;&gt;\(n_1 = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 observations in sample 2: &lt;span class=&#34;math inline&#34;&gt;\(n_2 = 6\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of sample 1: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_1 = 0.42\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of sample 2: &lt;span class=&#34;math inline&#34;&gt;\(\bar{x}_2 = 1.247\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variance of sample 1: &lt;span class=&#34;math inline&#34;&gt;\(s^2_1 = 0.107\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variance of sample 2: &lt;span class=&#34;math inline&#34;&gt;\(s^2_1 = 0.303\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the 4 steps of hypothesis testing we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu_1 = \mu_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_1 - \mu_2 &amp;lt; 0\)&lt;/span&gt;. (&amp;lt; because we want to test if the mean of the first population is smaller than the mean of the second population.)&lt;/li&gt;
&lt;li&gt;Test statistic: &lt;span class=&#34;math display&#34;&gt;\[t_{obs} = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{\sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}}}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \frac{0.42-1.247-0}{0.268} = -3.084\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Critical value: &lt;span class=&#34;math inline&#34;&gt;\(-t_{\alpha, \upsilon}\)&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[\upsilon = \frac{\bigg(\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2} \bigg)^2}{\frac{\bigg(\frac{s^2_1}{n_1}\bigg)^2}{n_1 - 1} + \frac{\bigg(\frac{s^2_2}{n_2}\bigg)^2}{n_2 - 1}} = 8.28\]&lt;/span&gt; so &lt;span class=&#34;math display&#34;&gt;\[-t_{0.05, 8.28} = -1.851\]&lt;/span&gt; The degrees of freedom 8.28 does not exist in the standard Student distribution table, so simply take 8, or compute it in R with &lt;code&gt;qt(p = 0.05, df = 8.28)&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Conclusion: The rejection region is thus from &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; to -1.851. The test statistic lies within the rejection region so we reject the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. In terms of the initial question: At the 5% significance level, we conclude that the population 1 is smaller than the population 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-4-paired-samples-where-the-variance-of-the-differences-is-known&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 4: Paired samples where the variance of the differences is known&lt;/h2&gt;
&lt;p&gt;Student’s t-test with paired samples are a bit different than with independent samples, they are actually more similar to &lt;a href=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r-test-on-one-mean/&#34;&gt;one sample Student’s t-test&lt;/a&gt;. Here is how it works. We actually compute the difference between the two samples for each pair of observations, and then we work on these differences as if we were doing a one sample Student’s t-test by computing the test statistic on these differences.&lt;/p&gt;
&lt;p&gt;In case it is not clear, here is the fourth scenario as an illustration. Suppose the data below. Moreover, suppose that the two samples are dependent (matched), that the variance of the differences in the population is known and equal to 1 (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_D = 1\)&lt;/span&gt;) and that we would like to test whether the difference in the population is different than 0.&lt;/p&gt;
&lt;table style=&#34;width:25%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;before&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;after&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The first thing to do is to compute the differences for all pairs of observations:&lt;/p&gt;
&lt;table style=&#34;width:42%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;before&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;after&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;difference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;-0.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of pairs: &lt;span class=&#34;math inline&#34;&gt;\(n = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of the difference: &lt;span class=&#34;math inline&#34;&gt;\(\bar{D} = 0.04\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variance of the difference in the population: &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_D = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;standard deviation of the difference in the population: &lt;span class=&#34;math inline&#34;&gt;\(\sigma_D = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the 4 steps of hypothesis testing we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu_D = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_D \ne 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Test statistic: &lt;span class=&#34;math display&#34;&gt;\[z_{obs} = \frac{\bar{D} - \mu_0}{\frac{\sigma_D}{\sqrt{n}}} = \frac{0.04-0}{0.447} = 0.089\]&lt;/span&gt;
(This formula is exactly the same than for one sample Student’s t-test with a known variance, except that we work on the mean of the differences.)&lt;/li&gt;
&lt;li&gt;Critical value: &lt;span class=&#34;math inline&#34;&gt;\(\pm z_{\alpha/2} = \pm z_{0.025} = \pm 1.96\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Conclusion: The rejection regions are thus from &lt;span class=&#34;math inline&#34;&gt;\(-\infty\)&lt;/span&gt; to -1.96 and from 1.96 to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;. The test statistic is outside the rejection regions so we do not reject the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. In terms of the initial question: At the 5% significance level, we do not reject the hypothesis that the difference in the two populations is equal to 0.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-5-paired-samples-where-the-variance-of-the-differences-is-unknown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 5: Paired samples where the variance of the differences is unknown&lt;/h2&gt;
&lt;p&gt;For the fifth and final scenario, suppose the data below. Moreover, suppose that the two samples are dependent (matched), that the variance of the differences in the population is unknown and that we would like to test whether a treatment is effective in increasing running capabilities (the higher the value, the better in terms of running capabilities).&lt;/p&gt;
&lt;table style=&#34;width:25%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;before&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;after&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The first thing to do is to compute the differences for all pairs of observations:&lt;/p&gt;
&lt;table style=&#34;width:42%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;before&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;after&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;difference&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;number of pairs: &lt;span class=&#34;math inline&#34;&gt;\(n = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;mean of the difference: &lt;span class=&#34;math inline&#34;&gt;\(\bar{D} = 8\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;variance of the difference in the sample: &lt;span class=&#34;math inline&#34;&gt;\(s^2_D = 16\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;standard deviation of the difference in the sample: &lt;span class=&#34;math inline&#34;&gt;\(s_D = 4\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the 4 steps of hypothesis testing we have:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu_D = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_D &amp;gt; 0\)&lt;/span&gt; (&amp;gt; because we would like to test whether the treatment is effective, so whether the treatment has a positive impact on the running capabilities.)&lt;/li&gt;
&lt;li&gt;Test statistic: &lt;span class=&#34;math display&#34;&gt;\[t_{obs} = \frac{\bar{D} - \mu_0}{\frac{s_D}{\sqrt{n}}} = \frac{8-0}{1.789} = 4.472\]&lt;/span&gt;
(This formula is exactly the same than for one sample Student’s t-test with an unknown variance, except that we work on the mean of the differences.)&lt;/li&gt;
&lt;li&gt;Critical value: &lt;span class=&#34;math inline&#34;&gt;\(t_{\alpha, n-1} = t_{0.05, 4} = 2.132\)&lt;/span&gt; (&lt;em&gt;n&lt;/em&gt; is the number of pairs, not the number of observations!)&lt;/li&gt;
&lt;li&gt;Conclusion: The rejection regions are thus from 2.132 to &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;. The test statistic lies within the rejection region so we reject the null hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;. In terms of the initial question: At the 5% significance level, we conclude that the treatment has a positive impact on the running capabilities.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This concludes how to perform the different versions of the Student’s t-test for two samples by hand. In the next sections, we detail how to perform the exact same tests in R.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-compute-students-t-test-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to compute Student’s t-test in R?&lt;/h1&gt;
&lt;p&gt;A good practice before doing t-tests in R is to visualize the data by group thanks to a boxplot (or a &lt;a href=&#34;/blog/descriptive-statistics-in-r/#density-plot&#34;&gt;density plot&lt;/a&gt;, or eventually both). A boxplot with the two boxes overlapping each other gives a first indication that the two samples are similar, and thus, that the null hypothesis of equal means may not be rejected. On the contrary, if the two boxes are not overlapping, it indicates that the two samples are not similar, and thus, that the populations may be different. However, even if boxplots or density plots are great in showing a comparison between the two groups, only a sound statistical test will confirm our first impression.&lt;/p&gt;
&lt;p&gt;After a visualization of the data by group, we replicate in R the results found by hand. We will see that for some versions of the t-test, there is no default function built in R (at least to my knowledge, do not hesitate to let me know if I’m mistaken). In these cases, a function is written to replicate the results by hand.&lt;/p&gt;
&lt;p&gt;Note that we use the same data, the same assumptions and the same question for all 5 scenarios to facilitate the comparison between the tests performed by hand and in R.&lt;/p&gt;
&lt;div id=&#34;scenario-1-independent-samples-with-2-known-variances-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 1: Independent samples with 2 known variances&lt;/h2&gt;
&lt;p&gt;For the first scenario, suppose the data below. Moreover, suppose that the two samples are independent, that the variances &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 1\)&lt;/span&gt; in both populations and that we would like to test whether the two populations are different.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat1 &amp;lt;- data.frame(
  sample1 = c(0.9, -0.8, 0.1, -0.3, 0.2),
  sample2 = c(0.8, -0.9, -0.1, 0.4, 0.1)
)
dat1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   sample1 sample2
## 1     0.9     0.8
## 2    -0.8    -0.9
## 3     0.1    -0.1
## 4    -0.3     0.4
## 5     0.2     0.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_ggplot &amp;lt;- data.frame(
  value = c(0.9, -0.8, 0.1, -0.3, 0.2, 0.8, -0.9, -0.1, 0.4, 0.1),
  sample = c(rep(&amp;quot;1&amp;quot;, 5), rep(&amp;quot;2&amp;quot;, 5))
)

library(ggplot2)

ggplot(dat_ggplot) +
  aes(x = sample, y = value) +
  geom_boxplot() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that you can use the &lt;a href=&#34;/blog/rstudio-addins-or-how-to-make-your-coding-life-easier/#esquisse&#34;&gt;&lt;code&gt;{esquisse}&lt;/code&gt; RStudio addin&lt;/a&gt; if you want to draw a boxplot with the package &lt;code&gt;{ggplot2}&lt;/code&gt; without writing the code yourself. If you prefer the default graphics, use the &lt;code&gt;boxplot()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(value ~ sample,
  data = dat_ggplot
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The two boxes seem to overlap which illustrate that the two samples are quite similar, so we tend to believe that we will not be able to reject the null hypothesis that the two populations are similar. However, only a formal statistical test will confirm this belief.&lt;/p&gt;
&lt;p&gt;Since there is no function in R to perform a t-test with known variances, here is one with arguments accepting the two samples (&lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;), the two variances of the populations (&lt;code&gt;V1&lt;/code&gt; and &lt;code&gt;V2&lt;/code&gt;), the difference in means under the null hypothesis (&lt;code&gt;m0&lt;/code&gt;, default is &lt;code&gt;0&lt;/code&gt;), the significance level (&lt;code&gt;alpha&lt;/code&gt;, default is &lt;code&gt;0.05&lt;/code&gt;) and the alternative (&lt;code&gt;alternative&lt;/code&gt;, one of &lt;code&gt;&amp;quot;two.sided&amp;quot;&lt;/code&gt; (default), &lt;code&gt;&amp;quot;less&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;greater&amp;quot;&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test_knownvar &amp;lt;- function(x, y, V1, V2, m0 = 0, alpha = 0.05, alternative = &amp;quot;two.sided&amp;quot;) {
  M1 &amp;lt;- mean(x)
  M2 &amp;lt;- mean(y)
  n1 &amp;lt;- length(x)
  n2 &amp;lt;- length(y)
  sigma1 &amp;lt;- sqrt(V1)
  sigma2 &amp;lt;- sqrt(V2)
  S &amp;lt;- sqrt((V1 / n1) + (V2 / n2))
  statistic &amp;lt;- (M1 - M2 - m0) / S
  p &amp;lt;- if (alternative == &amp;quot;two.sided&amp;quot;) {
    2 * pnorm(abs(statistic), lower.tail = FALSE)
  } else if (alternative == &amp;quot;less&amp;quot;) {
    pnorm(statistic, lower.tail = TRUE)
  } else {
    pnorm(statistic, lower.tail = FALSE)
  }
  LCL &amp;lt;- (M1 - M2 - S * qnorm(1 - alpha / 2))
  UCL &amp;lt;- (M1 - M2 + S * qnorm(1 - alpha / 2))
  value &amp;lt;- list(mean1 = M1, mean2 = M2, m0 = m0, sigma1 = sigma1, sigma2 = sigma2, S = S, statistic = statistic, p.value = p, LCL = LCL, UCL = UCL, alternative = alternative)
  # print(sprintf(&amp;quot;P-value = %g&amp;quot;,p))
  # print(sprintf(&amp;quot;Lower %.2f%% Confidence Limit = %g&amp;quot;,
  #               alpha, LCL))
  # print(sprintf(&amp;quot;Upper %.2f%% Confidence Limit = %g&amp;quot;,
  #               alpha, UCL))
  return(value)
}

test &amp;lt;- t.test_knownvar(dat1$sample1, dat1$sample2,
  V1 = 1, V2 = 1
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $mean1
## [1] 0.02
## 
## $mean2
## [1] 0.06
## 
## $m0
## [1] 0
## 
## $sigma1
## [1] 1
## 
## $sigma2
## [1] 1
## 
## $S
## [1] 0.6324555
## 
## $statistic
## [1] -0.06324555
## 
## $p.value
## [1] 0.949571
## 
## $LCL
## [1] -1.27959
## 
## $UCL
## [1] 1.19959
## 
## $alternative
## [1] &amp;quot;two.sided&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above recaps all the information needed to perform the test: the test statistic, the &lt;em&gt;p&lt;/em&gt;-value, the alternative used, the two sample means and the two variances of the populations (compare these results found in R with the results found by hand).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value can be extracted as usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.949571&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value is 0.95 so at the 5% significance level we do not reject the null hypothesis of equal means. There is no sufficient evidence in the data to reject the hypothesis that the two means in the populations are similar. This result confirms what we found by hand.&lt;/p&gt;
&lt;div id=&#34;a-note-on-p-value-and-significance-level-alpha&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A note on &lt;em&gt;p&lt;/em&gt;-value and significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;For those unfamiliar with the concept of &lt;em&gt;p&lt;/em&gt;-value, the &lt;em&gt;p&lt;/em&gt;-value is a probability and as any probability it goes from 0 to 1. The &lt;strong&gt;&lt;em&gt;p&lt;/em&gt;-value is the probability of having observations as extreme as what we measured (via the samples) if the null hypothesis was true&lt;/strong&gt;. In other words, it is the probability of having a test statistic as extreme as what we computed, given that the null hypothesis is true. If the observations are not so extreme, i.e., not unlikely to occur if the null hypothesis was true, we do not reject this null hypothesis because it is deemed plausible to be true. And if the observations are considered too extreme, i.e., too unlikely to happen under the null hypothesis, we reject the null hypothesis because it is deemed too implausible to be true. Note that it does not mean that we are 100% sure that it is too unlikely, it happens sometimes that the null hypothesis is rejected although it is true (see the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; later on).&lt;/p&gt;
&lt;p&gt;In our example above, the observations are not really extreme and the difference between the two means is not extreme, so the test statistic is not extreme (since the test statistic is partially based on the difference of the means of the two samples). Having a test statistic which is not extreme is not unlikely and that is the reason why the &lt;em&gt;p&lt;/em&gt;-value is quite high. The &lt;em&gt;p&lt;/em&gt;-value of 0.95 actually tells us that the probability of having two samples with a difference in means of -0.04 (= 0.02 - 0.06), given that the difference in means in the populations is 0 (the null hypothesis), equals 95%. A probability of 95% is definitely considered as plausible, so we do not reject the null hypothesis of equal means in the populations.&lt;/p&gt;
&lt;p&gt;One may then wonder, “What is too extreme for a test statistic?” Most of the time, we consider that a test statistic is too extreme to happen just by chance when the probability of having such an extreme test statistic given that the null hypothesis is true is below 5%. The threshold of 5% (&lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;) that you very often see in statistic courses or textbooks is the threshold used in many fields. With a &lt;em&gt;p&lt;/em&gt;-value under that threshold of 5%, we consider that the observations (and thus the test statistic) is &lt;strong&gt;too unlikely&lt;/strong&gt; to happen just by chance if the null hypothesis was true, so the null hypothesis is rejected. With a &lt;em&gt;p&lt;/em&gt;-value above that threshold of 5%, we consider that it is not really implausible to face the observations we have if the null hypothesis was true, and we therefore do not reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;Note that I wrote “we do not reject the null hypothesis”, and not “we accept the null hypothesis”. This is because it may be the case that the null hypothesis is in fact false, but we failed to prove it with the samples. Suppose the analogy of a suspect accused of murder and we do not know the truth. On the one hand, if we have collected enough evidence that the suspect committed the murder, he is considered guilty: we reject the null hypothesis that he is innocent. On the other hand, if we have &lt;em&gt;not&lt;/em&gt; collected enough evidence against the suspect, he is presumed to be innocent although he may in fact have committed the crime: we failed to reject the null hypothesis of him being innocent. We are never sure that he did not committed the crime even if he is released, we just did not find sufficient evidence against the null hypothesis of the suspect being innocent. This is the reason why we do not reject the null hypothesis instead of accepting it, and why you will often read things like “there is no sufficient evidence in the data to reject the null hypothesis” or “based on the samples we fail to reject the null hypothesis”.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;&lt;/strong&gt;, derived from the threshold of 5% mentioned earlier, &lt;strong&gt;is the probability of rejecting the null hypothesis when it is in fact true&lt;/strong&gt;. In this sense, it is an error (of 5%) that we accept to deal with, in order to be able to draw conclusions. If we would accept no error (an error of 0%), we would not be able to draw any conclusion about the population(s) since we only have access to a limited portion of the population(s) via the sample(s). As a consequence, we will never be 100% sure when interpreting the result of a hypothesis test unless we have access to the data for the entire population, but then there is no reason to do a hypothesis test anymore since we can simply compare the two populations. We usually allow this error (called Type I error) to be 5%, but in order to be a bit more certain when concluding that we reject the null hypothesis, the alpha level can also be set to 1% (or even to 0.1% in some rare cases).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To sum up&lt;/strong&gt; what you need to remember about &lt;em&gt;p&lt;/em&gt;-value and significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the &lt;em&gt;p&lt;/em&gt;-value is smaller than the predetermined significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (usually 5%) so if &lt;em&gt;p&lt;/em&gt;-value &amp;lt; 0.05, we reject the null hypothesis&lt;/li&gt;
&lt;li&gt;If the &lt;em&gt;p&lt;/em&gt;-value is greater than or equal to the predetermined significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (usually 5%) so if &lt;em&gt;p&lt;/em&gt;-value &lt;span class=&#34;math inline&#34;&gt;\(\ge\)&lt;/span&gt; 0.05, we do &lt;strong&gt;not reject&lt;/strong&gt; the null hypothesis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This applies to all statistical tests without exception. Of course, the null and alternative hypotheses change depending on the test.&lt;/p&gt;
&lt;p&gt;A rule of thumb is that, for most hypothesis tests, the alternative hypothesis is what you want to test and the null hypothesis is the status quo. Take this with extreme caution (!) because, even if it works for all versions of the Student’s t-test it does not apply to ALL statistical tests. For example, when testing for normality, you usually want to test whether your distribution follows a normal distribution. Following this piece of advice, you would write the alternative hypothesis &lt;span class=&#34;math inline&#34;&gt;\(H_1:\)&lt;/span&gt; the distribution follows a normal distribution. Nonetheless, for &lt;a href=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/#normality-test&#34;&gt;normality tests&lt;/a&gt; such as the Shapiro-Wilk or Kolmogorov-Smirnov test, it is the opposite; the alternative hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(H_1:\)&lt;/span&gt; the distribution does not follow a normal distribution. So for every test, make sure to use the correct hypotheses, otherwise the conclusion and interpretation of your test will be wrong.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-2-independent-samples-with-2-equal-but-unknown-variances-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 2: Independent samples with 2 equal but unknown variances&lt;/h2&gt;
&lt;p&gt;For the second scenario, suppose the data below. Moreover, suppose that the two samples are independent, that the variances in both populations are unknown but equal (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_1 = \sigma^2_1\)&lt;/span&gt;) and that we would like to test whether population 1 is larger than population 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat2 &amp;lt;- data.frame(
  sample1 = c(1.78, 1.5, 0.9, 0.6, 0.8, 1.9),
  sample2 = c(0.8, -0.7, -0.1, 0.4, 0.1, NA)
)
dat2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   sample1 sample2
## 1    1.78     0.8
## 2    1.50    -0.7
## 3    0.90    -0.1
## 4    0.60     0.4
## 5    0.80     0.1
## 6    1.90      NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat_ggplot &amp;lt;- data.frame(
  value = c(1.78, 1.5, 0.9, 0.6, 0.8, 1.9, 0.8, -0.7, -0.1, 0.4, 0.1),
  sample = c(rep(&amp;quot;1&amp;quot;, 6), rep(&amp;quot;2&amp;quot;, 5))
)

ggplot(dat_ggplot) +
  aes(x = sample, y = value) +
  geom_boxplot() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unlike the previous scenario, the two boxes do not overlap which illustrates that the two samples are different from each other. From this boxplot, we can expect the test to reject the null hypothesis of equal means in the populations. Nonetheless, only a formal statistical test will confirm this expectation.&lt;/p&gt;
&lt;p&gt;There is a function in R, and it is simply the &lt;code&gt;t.test()&lt;/code&gt; function. This version of the test is actually the “standard” Student’s t-test for two samples. Note that it is assumed that the variances of the two populations are equal so we need to specify it in the function with the argument &lt;code&gt;var.equal = TRUE&lt;/code&gt; (the default is &lt;code&gt;FALSE&lt;/code&gt;) and the alternative hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_1 - \mu_2 &amp;gt; 0\)&lt;/span&gt; so we need to add the argument &lt;code&gt;alternative = &amp;quot;greater&amp;quot;&lt;/code&gt; as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- t.test(dat2$sample1, dat2$sample2,
  var.equal = TRUE, alternative = &amp;quot;greater&amp;quot;
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  dat2$sample1 and dat2$sample2
## t = 3.4113, df = 9, p-value = 0.003867
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.5304908       Inf
## sample estimates:
## mean of x mean of y 
##  1.246667  0.100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above recaps all the information needed to perform the test: the name of the test, the test statistic, the degrees of freedom, the &lt;em&gt;p&lt;/em&gt;-value, the alternative used and the two sample means (compare these results found in R with the results found by hand).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value can be extracted as usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.003866756&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value is 0.004 so at the 5% significance level we reject the null hypothesis of equal means. This result confirms what we found by hand.&lt;/p&gt;
&lt;p&gt;Unlike the first scenario, the &lt;em&gt;p&lt;/em&gt;-value in this scenario is below 5% so we reject the null hypothesis. At the 5% significance level, we can conclude that the population 1 is larger than the population 2.&lt;/p&gt;
&lt;p&gt;If your data is formatted in the long format (which is even better), simply use the &lt;code&gt;~&lt;/code&gt;. For instance, imagine the exact same data presented like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat2bis &amp;lt;- data.frame(
  value = c(1.78, 1.5, 0.9, 0.6, 0.8, 1.9, 0.8, -0.7, -0.1, 0.4, 0.1),
  sample = c(rep(&amp;quot;1&amp;quot;, 6), rep(&amp;quot;2&amp;quot;, 5))
)
dat2bis&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    value sample
## 1   1.78      1
## 2   1.50      1
## 3   0.90      1
## 4   0.60      1
## 5   0.80      1
## 6   1.90      1
## 7   0.80      2
## 8  -0.70      2
## 9  -0.10      2
## 10  0.40      2
## 11  0.10      2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is how to perform the Student’s t-test in R with long data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- t.test(value ~ sample,
  data = dat2bis,
  var.equal = TRUE,
  alternative = &amp;quot;greater&amp;quot;
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  value by sample
## t = 3.4113, df = 9, p-value = 0.003867
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.5304908       Inf
## sample estimates:
## mean in group 1 mean in group 2 
##        1.246667        0.100000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.003866756&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results are exactly the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-3-independent-samples-with-2-unequal-and-unknown-variances-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 3: Independent samples with 2 unequal and unknown variances&lt;/h2&gt;
&lt;p&gt;For the third scenario, suppose the data below. Moreover, suppose that the two samples are independent, that the variances in both populations are unknown and unequal (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_1 \ne \sigma^2_1\)&lt;/span&gt;) and that we would like to test whether population 1 is smaller than population 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat3 &amp;lt;- data.frame(
  value = c(0.8, 0.7, 0.1, 0.4, 0.1, 1.78, 1.5, 0.9, 0.6, 0.8, 1.9),
  sample = c(rep(&amp;quot;1&amp;quot;, 5), rep(&amp;quot;2&amp;quot;, 6))
)
dat3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    value sample
## 1   0.80      1
## 2   0.70      1
## 3   0.10      1
## 4   0.40      1
## 5   0.10      1
## 6   1.78      2
## 7   1.50      2
## 8   0.90      2
## 9   0.60      2
## 10  0.80      2
## 11  1.90      2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat3) +
  aes(x = sample, y = value) +
  geom_boxplot() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a function in R for this version of the test as well, and it is simply the &lt;code&gt;t.test()&lt;/code&gt; function with the &lt;code&gt;var.equal = FALSE&lt;/code&gt; argument. &lt;code&gt;FALSE&lt;/code&gt; is the default option for the &lt;code&gt;var.equal&lt;/code&gt; argument so you actually do not need to specify it. This version of the test is actually the Welch test, used when the variances of the populations are unknown and unequal. To test if two variances are equal, you can use the Levene’s test (&lt;code&gt;leveneTest(dat3$value, dat3$sample)&lt;/code&gt; from the &lt;code&gt;{car}&lt;/code&gt; package). Note that the alternative hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_1 - \mu_2 &amp;lt; 0\)&lt;/span&gt; so we need to add the argument &lt;code&gt;alternative = &amp;quot;less&amp;quot;&lt;/code&gt; as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- t.test(value ~ sample,
  data = dat3,
  var.equal = FALSE,
  alternative = &amp;quot;less&amp;quot;
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  value by sample
## t = -3.0841, df = 8.2796, p-value = 0.007206
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##        -Inf -0.3304098
## sample estimates:
## mean in group 1 mean in group 2 
##        0.420000        1.246667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above recaps all the information needed to perform the test (compare these results found in R with the results found by hand).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value can be extracted as usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.00720603&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value is 0.007 so at the 5% significance level we reject the null hypothesis of equal means, meaning that we can conclude that the population 1 is smaller than the population 2. This result confirms what we found by hand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-4-paired-samples-where-the-variance-of-the-differences-is-known-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 4: Paired samples where the variance of the differences is known&lt;/h2&gt;
&lt;p&gt;For the fourth scenario, suppose the data below. Moreover, suppose that the two samples are dependent (matched), that the variance of the differences in the population is known and equal to 1 (&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_D = 1\)&lt;/span&gt;) and that we would like to test whether the difference in the population is different than 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat4 &amp;lt;- data.frame(
  before = c(0.9, -0.8, 0.1, -0.3, 0.2),
  after = c(0.8, -0.9, -0.1, 0.4, 0.1)
)
dat4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   before after
## 1    0.9   0.8
## 2   -0.8  -0.9
## 3    0.1  -0.1
## 4   -0.3   0.4
## 5    0.2   0.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat4$difference &amp;lt;- dat4$after - dat4$before

ggplot(dat4) +
  aes(y = difference) +
  geom_boxplot() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since there is no function in R to perform a t-test with paired samples where the variance of the differences is known, here is one with arguments accepting the differences between the two samples (&lt;code&gt;x&lt;/code&gt;), the variance of the differences in the population (&lt;code&gt;V&lt;/code&gt;), the mean of the differences under the null hypothesis (&lt;code&gt;m0&lt;/code&gt;, default is &lt;code&gt;0&lt;/code&gt;), the significance level (&lt;code&gt;alpha&lt;/code&gt;, default is &lt;code&gt;0.05&lt;/code&gt;) and the alternative (&lt;code&gt;alternative&lt;/code&gt;, one of &lt;code&gt;&amp;quot;two.sided&amp;quot;&lt;/code&gt; (default), &lt;code&gt;&amp;quot;less&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;greater&amp;quot;&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test_pairedknownvar &amp;lt;- function(x, V, m0 = 0, alpha = 0.05, alternative = &amp;quot;two.sided&amp;quot;) {
  M &amp;lt;- mean(x)
  n &amp;lt;- length(x)
  sigma &amp;lt;- sqrt(V)
  S &amp;lt;- sqrt(V / n)
  statistic &amp;lt;- (M - m0) / S
  p &amp;lt;- if (alternative == &amp;quot;two.sided&amp;quot;) {
    2 * pnorm(abs(statistic), lower.tail = FALSE)
  } else if (alternative == &amp;quot;less&amp;quot;) {
    pnorm(statistic, lower.tail = TRUE)
  } else {
    pnorm(statistic, lower.tail = FALSE)
  }
  LCL &amp;lt;- (M - S * qnorm(1 - alpha / 2))
  UCL &amp;lt;- (M + S * qnorm(1 - alpha / 2))
  value &amp;lt;- list(mean = M, m0 = m0, sigma = sigma, statistic = statistic, p.value = p, LCL = LCL, UCL = UCL, alternative = alternative)
  # print(sprintf(&amp;quot;P-value = %g&amp;quot;,p))
  # print(sprintf(&amp;quot;Lower %.2f%% Confidence Limit = %g&amp;quot;,
  #               alpha, LCL))
  # print(sprintf(&amp;quot;Upper %.2f%% Confidence Limit = %g&amp;quot;,
  #               alpha, UCL))
  return(value)
}

test &amp;lt;- t.test_pairedknownvar(dat4$after - dat4$before,
  V = 1
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $mean
## [1] 0.04
## 
## $m0
## [1] 0
## 
## $sigma
## [1] 1
## 
## $statistic
## [1] 0.08944272
## 
## $p.value
## [1] 0.9287301
## 
## $LCL
## [1] -0.8365225
## 
## $UCL
## [1] 0.9165225
## 
## $alternative
## [1] &amp;quot;two.sided&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above recaps all the information needed to perform the test (compare these results found in R with the results found by hand).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value can be extracted as usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9287301&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value is 0.929 so at the 5% significance level we do not reject the null hypothesis of the mean of the differences being equal to 0. There is no sufficient evidence in the data to reject the hypothesis that the difference in the two populations is equal to 0. This result confirms what we found by hand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scenario-5-paired-samples-where-the-variance-of-the-differences-is-unknown-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scenario 5: Paired samples where the variance of the differences is unknown&lt;/h2&gt;
&lt;p&gt;For the fifth and final scenario, suppose the data below. Moreover, suppose that the two samples are dependent (matched), that the variance of the differences in the population is unknown and that we would like to test whether a treatment is effective in increasing running capabilities (the higher the value, the better in terms of running capabilities).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat5 &amp;lt;- data.frame(
  before = c(9, 8, 1, 3, 2),
  after = c(16, 11, 15, 12, 9)
)
dat5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   before after
## 1      9    16
## 2      8    11
## 3      1    15
## 4      3    12
## 5      2     9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat5$difference &amp;lt;- dat5$after - dat5$before

ggplot(dat5) +
  aes(y = difference) +
  geom_boxplot() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a function in R for this version of the test, and it is simply the &lt;code&gt;t.test()&lt;/code&gt; function with the &lt;code&gt;paired = TRUE&lt;/code&gt; argument. This version of the test is actually the standard version of the Student’s t-test with paired samples. Note that the alternative hypothesis is &lt;span class=&#34;math inline&#34;&gt;\(H_1: \mu_D &amp;gt; 0\)&lt;/span&gt; so we need to add the argument &lt;code&gt;alternative = &amp;quot;greater&amp;quot;&lt;/code&gt; as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- t.test(dat5$after, dat5$before,
  alternative = &amp;quot;greater&amp;quot;,
  paired = TRUE
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  dat5$after and dat5$before
## t = 4.4721, df = 4, p-value = 0.005528
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  4.186437      Inf
## sample estimates:
## mean of the differences 
##                       8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we wrote &lt;code&gt;after&lt;/code&gt; and then &lt;code&gt;before&lt;/code&gt; in this order. If you write &lt;code&gt;before&lt;/code&gt; and then &lt;code&gt;after&lt;/code&gt;, make sure to change the alternative to &lt;code&gt;alternative = &amp;quot;less&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If your data is in the long format, use the &lt;code&gt;~&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat5 &amp;lt;- data.frame(
  value = c(9, 8, 1, 3, 2, 16, 11, 15, 12, 9),
  time = c(rep(&amp;quot;before&amp;quot;, 5), rep(&amp;quot;after&amp;quot;, 5))
)
dat5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    value   time
## 1      9 before
## 2      8 before
## 3      1 before
## 4      3 before
## 5      2 before
## 6     16  after
## 7     11  after
## 8     15  after
## 9     12  after
## 10     9  after&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- t.test(value ~ time,
  data = dat5,
  alternative = &amp;quot;greater&amp;quot;,
  paired = TRUE
)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Paired t-test
## 
## data:  value by time
## t = 4.4721, df = 4, p-value = 0.005528
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  4.186437      Inf
## sample estimates:
## mean of the differences 
##                       8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output above recaps all the information needed to perform the test (compare these results found in R with the results found by hand).&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value can be extracted as usual:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.005528247&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;p&lt;/em&gt;-value is 0.006 so at the 5% significance level we reject the null hypothesis of the mean of the differences being equal to 0, meaning that we can conclude that the treatment is effective in increasing the running capabilities. This result confirms what we found by hand.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;assumptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Assumptions&lt;/h1&gt;
&lt;p&gt;As for many statistical tests, there are some assumptions that need to be met in order to be able to interpret the results. When one or several assumptions are not met, although it is technically possible to perform these tests, it would be incorrect to interpret the results. Below are the assumptions of the Student’s t-test for two samples, how to test them and which other tests exist if an assumption is not met:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The data, collected from a representative and randomly selected portion of the total population, should be independent. If observations between the two samples are dependent (for example if two measurements have been collected on the &lt;strong&gt;same individual&lt;/strong&gt; as it is often the case in medical studies when measuring a value after and before a treatment), the paired version of the Student’s t-test, called the Student’s t-test for paired samples, should be preferred in order to take into account the dependency between the two groups to be compared.&lt;/li&gt;
&lt;li&gt;When the two samples are independent, observations in &lt;strong&gt;both samples&lt;/strong&gt; should follow a &lt;a href=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r&#34;&gt;&lt;strong&gt;normal distribution&lt;/strong&gt;&lt;/a&gt;. When using the Student’s t-test for paired samples, it is the difference between the observations of the two samples that should follow a normal distribution. The normality assumption can be tested visually thanks to a histogram and a QQ-plot, and/or formally via a normality test such as the Shapiro-Wilk or Kolmogorov-Smirnov test (see more information about the normality assumption and how to test it &lt;a href=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/#how-to-test-the-normality-assumption&#34;&gt;here&lt;/a&gt;). If, even after a transformation (logarithmic transformation, etc.), your data still do not follow a normal distribution, the Wilcoxon test (&lt;code&gt;wilcox.test(variable1 ~ variable2, data = dat&lt;/code&gt; in R) can be applied. This test, robust to non normal distributions, compares the medians instead of the means in order to compare the two populations.&lt;/li&gt;
&lt;li&gt;When the two samples are independent, the variances of the two groups should be equal in the populations (an assumption called homogeneity of the variances, or even sometimes referred as homoscedasticity, as opposed to heteroscedasticity). This assumption can be tested thanks to the Levene’s test (&lt;code&gt;leveneTest(variable ~ group)&lt;/code&gt; from the &lt;code&gt;{car}&lt;/code&gt; package) or via a F test (&lt;code&gt;var.test(variable ~ group)&lt;/code&gt;). If the hypothesis of equal variances is rejected, another version of the Student’s t-test can be used: the Welch test (&lt;code&gt;t.test(variable ~ group, var.equal = FALSE)&lt;/code&gt;). Note that the Welch test does not require homogeneity of the variances, but the distributions should still follow a normal distribution. If your distributions are not normally distributed and the variances are unequal, the Wilcoxon test should be used. This test does not require normality or homoscedasticity of the variances.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This concludes a relatively long article. Thanks for reading it. I hope this article helped you to understand how the different versions of the Student’s t-test for two samples work and how to perform them by hand and in R. If you are interested, here is a &lt;a href=&#34;/blog/a-shiny-app-for-inferential-statistics-by-hand&#34;&gt;Shiny app&lt;/a&gt; to perform these tests by hand easily (you just need to enter your data and select the appropriate version of the test thanks to the sidebar menu). Moreover, read &lt;a href=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r-test-on-one-mean/&#34;&gt;this article&lt;/a&gt; if you would like to know how to compute the Student’s t-test but this time, for one sample.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug (which is quite likely in this article given all the computations), you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-rowntree2000statistics&#34;&gt;
&lt;p&gt;Rowntree, Derek. 2000. &lt;em&gt;Statistics Without Tears&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Remind that inferential statistics, as opposed to &lt;a href=&#34;/blog/descriptive-statistics-by-hand&#34;&gt;descriptive statistics&lt;/a&gt;, is a branch of statistics defined as the science of drawing conclusions about a population from observations made on a representative sample of that population. See the &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample&#34;&gt;difference between population and sample&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For the rest of the present article, when we write Student’s t-test, we refer to the case of 2 samples. See &lt;a href=&#34;/blog/how-to-perform-a-one-sample-t-test-by-hand-and-in-r-test-on-one-mean/&#34;&gt;one sample t-test&lt;/a&gt; if you want to compare only one sample.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;It is a least the case for parametric hypothesis tests. A parametric test means that it is based on a theoretical statistical distribution, which depends on some defined parameters. In the case of the Student’s t-test for two samples, it is based on the Student’s t distribution with a single parameter, the degrees of freedom (&lt;span class=&#34;math inline&#34;&gt;\(df = n_1 + n_2 - 2\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_2\)&lt;/span&gt; are the two sample sizes), or the normal distribution.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Correlogram in R: how to highlight the most correlated variables in a dataset</title>
      <link>/blog/correlogram-in-r-how-to-highlight-the-most-correlated-variables-in-a-dataset/</link>
      <pubDate>Sat, 22 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/correlogram-in-r-how-to-highlight-the-most-correlated-variables-in-a-dataset/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-matrix&#34;&gt;Correlation matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlogram&#34;&gt;Correlogram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-test&#34;&gt;Correlation test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lares-package&#34;&gt;&lt;code&gt;{lares}&lt;/code&gt; package&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#all-possible-correlations&#34;&gt;All possible correlations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-of-one-variable-against-all-others&#34;&gt;Correlation of one variable against all others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/correlogram-in-r-how-to-highlight-correlations-between-variables_files/correlogram-in-r-how-to-highlight-correlations-between-variables.jpeg&#34; alt=&#34;Photo by Pritesh Sudra&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Pritesh Sudra&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Correlation, often computed as part of &lt;a href=&#34;/blog/descriptive-statistics-in-r&#34;&gt;descriptive statistics&lt;/a&gt;, is a statistical tool used to study the relationship between two variables, that is, whether and how strongly couples of variables are associated.&lt;/p&gt;
&lt;p&gt;Correlations are measured between only 2 variables at a time. Therefore, for datasets with many variables, computing correlations can become quite cumbersome and time consuming.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-matrix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation matrix&lt;/h1&gt;
&lt;p&gt;A solution to this problem is to compute correlations and display them in a correlation matrix, which shows correlation coefficients for all possible combinations of two variables in the dataset.&lt;/p&gt;
&lt;p&gt;For example, below is the correlation matrix for the dataset &lt;code&gt;mtcars&lt;/code&gt; (which, as described by the help documentation of R, comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles).&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; For this article, we include only the &lt;a href=&#34;/blog/variable-types-and-examples/#continuous&#34;&gt;continuous&lt;/a&gt; variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- mtcars[, c(1, 3:7)]
round(cor(dat), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        mpg  disp    hp  drat    wt  qsec
## mpg   1.00 -0.85 -0.78  0.68 -0.87  0.42
## disp -0.85  1.00  0.79 -0.71  0.89 -0.43
## hp   -0.78  0.79  1.00 -0.45  0.66 -0.71
## drat  0.68 -0.71 -0.45  1.00 -0.71  0.09
## wt   -0.87  0.89  0.66 -0.71  1.00 -0.17
## qsec  0.42 -0.43 -0.71  0.09 -0.17  1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even after rounding the correlation coefficients to 2 digits, you will conceive that this correlation matrix is not easily and quickly interpretable.&lt;/p&gt;
&lt;p&gt;If you are using &lt;a href=&#34;/blog/getting-started-in-r-markdown&#34;&gt;R Markdown&lt;/a&gt;, you can use the &lt;code&gt;pander()&lt;/code&gt; function from the &lt;code&gt;{pander}&lt;/code&gt; package to make it slightly more readable, but still, we must admit that this table is not optimal when it comes to visualizing correlations between several variables of a dataset, especially for large datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlogram&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlogram&lt;/h1&gt;
&lt;p&gt;To tackle this issue and make it much more insightful, let’s transform the correlation matrix into a correlation plot. A correlation plot (also referred as a correlogram or corrgram in &lt;span class=&#34;citation&#34;&gt;Friendly (2002)&lt;/span&gt;) allows to highlight the variables that are most (positively and negatively) correlated. Below an example with the same dataset presented above:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/correlogram-in-r-how-to-highlight-correlations-between-variables_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The correlogram represents the correlations for all pairs of variables. Positive correlations are displayed in blue and negative correlations in red. The intensity of the color is proportional to the correlation coefficient so the stronger the correlation (i.e., the closer to -1 or 1), the darker the boxes. The color legend on the right hand side of the correlogram shows the correlation coefficients and the corresponding colors.&lt;/p&gt;
&lt;p&gt;As a reminder, a negative correlation implies that the two variables under consideration vary in opposite directions, that is, if one variable increases the other decreases and vice versa. A positive correlation implies that the two variables under consideration vary in the same direction, that is, if one variable increases the other increases and if one variable decreases the other decreases as well. Furthermore, the stronger the correlation, the stronger the association between the two variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation test&lt;/h1&gt;
&lt;p&gt;Finally, a white box in the correlogram indicates that the correlation is not significantly different from 0 at the specified significance level (in this example, at &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 5\)&lt;/span&gt;%) for the couple of variables. A correlation not significantly different from 0 means that there is &lt;strong&gt;no linear&lt;/strong&gt; relationship between the two variables considered (there could be another kind of association, but other than linear).&lt;/p&gt;
&lt;p&gt;To determine whether a specific correlation coefficient is significantly different from 0, a correlation test has been performed. Remind that the null and alternative hypotheses of this test are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\rho = 0\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\rho \ne 0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is the correlation coefficient. The correlation test is based on two factors: the number of observations and the correlation coefficient. The more observations and the stronger the correlation between 2 variables, the more likely it is to reject the null hypothesis of no correlation between these 2 variables.&lt;/p&gt;
&lt;p&gt;In the context of our example, the correlogram above shows that the variables &lt;code&gt;wt&lt;/code&gt; (weight) and &lt;code&gt;hp&lt;/code&gt; (horsepower) are positively correlated, while the variables &lt;code&gt;mpg&lt;/code&gt; (miles per gallon) and &lt;code&gt;wt&lt;/code&gt; (weight) are negatively correlated (both correlations make sense if we think about it). Furthermore, the variables &lt;code&gt;wt&lt;/code&gt; and &lt;code&gt;qsec&lt;/code&gt; are not correlated (indicated by a white box). Even if the correlation coefficient is -0.17 between the 2 variables, the correlation test has shown that we cannot reject the hypothesis of no correlation. This is the reason the box for these two variable is white.&lt;/p&gt;
&lt;p&gt;Although this correlogram presents exactly the same information than the correlation matrix, the correlogram presents a visual representation of the correlation matrix, allowing to quickly scan through it to see which variables are correlated and which are not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Code&lt;/h1&gt;
&lt;p&gt;For those interested to draw this correlogram with their own data, here is the code of the function I adapted based on the &lt;code&gt;corrplot()&lt;/code&gt; function from the &lt;code&gt;{corrplot}&lt;/code&gt; package (thanks again to all contributors of this package):&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/AntoineSoetewey/1fc0fe939336a8b8085e1872e045b48f.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;The main arguments in the &lt;code&gt;corrplot2()&lt;/code&gt; function are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data&lt;/code&gt;: name of your dataset&lt;/li&gt;
&lt;li&gt;&lt;code&gt;method&lt;/code&gt;: the correlation method to be computed, one of “pearson” (default), “kendall”, or “spearman”. As a rule of thumb, if your dataset contains quantitative continuous variables, you can keep the Pearson method, if you have &lt;a href=&#34;/blog/variable-types-and-examples&#34;&gt;qualitative ordinal&lt;/a&gt; variables, the Spearman method is more appropriate&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sig.level&lt;/code&gt;: the significance level for the correlation test, default is 0.05&lt;/li&gt;
&lt;li&gt;&lt;code&gt;order&lt;/code&gt;: order of the variables, one of “original” (default), “AOE” (angular order of the eigenvectors), “FPC” (first principal component order), “hclust” (hierarchical clustering order), “alphabet” (alphabetical order)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;diag&lt;/code&gt;: display the correlation coefficients on the diagonal? The default is &lt;code&gt;FALSE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;type&lt;/code&gt;: display the entire correlation matrix or simply the upper/lower part, one of “upper” (default), “lower”, “full”&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tl.srt&lt;/code&gt;: rotation of the variable labels&lt;/li&gt;
&lt;li&gt;(note that missing values in the dataset are automatically removed)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also play with the arguments of the &lt;code&gt;corrplot2&lt;/code&gt; function and see the results thanks to this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/correlogram/&#34;&gt;R Shiny app&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lares-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;code&gt;{lares}&lt;/code&gt; package&lt;/h1&gt;
&lt;p&gt;Thanks to this article, I discovered the &lt;code&gt;{lares}&lt;/code&gt; package which has really nice features regarding plotting correlations. Another advantage of this package is that it can be used to compute correlations with numerical, logical, categorical and date variables.&lt;/p&gt;
&lt;div id=&#34;all-possible-correlations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;All possible correlations&lt;/h2&gt;
&lt;p&gt;Use the &lt;code&gt;corr_cross()&lt;/code&gt; function if you want to compute all correlations and return the highest and significant ones in a plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# devtools::install_github(&amp;quot;laresbernardo/lares&amp;quot;)
library(lares)

corr_cross(dat, # name of dataset
  max_pvalue = 0.05, # display only significant correlations (at 5% level)
  top = 10 # display top 10 couples of variables (by correlation coefficient)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/correlogram-in-r-how-to-highlight-correlations-between-variables_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Negative correlations are represented in red and positive correlations in blue.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-of-one-variable-against-all-others&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlation of one variable against all others&lt;/h2&gt;
&lt;p&gt;Use the &lt;code&gt;corr_var()&lt;/code&gt; function if you want to focus on the correlation of one variable against all others, and return the highest ones in a plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corr_var(dat, # name of dataset
  mpg, # name of variable to focus on
  top = 5 # display top 5 correlations
) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/correlogram-in-r-how-to-highlight-correlations-between-variables_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article will help you to visualize correlations between variables in a dataset and to make correlation matrices more insightful and more appealing.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-friendly2002corrgrams&#34;&gt;
&lt;p&gt;Friendly, Michael. 2002. “Corrgrams: Exploratory Displays for Correlation Matrices.” &lt;em&gt;The American Statistician&lt;/em&gt; 56 (4). Taylor &amp;amp; Francis: 316–24.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The dataset &lt;code&gt;mtcars&lt;/code&gt; is preloaded in R by default, so there is no need to import it into R. Check the article “&lt;a href=&#34;/blog/how-to-import-an-excel-file-in-rstudio&#34;&gt;How to import an Excel file in R&lt;/a&gt;” if you need help in importing your own dataset.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The complete guide to clustering analysis: k-means and hierarchical clustering by hand and in R</title>
      <link>/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-clustering-analysis&#34;&gt;What is clustering analysis?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#application-1-computing-distances&#34;&gt;Application 1: Computing distances&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#solution&#34;&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#application-2-k-means-clustering&#34;&gt;Application 2: &lt;em&gt;k&lt;/em&gt;-means clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kmeans-with-2-groups&#34;&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 2 groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quality-of-a-k-means-partition&#34;&gt;Quality of a &lt;em&gt;k&lt;/em&gt;-means partition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nstart-for-several-initial-centers-and-better-stability&#34;&gt;&lt;code&gt;nstart&lt;/code&gt; for several initial centers and better stability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kmeans-with-3-groups&#34;&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 3 groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#manual-application-and-verification-in-r&#34;&gt;Manual application and verification in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#solution-by-hand&#34;&gt;Solution by hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solution-in-r&#34;&gt;Solution in R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-clustering&#34;&gt;Hierarchical clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#application-3-hierarchical-clustering&#34;&gt;Application 3: hierarchical clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-1&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solution-by-hand-1&#34;&gt;Solution by hand&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#single-linkage&#34;&gt;Single linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complete-linkage&#34;&gt;Complete linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-linkage&#34;&gt;Average linkage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solution-in-r-1&#34;&gt;Solution in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#single-linkage-1&#34;&gt;Single linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complete-linkage-1&#34;&gt;Complete linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-linkage-1&#34;&gt;Average linkage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-versus-hierarchical-clustering&#34;&gt;&lt;em&gt;k&lt;/em&gt;-means versus hierarchical clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r-statsandr.com.jpeg&#34; alt=&#34;Photo by Nikola Johnny Mirkovic&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Nikola Johnny Mirkovic&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-clustering-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is clustering analysis?&lt;/h1&gt;
&lt;p&gt;Clustering analysis is a form of exploratory data analysis in which observations are divided into different groups that share common characteristics.&lt;/p&gt;
&lt;p&gt;The purpose of cluster analysis (also known as classification) is to construct groups (or classes or &lt;em&gt;clusters&lt;/em&gt;) while ensuring the following property: &lt;strong&gt;within a group&lt;/strong&gt; the observations must be as &lt;strong&gt;similar&lt;/strong&gt; as possible, while observations belonging to &lt;strong&gt;different groups&lt;/strong&gt; must be as &lt;strong&gt;different&lt;/strong&gt; as possible.&lt;/p&gt;
&lt;p&gt;There are two main types of classification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/li&gt;
&lt;li&gt;Hierarchical clustering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first is generally used when the &lt;strong&gt;number of classes is fixed&lt;/strong&gt; in advance, while the second is generally used for an &lt;strong&gt;unknown number of classes&lt;/strong&gt; and helps to determine this optimal number. Both methods are illustrated below through applications by hand and in R. Note that for hierarchical clustering, only the &lt;em&gt;ascending&lt;/em&gt; classification is presented in this article.&lt;/p&gt;
&lt;p&gt;Clustering algorithms use the &lt;strong&gt;distance&lt;/strong&gt; in order to separate observations into different groups. Therefore, before diving into the presentation of the two classification methods, a reminder exercise on how to compute distances between points is presented.&lt;/p&gt;
&lt;div id=&#34;application-1-computing-distances&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application 1: Computing distances&lt;/h2&gt;
&lt;p&gt;Let a data set containing the points &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{a} = (0, 0)&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{b} = (1, 0)&amp;#39;\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{c} = (5, 5)&amp;#39;\)&lt;/span&gt;. Compute the matrix of Euclidean distances between the points by hand and in R.&lt;/p&gt;
&lt;div id=&#34;solution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;The points are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We create the points in R
a &amp;lt;- c(0, 0)
b &amp;lt;- c(1, 0)
c &amp;lt;- c(5, 5)

X &amp;lt;- rbind(a, b, c) # a, b and c are combined per row
colnames(X) &amp;lt;- c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;) # rename columns

X # display the points&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y
## a 0 0
## b 1 0
## c 5 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the Pythagorean theorem, we will remember that the distance between 2 points &lt;span class=&#34;math inline&#34;&gt;\((x_a, y_a)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((x_b, y_b)\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^2\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\)&lt;/span&gt;. So for instance, for the distance between the points &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{b} = (1, 0)&amp;#39;\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{c} = (5, 5)&amp;#39;\)&lt;/span&gt; presented in the statement above, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\sqrt{(x_b - x_c)^2 + (y_b - y_c)^2} = \sqrt{(1-5)^2 + (0-5)^2} = 6.403124
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can proceed similarly for all pairs of points to find the distance matrix by hand. In R, the &lt;code&gt;dist()&lt;/code&gt; function allows you to find the distance of points in a matrix or dataframe in a very simple way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The distance is found using the dist() function:
distance &amp;lt;- dist(X, method = &amp;quot;euclidean&amp;quot;)
distance # display the distance matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          a        b
## b 1.000000         
## c 7.071068 6.403124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the argument &lt;code&gt;method = &amp;quot;euclidean&amp;quot;&lt;/code&gt; is not mandatory because the Euclidean method is the default one.&lt;/p&gt;
&lt;p&gt;The distance matrix resulting from the &lt;code&gt;dist()&lt;/code&gt; function gives the distance between the different points. The Euclidean distance between the points &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{b}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{c}\)&lt;/span&gt; is 6.403124, which corresponds to what we found above via the Pythagorean formula.&lt;/p&gt;
&lt;p&gt;Now that the distance has been presented, let’s see how to perform clustering analysis with the k-means algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;k-means-clustering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/h1&gt;
&lt;p&gt;The first form of classification is the method called &lt;em&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/em&gt; or the mobile center algorithm. As a reminder, this method aims at partitioning &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations into &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; clusters in which each observation belongs to the cluster with the closest average, serving as a prototype of the cluster. It is presented below.&lt;/p&gt;
&lt;div id=&#34;application-2-k-means-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application 2: &lt;em&gt;k&lt;/em&gt;-means clustering&lt;/h2&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;For this exercise, the &lt;code&gt;Eurojobs.csv&lt;/code&gt; database available &lt;a href=&#34;/blog/data/Eurojobs.csv&#34;&gt;here&lt;/a&gt; is used.&lt;/p&gt;
&lt;p&gt;This database contains the percentage of the population employed in different industries in 26 European countries in 1979. It contains 10 variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Country&lt;/code&gt; - the name of the country (identifier)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Agr&lt;/code&gt; - % of workforce employed in agriculture&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Min&lt;/code&gt; - % in mining&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Man&lt;/code&gt; - % in manufacturing&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PS&lt;/code&gt; - % in power supplies industries&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Con&lt;/code&gt; - % in construction&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SI&lt;/code&gt; - % in service industries&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Fin&lt;/code&gt; - % in finance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SPS&lt;/code&gt; - % in social and personal services&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TC&lt;/code&gt; - % in transportation and communications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We first import the dataset. See &lt;a href=&#34;/blog/how-to-import-an-excel-file-in-rstudio&#34;&gt;how to import data into R&lt;/a&gt; if you need a reminder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Import data
Eurojobs &amp;lt;- read.csv(
  file = &amp;quot;data/Eurojobs.csv&amp;quot;,
  sep = &amp;quot;,&amp;quot;, dec = &amp;quot;.&amp;quot;, header = TRUE
)
head(Eurojobs) # head() is used to display only the first 6 observations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Country  Agr Min  Man  PS  Con   SI Fin  SPS  TC
## 1    Belgium  3.3 0.9 27.6 0.9  8.2 19.1 6.2 26.6 7.2
## 2    Denmark  9.2 0.1 21.8 0.6  8.3 14.6 6.5 32.2 7.1
## 3     France 10.8 0.8 27.5 0.9  8.9 16.8 6.0 22.6 5.7
## 4 W. Germany  6.7 1.3 35.8 0.9  7.3 14.4 5.0 22.3 6.1
## 5    Ireland 23.2 1.0 20.7 1.3  7.5 16.8 2.8 20.8 6.1
## 6      Italy 15.9 0.6 27.6 0.5 10.0 18.1 1.6 20.1 5.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that there is a numbering before the first variable &lt;code&gt;Country&lt;/code&gt;. For more clarity, we will replace this numbering by the country. To do this, we add the argument &lt;code&gt;row.names = 1&lt;/code&gt; in the import function &lt;code&gt;read.csv()&lt;/code&gt; to specify that the first column corresponds to the row names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Eurojobs &amp;lt;- read.csv(
  file = &amp;quot;data/Eurojobs.csv&amp;quot;,
  sep = &amp;quot;,&amp;quot;, dec = &amp;quot;.&amp;quot;, header = TRUE, row.names = 1
)
Eurojobs # displays dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Agr Min  Man  PS  Con   SI  Fin  SPS  TC
## Belgium         3.3 0.9 27.6 0.9  8.2 19.1  6.2 26.6 7.2
## Denmark         9.2 0.1 21.8 0.6  8.3 14.6  6.5 32.2 7.1
## France         10.8 0.8 27.5 0.9  8.9 16.8  6.0 22.6 5.7
## W. Germany      6.7 1.3 35.8 0.9  7.3 14.4  5.0 22.3 6.1
## Ireland        23.2 1.0 20.7 1.3  7.5 16.8  2.8 20.8 6.1
## Italy          15.9 0.6 27.6 0.5 10.0 18.1  1.6 20.1 5.7
## Luxembourg      7.7 3.1 30.8 0.8  9.2 18.5  4.6 19.2 6.2
## Netherlands     6.3 0.1 22.5 1.0  9.9 18.0  6.8 28.5 6.8
## United Kingdom  2.7 1.4 30.2 1.4  6.9 16.9  5.7 28.3 6.4
## Austria        12.7 1.1 30.2 1.4  9.0 16.8  4.9 16.8 7.0
## Finland        13.0 0.4 25.9 1.3  7.4 14.7  5.5 24.3 7.6
## Greece         41.4 0.6 17.6 0.6  8.1 11.5  2.4 11.0 6.7
## Norway          9.0 0.5 22.4 0.8  8.6 16.9  4.7 27.6 9.4
## Portugal       27.8 0.3 24.5 0.6  8.4 13.3  2.7 16.7 5.7
## Spain          22.9 0.8 28.5 0.7 11.5  9.7  8.5 11.8 5.5
## Sweden          6.1 0.4 25.9 0.8  7.2 14.4  6.0 32.4 6.8
## Switzerland     7.7 0.2 37.8 0.8  9.5 17.5  5.3 15.4 5.7
## Turkey         66.8 0.7  7.9 0.1  2.8  5.2  1.1 11.9 3.2
## Bulgaria       23.6 1.9 32.3 0.6  7.9  8.0  0.7 18.2 6.7
## Czechoslovakia 16.5 2.9 35.5 1.2  8.7  9.2  0.9 17.9 7.0
## E. Germany      4.2 2.9 41.2 1.3  7.6 11.2  1.2 22.1 8.4
## Hungary        21.7 3.1 29.6 1.9  8.2  9.4  0.9 17.2 8.0
## Poland         31.1 2.5 25.7 0.9  8.4  7.5  0.9 16.1 6.9
## Rumania        34.7 2.1 30.1 0.6  8.7  5.9  1.3 11.7 5.0
## USSR           23.7 1.4 25.8 0.6  9.2  6.1  0.5 23.6 9.3
## Yugoslavia     48.7 1.5 16.8 1.1  4.9  6.4 11.3  5.3 4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(Eurojobs) # displays the number of rows and columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 26  9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a “clean” dataset of 26 observations and 9 variables on which we can base the classification. Note that in this case it is not necessary to standardize the data because they are all expressed in the same unit (in percentage). If this was not the case, we would have had to standardize the data via the &lt;code&gt;scale()&lt;/code&gt; function (do not forget it otherwise your results may be completely different!). The so-called &lt;em&gt;k&lt;/em&gt;-means clustering is done via the &lt;code&gt;kmeans()&lt;/code&gt; function. We apply the classification with 2 classes and then 3 classes as examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kmeans-with-2-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 2 groups&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- kmeans(Eurojobs, centers = 2)

# displays the class determined by
# the model for all observations:
print(model$cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Belgium        Denmark         France     W. Germany        Ireland 
##              1              1              1              1              2 
##          Italy     Luxembourg    Netherlands United Kingdom        Austria 
##              1              1              1              1              1 
##        Finland         Greece         Norway       Portugal          Spain 
##              1              2              1              2              2 
##         Sweden    Switzerland         Turkey       Bulgaria Czechoslovakia 
##              1              1              2              2              1 
##     E. Germany        Hungary         Poland        Rumania           USSR 
##              1              2              2              2              2 
##     Yugoslavia 
##              2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the argument &lt;code&gt;centers = 2&lt;/code&gt; is used to set the number of clusters, determined in advance. In this exercise the number of clusters has been determined arbitrarily. This number of clusters should be determined according to the context and goal of your analysis. Calling &lt;code&gt;print(model$cluster)&lt;/code&gt; or &lt;code&gt;model$cluster&lt;/code&gt; is the same. This output specifies the group (i.e., 1 or 2) to which each country belongs to.&lt;/p&gt;
&lt;p&gt;The cluster for each observation can be stored directly in the dataset as a column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Eurojobs_cluster &amp;lt;- data.frame(Eurojobs,
                               cluster = as.factor(model$cluster))
head(Eurojobs_cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Agr Min  Man  PS  Con   SI Fin  SPS  TC cluster
## Belgium     3.3 0.9 27.6 0.9  8.2 19.1 6.2 26.6 7.2       1
## Denmark     9.2 0.1 21.8 0.6  8.3 14.6 6.5 32.2 7.1       1
## France     10.8 0.8 27.5 0.9  8.9 16.8 6.0 22.6 5.7       1
## W. Germany  6.7 1.3 35.8 0.9  7.3 14.4 5.0 22.3 6.1       1
## Ireland    23.2 1.0 20.7 1.3  7.5 16.8 2.8 20.8 6.1       2
## Italy      15.9 0.6 27.6 0.5 10.0 18.1 1.6 20.1 5.7       1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;quality-of-a-k-means-partition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quality of a &lt;em&gt;k&lt;/em&gt;-means partition&lt;/h3&gt;
&lt;p&gt;The quality of a &lt;em&gt;k&lt;/em&gt;-means partition is found by calculating the percentage of the &lt;em&gt;TSS&lt;/em&gt; “explained” by the partition using the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\dfrac{\operatorname{BSS}}{\operatorname{TSS}} \times 100\%
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;BSS&lt;/em&gt; and &lt;em&gt;TSS&lt;/em&gt; stand for &lt;em&gt;Between Sum of Squares&lt;/em&gt; and &lt;em&gt;Total Sum of Squares&lt;/em&gt;, respectively. The higher the percentage, the better the score (and thus the quality) because it means that &lt;em&gt;BSS&lt;/em&gt; is large and/or &lt;em&gt;WSS&lt;/em&gt; is small.&lt;/p&gt;
&lt;p&gt;Here is how you can check the quality of the partition in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BSS and TSS are extracted from the model and stored
(BSS &amp;lt;- model$betweenss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4823.535&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(TSS &amp;lt;- model$totss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9299.59&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We calculate the quality of the partition
BSS / TSS * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.86826&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The quality of the partition is 51.87%. This value has no real interpretation in absolute terms except that a higher quality means a higher explained percentage. However, it is more insightful when it is compared to the quality of other partitions (with the same number of clusters!) in order to determine the best partition among the ones considered.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nstart-for-several-initial-centers-and-better-stability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;nstart&lt;/code&gt; for several initial centers and better stability&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;k&lt;/em&gt;-means algorithm uses a random set of initial points to arrive at the final classification. Due to the fact that the initial centers are randomly chosen, the same command &lt;code&gt;kmeans(Eurojobs, centers = 2)&lt;/code&gt; may give slightly different results every time it is run, and thus slight differences in the quality of the partitions. The &lt;code&gt;nstart&lt;/code&gt; argument in the &lt;code&gt;kmeans()&lt;/code&gt; function allows to run the algorithm several times with different initial centers, in order to obtain a potentially better partition:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model2 &amp;lt;- kmeans(Eurojobs, centers = 2, nstart = 10)
100 * model2$betweenss / model2$totss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 54.2503&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depending on the initial random choices, this new partition will be better or not compared to the first one. In our example, the partition is better as the quality increased to 54.25%.&lt;/p&gt;
&lt;p&gt;One of the main limitation often cited regarding &lt;em&gt;k&lt;/em&gt;-means is the stability of the results. As the initial centers are randomly chosen, running the same command may yield different results. Adding the &lt;code&gt;nstart&lt;/code&gt; argument in the &lt;code&gt;kmeans()&lt;/code&gt; function limits this issue as it will generate several different initializations and take the most optimal one, leading to a better stability of the classification.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kmeans-with-3-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 3 groups&lt;/h3&gt;
&lt;p&gt;We now perform the &lt;em&gt;k&lt;/em&gt;-means classification with 3 clusters and compute its quality:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model3 &amp;lt;- kmeans(Eurojobs, centers = 3)
BSS3 &amp;lt;- model3$betweenss
TSS3 &amp;lt;- model3$totss
BSS3 / TSS3 * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 74.59455&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can be seen that the classification into three groups allows for a higher explained percentage and a higher quality. This will always be the case: with more classes, the partition will be finer, and the &lt;em&gt;BSS&lt;/em&gt; contribution will be higher. On the other hand, the “model” will be more complex, requiring more classes. In the extreme case where &lt;em&gt;k = n&lt;/em&gt; (each observation is a singleton class), we have &lt;em&gt;BSS = TSS&lt;/em&gt;, but the partition has lost all interest.&lt;/p&gt;
&lt;p&gt;Now that the &lt;em&gt;k&lt;/em&gt;-means clustering has been detailed in R, see how to do the algorithm by hand in the following sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;manual-application-and-verification-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Manual application and verification in R&lt;/h3&gt;
&lt;p&gt;Perform &lt;strong&gt;by hand&lt;/strong&gt; the &lt;em&gt;k&lt;/em&gt;-means algorithm for the points shown in the graph below, with &lt;em&gt;k&lt;/em&gt; = 2 and with the points &lt;em&gt;i&lt;/em&gt; = 5 and &lt;em&gt;i&lt;/em&gt; = 6 as initial centers. Compute the quality of the partition you just found and then &lt;strong&gt;check&lt;/strong&gt; your answers &lt;strong&gt;in R&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Assume that the variables have the same units so there is no need to scale the data.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;solution-by-hand&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Solution by hand&lt;/h4&gt;
&lt;p&gt;Step 1. Here are the coordinates of the 6 points:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
point
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
x
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
y
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
7
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
9
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;And the initial centers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group 1: point 5 with center &lt;em&gt;(9, 7)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Group 2: point 6 with center &lt;em&gt;(6, 8)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 2. Compute the distance matrix point by point with the Pythagorean theorem. Remind that the distance between point &lt;em&gt;a&lt;/em&gt; and point &lt;em&gt;b&lt;/em&gt; is found with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to two decimals):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(dist(X), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4     5
## 2  3.61                        
## 3  5.10  2.24                  
## 4  7.28  5.66  3.61            
## 5  4.47  5.39  7.62 10.82      
## 6  5.10  3.61  5.66  9.22  3.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 3. Based on the distance matrix computed in step 2, we can put each point to its closest group and compute the coordinates of the center.&lt;/p&gt;
&lt;p&gt;We first put each point in its closest group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;point 1 is closer to point 5 than to point 6 because the distance between points 1 and 5 is 4.47 while the distance between points 1 and 6 is 5.10&lt;/li&gt;
&lt;li&gt;point 2 is closer to point 6 than to point 5 because the distance between points 2 and 5 is 5.39 while the distance between points 2 and 6 is 3.61&lt;/li&gt;
&lt;li&gt;point 3 is closer to point 6 than to point 5 because the distance between points 3 and 5 is 7.62 while the distance between points 3 and 6 is 5.66&lt;/li&gt;
&lt;li&gt;point 4 is closer to point 6 than to point 5 because the distance between points 4 and 5 is 10.82 while the distance between points 4 and 6 is 9.22&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that computing the distances between each point and the points 5 and 6 is sufficient. There is no need to compute the distance between the points 1 and 2 for example, as we compare each point to the initial centers (which are points 5 and 6).&lt;/p&gt;
&lt;p&gt;We then compute the coordinates of the centers of the two groups by taking the mean of the coordinates &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group 1 includes the points 5 and 1 with &lt;em&gt;(8, 5)&lt;/em&gt; as center (&lt;span class=&#34;math inline&#34;&gt;\(8 = \frac{9+7}{2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(5 = \frac{7+3}{2}\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Group 2 includes the points 6, 2, 3 and 4 with &lt;em&gt;(3, 4.5)&lt;/em&gt; as center (&lt;span class=&#34;math inline&#34;&gt;\(3 = \frac{6+4+2+0}{4}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(4.5 = \frac{8+5+4+1}{4}\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We thus have:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
center
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5 &amp;amp; 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(8, 5)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
cluster 2
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6, 2, 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
(3, 4.5)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Step 4. We make sure that the allocation is optimal by checking that each point is in the nearest cluster. The distance between a point and the center of a cluster is again computed thanks to the Pythagorean theorem. Thus, we have:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-scde{color:#009901;text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-yi9q{color:#009901;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 2
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
2.24
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4.27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
1.12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6.08
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
1.12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
8.94
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
4.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
2.24
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
3.61
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The minimum distance between the points and the two clusters is colored in green.&lt;/p&gt;
&lt;p&gt;We check that each point is in the correct group (i.e., the closest cluster). According to the distance in the table above, point 6 seems to be closer to the cluster 1 than to the cluster 2. Therefore, the allocation is not optimal and point 6 should be reallocated to cluster 1.&lt;/p&gt;
&lt;p&gt;Step 5. We compute again the centers of the clusters after this reallocation. The centers are found by taking the mean of the coordinates &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; of the points belonging to the cluster. We thus have:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
center
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1, 5 &amp;amp; 6
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(7.33, 6)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
cluster 2
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
2, 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
(2, 3.33)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;where, for instance, 3.33 is simply &lt;span class=&#34;math inline&#34;&gt;\(\frac{5+4+1}{3}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Step 6. Repeat step 4 until the allocation is optimal. If the allocation is optimal, the algorithm stops. In our example we have:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-scde{color:#009901;text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 2
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
3.02
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
3.48
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
2.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5.69
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
0.67
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
8.87
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
3.07
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
1.95
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
7.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
2.4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5.08
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;All points are correctly allocated to its nearest cluster, so the allocation is optimal and the algorithm stops.&lt;/p&gt;
&lt;p&gt;Step 7. State the final partition and the centers. In our example:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
center
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1, 5 &amp;amp; 6
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(7.33, 6)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2, 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(2, 3.33)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Now that we have the clusters and the final centers, we compute the quality of the partition we just found. Remember that we need to compute the BSS and TSS to find the quality. Below the steps to compute the quality of this partition by &lt;em&gt;k&lt;/em&gt;-means, based on this summary table:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-0a7q{border-color:#000000;text-align:left;vertical-align:middle}
.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34; colspan=&#34;3&#34;&gt;
cluster 1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34; colspan=&#34;3&#34;&gt;
cluster 2
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
point
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
x
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
y
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
point
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
x
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
y
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
7
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
9
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
7
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
8
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
mean
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
7.33
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3.33
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Step 1. Compute the overall mean of the &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; coordinates:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\overline{\overline{x}} = \frac{7+4+2+0+9+6+3+5+4+1+7+8}{12} = 4.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 2. Compute TSS and WSS:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TSS = (7-4.67)^2 + (4-4.67)^2 + (2-4.67)^2 + (0-4.67)^2 \\+ (9-4.67)^2 + (6-4.67)^2 + (3-4.67)^2 + (5-4.67)^2 \\ + (4-4.67)^2 + (1-4.67)^2 + (7-4.67)^2 + (8-4.67)^2 = 88.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Regarding WSS, it is splitted between cluster 1 and cluster 2. For cluster 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[WSS[1] = (7-7.33)^2 + (9 - 7.33)^2 + (6 - 7.33)^2 \\ + (3-6)^2 + (7-6)^2 + (8-6)^2 = 18.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For cluster 2:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[WSS[2] = (4-2)^2 + (2-2)^2 + (0-2)^2 \\ + (5-3.33)^2 + (4-3.33)^2 + (1-3.33)^2 = 16.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the total WSS is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[WSS = WSS[1] + WSS[2] = 18.67 + 16.67 = 35.34\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To find the BSS:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[BSS = TSS - WSS = 88.67-35.34 = 53.33\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Finally, the quality of the partition is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Quality = \frac{BSS}{TSS} = \frac{53.33}{88.67} = 0.6014\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the quality of the partition is 60.14%.&lt;/p&gt;
&lt;p&gt;We are now going to verify all these solutions (the partition, the final centers and the quality) in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-in-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Solution in R&lt;/h4&gt;
&lt;p&gt;As you can imagine, the solution in R us much shorter and requires much less computation on the user side. We first need to enter the data as a matrix or dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- matrix(c(7, 3, 4, 5, 2, 4, 0, 1, 9, 7, 6, 8),
  nrow = 6, byrow = TRUE
)
X # display the coordinates of the points&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    7    3
## [2,]    4    5
## [3,]    2    4
## [4,]    0    1
## [5,]    9    7
## [6,]    6    8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now perform the &lt;em&gt;k&lt;/em&gt;-means via the &lt;code&gt;kmeans()&lt;/code&gt; function with the point 5 and 6 as initial centers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# take rows 5 and 6 of the X matrix as initial centers
res.k &amp;lt;- kmeans(X,
  centers = X[c(5, 6), ],
  algorithm = &amp;quot;Lloyd&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike in the previous application with the dataset &lt;code&gt;Eurojobs.csv&lt;/code&gt; where the initial centers are randomly chosen by R, in this second application we want to specify which points are going to be the two initial centers. For this, we need to set &lt;code&gt;centers = X[c(5,6), ]&lt;/code&gt; to indicate that that there are 2 centers, and that they are going to be the points 5 and 6 (see a reminder on &lt;a href=&#34;/blog/data-manipulation-in-r&#34;&gt;how to subset a dataframe&lt;/a&gt; if needed).&lt;/p&gt;
&lt;p&gt;The reason for adding the argument &lt;code&gt;algorithm = &amp;quot;Lloyd&amp;quot;&lt;/code&gt; can be found in the usage of the R function &lt;code&gt;kmeans()&lt;/code&gt;. In fact, there are several variants of the &lt;em&gt;k&lt;/em&gt;-means algorithm. The default choice is the &lt;span class=&#34;citation&#34;&gt;Hartigan and Wong (1979)&lt;/span&gt; version, which is more sophisticated than the basic version detailed in the solution by hand. By using the original version of &lt;span class=&#34;citation&#34;&gt;Lloyd (1982)&lt;/span&gt;, we find the same solution in R and by hand. For more information, you can consult the documentation of the &lt;code&gt;kmeans()&lt;/code&gt; function (via &lt;code&gt;?kmeans&lt;/code&gt; or &lt;code&gt;help(kmeans)&lt;/code&gt;) and read the articles mentioned.&lt;/p&gt;
&lt;p&gt;The solution in R is then found by extracting&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the partition with &lt;code&gt;$cluster&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res.k$cluster&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 2 2 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Points 1, 5 and 6 belong to cluster 1, points 2, 3 and 4 belong to cluster 2.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the coordinates of the final centers with &lt;code&gt;$centers&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We extract the coordinates of the 2 final centers, rounded to 2 decimals
round(res.k$centers, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [,1] [,2]
## 1 7.33 6.00
## 2 2.00 3.33&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;and then the quality of the partition by dividing the BSS to the TSS:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res.k$betweenss / res.k$totss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6015038&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 3 results are equal to what we found by hand (except the quality which is slightly different due to rounding).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-clustering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hierarchical clustering&lt;/h1&gt;
&lt;p&gt;Remind that the difference with the partition by &lt;em&gt;k&lt;/em&gt;-means is that for hierarchical clustering, the number of classes is &lt;strong&gt;not&lt;/strong&gt; specified in advance. Hierarchical clustering will help to determine the optimal number of clusters.&lt;/p&gt;
&lt;p&gt;Before applying hierarchical clustering by hand and in R, let’s see how it is working step by step:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It starts by putting every point in its own cluster, so each cluster is a singleton&lt;/li&gt;
&lt;li&gt;It then merges the 2 points that are closest to each other based on the distances from the distance matrix. The consequence is that there is one less cluster&lt;/li&gt;
&lt;li&gt;It then recalculates the distances between the new and old clusters and save them in a new distance matrix which will be used in the next step&lt;/li&gt;
&lt;li&gt;Finally, steps 1 and 2 are repeated until all clusters are merged into one single cluster including all points.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There exists four main methods to measure the distance between clusters, referred as linkage methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single linkage: computes the minimum distance between clusters before merging them.&lt;/li&gt;
&lt;li&gt;Complete linkage: computes the maximum distance between clusters before merging them.&lt;/li&gt;
&lt;li&gt;Average linkage: computes the average distance between clusters before merging them.&lt;/li&gt;
&lt;li&gt;Centroid linkage: calculates centroids for both clusters, then computes the distance between the two before merging them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the following sections, only the three first linkage methods are presented (first by hand and then the results are verified in R).&lt;/p&gt;
&lt;div id=&#34;application-3-hierarchical-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application 3: hierarchical clustering&lt;/h2&gt;
&lt;div id=&#34;data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;Using the data from the graph and the table below, perform &lt;strong&gt;by hand&lt;/strong&gt; the 3 algorithms (single, complete and average linkage) and draw the dendrograms. Then &lt;strong&gt;check&lt;/strong&gt; your answers &lt;strong&gt;in R&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      V1    V2
## 1  2.03  0.06
## 2 -0.64 -0.10
## 3 -0.42 -0.53
## 4 -0.36  0.07
## 5  1.14  0.37&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Assume that the variables have the same units so there is no need to scale the data.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-by-hand-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solution by hand&lt;/h3&gt;
&lt;p&gt;Step 1. For all 3 algorithms, we first need to compute the distance matrix between the 5 points thanks to the Pythagorean theorem. Remind that the distance between point &lt;em&gt;a&lt;/em&gt; and point &lt;em&gt;b&lt;/em&gt; is found with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to three decimals):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;single-linkage&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Single linkage&lt;/h4&gt;
&lt;p&gt;Step 2. From the distance matrix computed in step 1, we see that the &lt;strong&gt;smallest distance&lt;/strong&gt; = 0.328 between points 2 and 4. 0.328 corresponds to the first height (more on this later when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp;amp; 4, 3 and 5. The new distances between the group 2 &amp;amp; 4 and all other points are now:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
3
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.390
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.520
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.483
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.530
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.801
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To construct this new distance matrix, proceed point by point:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the initial distance matrix (found in step 1), which was 2.520&lt;/li&gt;
&lt;li&gt;same goes for the distance between points 1 and 5 and points 3 and 5; the distances are the same than in the initial distance matrix since the points have not changed&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 4 has changed since points 2 &amp;amp; 4 are now together&lt;/li&gt;
&lt;li&gt;since we are applying the &lt;strong&gt;single linkage&lt;/strong&gt; criterion, the new distance between points 1 and 2 &amp;amp; 4 corresponds to the &lt;strong&gt;minimum distance&lt;/strong&gt; between the distance between points 1 and 2 and the distance between points 1 and 4&lt;/li&gt;
&lt;li&gt;the initial distance between points 1 and 2 is 2.675 and the initial distance between points 1 and 4 is 2.390&lt;/li&gt;
&lt;li&gt;therefore, the minimum distance between these two distances is 2.390&lt;/li&gt;
&lt;li&gt;2.390 is thus the new distance between points 1 and 2 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;we apply the same process for points 3 and 2 &amp;amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The minimum distance between these 2 distances is 0.483 so the new distance between points 3 and 2 &amp;amp; 4 is 0.483&lt;/li&gt;
&lt;li&gt;follow the same process for all other points&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 3. Based on the distance matrix in step 2, the smallest distance is 0.483 between points 3 and 2 &amp;amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp;amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp;amp; 3 &amp;amp; 4. The groups are thus: 1, 2 &amp;amp; 3 &amp;amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.390
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.530
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;points 1 and 5 have not change, so the distance between these two points are the same than in previous step&lt;/li&gt;
&lt;li&gt;from step 2 we see that the distance between points 1 and 2 &amp;amp; 4 is 2.390 and the distance between points 1 and 3 is 2.520&lt;/li&gt;
&lt;li&gt;since we apply the single linkage criterion, we take the minimum distance, which is 2.390&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.390&lt;/li&gt;
&lt;li&gt;same process for points 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp;amp; 5. The groups are thus: 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.530
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the only distance left to compute is the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;from the previous step we see that the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is 2.390 and the distance between points 5 and 2 &amp;amp; 3 &amp;amp; 4 is 1.530&lt;/li&gt;
&lt;li&gt;since we apply the single linkage criterion, we take the minimum distance, which is 1.530&lt;/li&gt;
&lt;li&gt;the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 is thus 1.530&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 5. The final combination of points is the combination of points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4, with a final height of 1.530. Heights are used to draw the dendrogram in the sixth and final step.&lt;/p&gt;
&lt;p&gt;Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the first combination of points was between points 2 and 4, with a height of 0.328&lt;/li&gt;
&lt;li&gt;the second combination was between points 3 and 2 &amp;amp; 4 with a height of 0.483&lt;/li&gt;
&lt;li&gt;the third combination was between points 1 and 5 with a height of 0.942&lt;/li&gt;
&lt;li&gt;the final combination was between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 with a height of 1.530&lt;/li&gt;
&lt;li&gt;this is exactly what is illustrated in the following dendrogram:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In hierarchical clustering, dendrograms are used to show the sequence of combinations of the clusters. The distances of merge between clusters, called heights, are illustrated on the y-axis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complete-linkage&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complete linkage&lt;/h4&gt;
&lt;p&gt;Complete linkage is quite similar to single linkage, except that instead of taking the smallest distance when computing the new distance between points that have been grouped, the &lt;strong&gt;maximum distance&lt;/strong&gt; is taken.&lt;/p&gt;
&lt;p&gt;The steps to perform the hierarchical clustering with the complete linkage (maximum) are detailed below.&lt;/p&gt;
&lt;p&gt;Step 1. Step 1 is exactly the same than for single linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 2. From the distance matrix computed in step 1, we see that the &lt;strong&gt;smallest distance&lt;/strong&gt; = 0.328 between points 2 and 4. It is important to note that even if we apply the complete linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp;amp; 4, 3 and 5. The new distances between the group 2 &amp;amp; 4 and all other points are now:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
3
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.675
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.520
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.603
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.841
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.801
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To construct this new distance matrix, proceed point by point as we did for single linkage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the initial distance matrix (found in step 1), which was 2.520&lt;/li&gt;
&lt;li&gt;same goes for the distance between points 1 and 5 and points 3 and 5; the distances are the same than in the initial distance matrix since the points have not changed&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 4 has changed since points 2 &amp;amp; 4 are now together&lt;/li&gt;
&lt;li&gt;since we are applying the &lt;strong&gt;complete linkage&lt;/strong&gt; criterion, the new distance between points 1 and 2 &amp;amp; 4 corresponds to the &lt;strong&gt;maximum distance&lt;/strong&gt; between the distance between points 1 and 2 and the distance between points 1 and 4&lt;/li&gt;
&lt;li&gt;the initial distance between points 1 and 2 is 2.675 and the initial distance between points 1 and 4 is 2.390&lt;/li&gt;
&lt;li&gt;therefore, the maximum distance between these two distances is 2.675&lt;/li&gt;
&lt;li&gt;2.675 is thus the new distance between points 1 and 2 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;we apply the same process for points 3 and 2 &amp;amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The maximum distance between these 2 distances is 0.603 so the new distance between points 3 and 2 &amp;amp; 4 is 0.603&lt;/li&gt;
&lt;li&gt;follow the same process for all other points&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 3. Based on the distance matrix in step 2, the smallest distance is 0.603 between points 3 and 2 &amp;amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp;amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp;amp; 3 &amp;amp; 4. The groups are thus: 1, 2 &amp;amp; 3 &amp;amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.675
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.841
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;points 1 and 5 have not change, so the distance between these two points are the same than in previous step&lt;/li&gt;
&lt;li&gt;from step 2 we see that the distance between points 1 and 2 &amp;amp; 4 is 2.675 and the distance between points 1 and 3 is 2.520&lt;/li&gt;
&lt;li&gt;since we apply the complete linkage criterion, we take the maximum distance, which is 2.675&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.675&lt;/li&gt;
&lt;li&gt;same process for points 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp;amp; 5. The groups are thus: 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.675
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the only distance left to compute is the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;from the previous step we see that the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is 2.675 and the distance between points 5 and 2 &amp;amp; 3 &amp;amp; 4 is 1.841&lt;/li&gt;
&lt;li&gt;since we apply the complete linkage criterion, we take the maximum distance, which is 2.675&lt;/li&gt;
&lt;li&gt;the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.675&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 5. The final combination of points is the combination of points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4, with a final height of 2.675. Heights are used to draw the dendrogram in the sixth and final step.&lt;/p&gt;
&lt;p&gt;Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the first combination of points was between points 2 and 4, with a height of 0.328&lt;/li&gt;
&lt;li&gt;the second combination was between points 3 and 2 &amp;amp; 4 with a height of 0.603&lt;/li&gt;
&lt;li&gt;the third combination was between points 1 and 5 with a height of 0.942&lt;/li&gt;
&lt;li&gt;the final combination was between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 with a height of 2.675&lt;/li&gt;
&lt;li&gt;this is exactly what is illustrated in the following dendrogram:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-linkage&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Average linkage&lt;/h4&gt;
&lt;p&gt;With the average linkage criterion, it is not the minimum nor the maximum distance that is taken when computing the new distance between points that have been grouped, but it is, as you guessed by now, the &lt;strong&gt;average distance&lt;/strong&gt; between the points.&lt;/p&gt;
&lt;p&gt;The steps to perform the hierarchical clustering with the average linkage are detailed below.&lt;/p&gt;
&lt;p&gt;Step 1. Step 1 is exactly the same than for single and complete linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 2. From the distance matrix computed in step 1, we see that the &lt;strong&gt;smallest distance&lt;/strong&gt; = 0.328 between points 2 and 4. It is important to note that even if we apply the average linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp;amp; 4, 3 and 5. The new distances between the group 2 &amp;amp; 4 and all other points are now:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
3
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.5325
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.520
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.543
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.6855
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.801
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To construct this new distance matrix, proceed point by point as we did for the two previous criteria:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the initial distance matrix (found in step 1), which was 2.520&lt;/li&gt;
&lt;li&gt;same goes for the distance between points 1 and 5 and points 3 and 5; the distances are the same than in the initial distance matrix since the points have not changed&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 4 has changed since points 2 &amp;amp; 4 are now together&lt;/li&gt;
&lt;li&gt;since we are applying the &lt;strong&gt;average linkage&lt;/strong&gt; criterion, the new distance between points 1 and 2 &amp;amp; 4 corresponds to the &lt;strong&gt;average distance&lt;/strong&gt; between the distance between points 1 and 2 and the distance between points 1 and 4&lt;/li&gt;
&lt;li&gt;the initial distance between points 1 and 2 is 2.675 and the initial distance between points 1 and 4 is 2.390&lt;/li&gt;
&lt;li&gt;therefore, the average distance between these two distances is &lt;span class=&#34;math inline&#34;&gt;\(\frac{2.675 + 2.390}{2} = 2.5325\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;2.5325 is thus the new distance between points 1 and 2 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;we apply the same process for points 3 and 2 &amp;amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The average distance between these 2 distances is 0.543 so the new distance between points 3 and 2 &amp;amp; 4 is 0.543&lt;/li&gt;
&lt;li&gt;follow the same process for all other points&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 3. Based on the distance matrix in step 2, the smallest distance is 0.543 between points 3 and 2 &amp;amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp;amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp;amp; 3 &amp;amp; 4. The groups are thus: 1, 2 &amp;amp; 3 &amp;amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.528333
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.724
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;points 1 and 5 have not change, so the distance between these two points are the same than in previous step&lt;/li&gt;
&lt;li&gt;from step 2 we see that the distance between points 1 and 2 &amp;amp; 4 is 2.5325 and the distance between points 1 and 3 is 2.520&lt;/li&gt;
&lt;li&gt;since we apply the average linkage criterion, we take the average distance&lt;/li&gt;
&lt;li&gt;however, we have to take into the consideration that there are 2 points in the group 2 &amp;amp; 4, while there is only one point in the group 3&lt;/li&gt;
&lt;li&gt;the average distance for the distance between 1 and 2 &amp;amp; 3 &amp;amp; 4 is thus: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(2 \cdot 2.5325) + (1 \cdot 2.520)}{3} = 2.528333\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;same process for points 5 and 2 &amp;amp; 3 &amp;amp; 4: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(2 \cdot 1.6855) + (1 \cdot 1.801)}{3} = 1.724\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp;amp; 5. The groups are thus: 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.126167
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the only distance left to compute is the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;from the previous step we see that the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is 2.528333 and the distance between points 5 and 2 &amp;amp; 3 &amp;amp; 4 is 1.724&lt;/li&gt;
&lt;li&gt;since we apply the average linkage criterion, we take the average distance, which is &lt;span class=&#34;math inline&#34;&gt;\(\frac{2.528333 + 1.724}{2} = 2.126167\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.126167&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 5. The final combination of points is the combination of points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4, with a final height of 2.126167. Heights are used to draw the dendrogram in the sixth and final step.&lt;/p&gt;
&lt;p&gt;Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the first combination of points was between points 2 and 4, with a height of 0.328&lt;/li&gt;
&lt;li&gt;the second combination was between points 3 and 2 &amp;amp; 4 with a height of 0.543&lt;/li&gt;
&lt;li&gt;the third combination was between points 1 and 5 with a height of 0.942&lt;/li&gt;
&lt;li&gt;the final combination was between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 with a height of 2.126167&lt;/li&gt;
&lt;li&gt;this is exactly what is illustrated in the following dendrogram:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-in-r-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solution in R&lt;/h3&gt;
&lt;p&gt;To perform the hierarchical clustering with any of the 3 criterion in R, we first need to enter the data (in this case as a matrix format, but it can also be entered as dataframes):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- matrix(c(2.03, 0.06, -0.64, -0.10, -0.42, -0.53, -0.36, 0.07, 1.14, 0.37),
  nrow = 5, byrow = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;single-linkage-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Single linkage&lt;/h4&gt;
&lt;p&gt;We can apply the hierarchical clustering with the single linkage criterion thanks to the &lt;code&gt;hclust()&lt;/code&gt; function with the argument &lt;code&gt;method = &amp;quot;single&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering: single linkage
hclust &amp;lt;- hclust(dist(X), method = &amp;quot;single&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;hclust()&lt;/code&gt; function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix &lt;code&gt;X&lt;/code&gt; corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the &lt;code&gt;dist()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We can now extract the heights and plot the dendrogram to check our results by hand found above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(hclust$height, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.328 0.483 0.942 1.530&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.&lt;/p&gt;
&lt;p&gt;Remember that hierarchical clustering is used to determine the optimal number of clusters. This optimal number of clusters can be determined thanks to the dendrogram. For this, we usually look at the largest difference of heights:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/dendrogram-single-linkage.png&#34; alt=&#34;How to determine the number of clusters from a dendrogram? Take the largest difference of heights and count how many vertical lines you see&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;How to determine the number of clusters from a dendrogram? Take the largest difference of heights and count how many vertical lines you see&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp;amp; 3 &amp;amp; 4 with the group 1 &amp;amp; 5. To determine the optimal number of clusters, simply count how many vertical lines you see within this largest difference. In our case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the &lt;code&gt;rect.hclust()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &amp;quot;blue&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we could also determine the optimal number of cluster thanks to a barplot of the heights (stored in &lt;code&gt;$height&lt;/code&gt; of the clustering output):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(hclust$height,
  names.arg = (nrow(X) - 1):1 # show the number of cluster below each bars
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, look for the largest jump of heights. In our case, the largest jump is from 1 to 2 classes. Therefore, the optimal number of classes is 2.&lt;/p&gt;
&lt;p&gt;Note that determining the number of clusters using the dendrogram or barplot is not a strict rule. You can also consider other methods such as the &lt;em&gt;silhouette plot&lt;/em&gt;, &lt;em&gt;elbow plot&lt;/em&gt; or some numerical measures like Dunn’s index, Hubert’s gamma, etc., which show the variation of the error with the number of clusters (&lt;em&gt;k&lt;/em&gt;), and you choose the value of &lt;em&gt;k&lt;/em&gt; where the error is smallest. Furthermore, measuring the goodness of clusters can be done thanks to the Dunn’s Index (the higher the index, the better). However, these methods are beyond the scope of this course and the method presented with the dendrogram is generally sufficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complete-linkage-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complete linkage&lt;/h4&gt;
&lt;p&gt;We can apply the hierarchical clustering with the complete linkage criterion thanks to the &lt;code&gt;hclust()&lt;/code&gt; function with the argument &lt;code&gt;method = &amp;quot;complete&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering: complete linkage
hclust &amp;lt;- hclust(dist(X), method = &amp;quot;complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;hclust()&lt;/code&gt; function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix &lt;code&gt;X&lt;/code&gt; corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the &lt;code&gt;dist()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We can now extract the heights and plot the dendrogram to check our results by hand found above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(hclust$height, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.328 0.603 0.942 2.675&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.&lt;/p&gt;
&lt;p&gt;Similar to the single linkage, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp;amp; 3 &amp;amp; 4 with the group 1 &amp;amp; 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the &lt;code&gt;rect.hclust()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &amp;quot;blue&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-linkage-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Average linkage&lt;/h4&gt;
&lt;p&gt;We can apply the hierarchical clustering with the average linkage criterion thanks to the &lt;code&gt;hclust()&lt;/code&gt; function with the argument &lt;code&gt;method = &amp;quot;average&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering: average linkage
hclust &amp;lt;- hclust(dist(X), method = &amp;quot;average&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;hclust()&lt;/code&gt; function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix &lt;code&gt;X&lt;/code&gt; corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the &lt;code&gt;dist()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We can now extract the heights and plot the dendrogram to check our results by hand found above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(hclust$height, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.328 0.543 0.942 2.126&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.&lt;/p&gt;
&lt;p&gt;Like the single and complete linkages, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp;amp; 3 &amp;amp; 4 with the group 1 &amp;amp; 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the &lt;code&gt;rect.hclust()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &amp;quot;blue&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;k-means-versus-hierarchical-clustering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;em&gt;k&lt;/em&gt;-means versus hierarchical clustering&lt;/h1&gt;
&lt;p&gt;Clustering is rather a subjective statistical analysis and there can be more than one appropriate algorithm, depending on the dataset at hand or the type of problem to be solved. So choosing between &lt;em&gt;k&lt;/em&gt;-means and hierarchical clustering is not always easy. If you have a good reason to think that there is a specific number of clusters in your dataset (for example if you would like to distinguish diseased and healthy patients depending on some characteristics but you do not know in which group patients belong to), you should probably opt for the &lt;em&gt;k&lt;/em&gt;-means clustering as this technique is used when the number of groups is specified in advance. If you do not have any reason to believe there is a certain number of groups in your dataset (for instance in marketing when trying to distinguish clients without any prior belief on the number of different types of customers), then you should probably opt for the hierarchical clustering to determine in how many clusters your data should be divided.&lt;/p&gt;
&lt;p&gt;In addition to this, if you are still undecided note that, on the one hand, with a large number of variables, &lt;em&gt;k&lt;/em&gt;-means may be computationally faster than hierarchical clustering if the number of clusters is small. On the other hand, the result of a hierarchical clustering is a structure that is more informative and interpretable than the unstructured set of flat clusters returned by &lt;em&gt;k&lt;/em&gt;-means. Therefore, it is easier to determine the optimal number of clusters by looking at the dendrogram of a hierarchical clustering than trying to predict this optimal number in advance in case of &lt;em&gt;k&lt;/em&gt;-means.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you understand the different clustering methods and how to compute them by hand and in R.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-jaaw28m&#34;&gt;
&lt;p&gt;Hartigan, J. A., and M. A. Wong. 1979. “A K-Means Clustering Algorithm.” &lt;em&gt;Applied Statistics&lt;/em&gt; 28: 100–108.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lloyd1982least&#34;&gt;
&lt;p&gt;Lloyd, Stuart. 1982. “Least Squares Quantization in Pcm.” &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 28 (2). IEEE: 129–37.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inferential statistics: confidence intervals and hypothesis tests explained in 4 easy steps</title>
      <link>/blog/inferential-statistics-confidence-intervals-and-hypothesis-tests-explained-in-4-easy-steps/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/inferential-statistics-confidence-intervals-and-hypothesis-tests-explained-in-4-easy-steps/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypothesis-tests-and-confidence-intervals-why-and-when&#34;&gt;Hypothesis tests and confidence intervals: why and when?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;xxx add image and put the link in the YAML&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Remember that &lt;a href=&#34;/blog/descriptive-statistics-by-hand&#34;&gt;descriptive statistics&lt;/a&gt; is a branch of statistics aiming at describing and summarizing a set of data in the best possible manner, that is, by reducing them down to a few meaningful key measures and visualizations (with as little loss of information as possible). In other words, descriptive statistics helps to have a better understanding and a clear image about a set of observations thanks to summary statistics and graphics. With descriptive statistics, there is no uncertainty because we describe only the group of observations that we decided to work on and no attempt is made to generalize the observed characteristics to another or to a larger group of observations.&lt;/p&gt;
&lt;p&gt;Inferential statistics is another branch of statistics that uses a random sample of data taken from a population to make inferences, i.e., to draw conclusions about the population (see the &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;difference between population and sample&lt;/a&gt;). In other words, information from the sample is used to make generalizations about the parameter of interest in the population. Inferential statistics includes two important tools: hypothesis tests and confidence intervals.&lt;/p&gt;
&lt;p&gt;As part of my teaching assistant position, I quickly realized that students often struggle to compute confidence intervals, perform hypothesis tests and interpret the results. It seems to me that students often encounter difficulties because this branch of statistics is rather unclear and abstract to them. I believe the main reason why it looks abstract to them is because they do not understand the final goal of inferential statistics, that is, the why behind these tools. They often perform hypothesis tests and confidence intervals by simply following the steps presented in another example whitout understanding the reasoning behind it, as they would follow a cooking recipe because they must prepare food, but not because they actually want to prepare &lt;em&gt;good&lt;/em&gt; food.&lt;/p&gt;
&lt;p&gt;For this reason, I though it would be useful to write an article on the goal of hypothesis tests and confidence intervals (the why), in which context they should be used (the when), how they work (in 4 easy steps) and how to interpret the results (as statistical results are meaningless without proper interpretation). Like anything else in statistics, it becomes much easier when we understand what we are trying to demonstrate before knowing how to compute it.&lt;/p&gt;
&lt;p&gt;Inferential statistics can be applied to many parameters. Nonetheless, in order to keep this article easy and accessible to people from diverse backgrounds, I focus on hypothesis tests and confidences intervals applied to the 3 main parameters: &lt;strong&gt;mean, proportion and variance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you are familiar with these two tools, below are 3 articles that may be of interest to you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/blog/xxx&#34;&gt;Hypothesis tests and confidence intervals for one and two means (independent and paired samples)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/blog/xxx&#34;&gt;Hypothesis tests and confidence intervals for one and two proportions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/blog/xxx&#34;&gt;Hypothesis tests and confidence intervals for one and two variances&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The 3 articles above focus on the practical application of the two tools of inferential statistics by hand and in R. The present article covers the same topic but from a theoritical perspective in order to lay the foundations of hypothesis testing and confidence interval, with a special focus on the understanding and the reasoning behind the tools. I believe that grasping the concepts behind these tools from a theoritical perspective is of great help when applying them in practice.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-tests-and-confidence-intervals-why-and-when&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis tests and confidence intervals: why and when?&lt;/h1&gt;
&lt;p&gt;Unlike descriptive statistics where we have describe only the data at hand, hypothesis tests and confidence intervals use a subset of observations (a sample) to draw conclusions about the population.&lt;/p&gt;
&lt;p&gt;One may wonder why we would try to “guess” a parameter of a population based on a sample, instead of simply collecting the data for the entire population and compute the statistics we are interested in. The main reason why we actually use a sample instead of the population is because most of the time collecting the data on the entire population is impossible, too complex, too expensive, it would take too long, or a combination of any of these reasons. Suppose a researcher wants to test if Belgian women are taller than French women. Suppose a health professional would like to know whether the proportion of smokers is the same among athletes and non-athletes. It would take way too long to measure the height of all Belgian and French women and to ask all athletes and non-athletes if they smoke or not.
For these reasons, we simply xxx&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you to understand better how to perform hypothesis tests and construct confidence intervals by hand.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Do my data follow a normal distribution? A note on the most widely used distribution and how to test for normality in R</title>
      <link>/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-normal-distribution&#34;&gt;What is a normal distribution?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#empirical-rule&#34;&gt;Empirical rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probabilities-and-standard-normal-distribution&#34;&gt;Probabilities and standard normal distribution&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#areas-under-the-normal-distribution-in-r-and-by-hand&#34;&gt;Areas under the normal distribution in R and by hand&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-1&#34;&gt;Ex. 1&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-2&#34;&gt;Ex. 2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r-1&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand-1&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-3&#34;&gt;Ex. 3&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r-2&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand-2&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-4&#34;&gt;Ex. 4&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r-3&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand-3&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-5&#34;&gt;Ex. 5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-is-the-normal-distribution-so-crucial-in-statistics&#34;&gt;Why is the normal distribution so crucial in statistics?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-test-the-normality-assumption&#34;&gt;How to test the normality assumption&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histogram&#34;&gt;Histogram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plot&#34;&gt;Density plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qq-plot&#34;&gt;QQ-plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normality-test&#34;&gt;Normality test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Do-my-data-follow-a-normal-distribution.jpeg&#34; alt=&#34;Photo by Tra Nguyen&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Tra Nguyen&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-normal-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is a normal distribution?&lt;/h1&gt;
&lt;p&gt;The normal distribution is a function that defines how a set of measurements is distributed around the center of these measurements (i.e., the mean). Many natural phenomena in real life can be approximated by a bell-shaped frequency distribution known as the normal distribution or the Gaussian distribution.&lt;/p&gt;
&lt;p&gt;The normal distribution is a mount-shaped, unimodal and symmetric distribution where most measurements gather around the mean. Moreover, the further a measure deviates from the mean, the lower the probability of occurring. In this sense, for a given variable, it is common to find values close to the mean, but less and less likely to find values as we move away from the mean. Last but not least, since the normal distribution is symmetric around its mean, extreme values in both tails of the distribution are equivalently unlikely. For instance, given that adult height follows a normal distribution, most adults are close to the average height and extremely short adults occur as infrequently as extremely tall adults.&lt;/p&gt;
&lt;p&gt;In this article, the focus is on understanding the normal distribution, the associated empirical rule, its parameters and how to compute &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores to find probabilities under the curve (illustrated with examples). As it is a requirement in some statistical tests, we also show 4 complementary methods to test the normality assumption in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-rule&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Empirical rule&lt;/h1&gt;
&lt;p&gt;Data possessing an approximately normal distribution have a definite variation, as expressed by the following empirical rule:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu \pm \sigma\)&lt;/span&gt; includes approximately 68% of the observations&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu \pm 2 \cdot \sigma\)&lt;/span&gt; includes approximately 95% of the observations&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu \pm 3 \cdot \sigma\)&lt;/span&gt; includes almost all of the observations (99.7% to be more precise)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/empirical-rule-normal-distribution.png&#34; alt=&#34;Normal distribution &amp;amp; empirical rule (68-95-99.7% rule)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Normal distribution &amp;amp; empirical rule (68-95-99.7% rule)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; correspond to the population mean and population standard deviation, respectively.&lt;/p&gt;
&lt;p&gt;The empirical rule, also known as the 68-95-99.7% rule, is illustrated by the following 2 examples. Suppose that the scores of an exam in statistics given to all students in a Belgian university are known to have, approximately, a normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = 67\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 9\)&lt;/span&gt;. It can then be deduced that approximately 68% of the scores are between 58 and 76, that approximately 95% of the scores are between 49 and 85, and that almost all of the scores (99.7%) are between 40 and 94. Thus, knowing the mean and the standard deviation gives us a fairly good picture of the distribution of scores. Now suppose that a single university student is randomly selected from those who took the exam. What is the probability that her score will be between 49 and 85? Based on the empirical rule, we find that 0.95 is a reasonable answer to this probability question.&lt;/p&gt;
&lt;p&gt;The utility and value of the empirical rule are due to the common occurrence of approximately normal distributions of measurements in nature. For example, IQ, shoe size, height, birth weight, etc. are approximately normally-distributed. You will find that approximately 95% of these measurements will be within &lt;span class=&#34;math inline&#34;&gt;\(2\sigma\)&lt;/span&gt; of their mean &lt;span class=&#34;citation&#34;&gt;(Wackerly, Mendenhall, and Scheaffer 2014)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parameters&lt;/h1&gt;
&lt;p&gt;Like many probability distributions, the shape and probabilities of the normal distribution is defined entirely by some parameters. The normal distribution has two parameters: (i) the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#mean&#34;&gt;mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;&lt;/a&gt; and (ii) the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#variance&#34;&gt;variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/a&gt; (i.e., the square of the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#standard-deviation&#34;&gt;standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;&lt;/a&gt;). The mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; locates the center of the distribution, that is, the central tendency of the observations, and the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; defines the width of the distribution, that is, the spread of the observations.&lt;/p&gt;
&lt;p&gt;The mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; can take on any finite value (i.e., &lt;span class=&#34;math inline&#34;&gt;\(-\infty &amp;lt; \mu &amp;lt; \infty\)&lt;/span&gt;), whereas the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; can assume any positive finite value (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 &amp;gt; 0\)&lt;/span&gt;). The shape of the normal distribution changes based on these two parameters. Since there is an infinite number of combinations of the mean and variance, there is an infinite number of normal distributions, and thus an infinite number of forms.&lt;/p&gt;
&lt;p&gt;For instance, see how the shapes of the normal distributions vary when the two parameters change:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see on the second graph, when the variance (or the standard deviation) decreases, the observations are closer to the mean. On the contrary, when the variance (or standard deviation) increases, it is more likely that observations will be further away from the mean.&lt;/p&gt;
&lt;p&gt;A random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; which follows a normal distribution with a mean of 430 and a variance of 17 is denoted &lt;span class=&#34;math inline&#34;&gt;\(X ~ \sim \mathcal{N}(\mu = 430, \sigma^2 = 17)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We have seen that, although different normal distributions have different shapes, all normal distributions have common characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They are symmetric, 50% of the population is above the mean and 50% of the population is below the mean&lt;/li&gt;
&lt;li&gt;The mean, median and mode are equal&lt;/li&gt;
&lt;li&gt;The empirical rule detailed earlier is applicable to all normal distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;probabilities-and-standard-normal-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probabilities and standard normal distribution&lt;/h1&gt;
&lt;p&gt;Probabilities and quantiles for random variables with normal distributions are easily found using R via the functions &lt;code&gt;pnorm()&lt;/code&gt; and &lt;code&gt;qnorm()&lt;/code&gt;. Probabilities associated with a normal distribution can also be found using this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-101/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shiny app&lt;/a&gt;. However, before computing probabilities, we need to learn more about the standard normal distribution and the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score.&lt;/p&gt;
&lt;p&gt;Although there are infinitely many normal distributions (since there is a normal distribution for every combination of mean and variance), we need only one table to find the probabilities under the normal curve: the &lt;strong&gt;standard normal distribution&lt;/strong&gt;. The normal standard distribution is a special case of the normal distribution where the mean is equal to 0 and the variance is equal to 1. A normal random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can always be transformed to a standard normal random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, a process known as “scaling” or “standardization”, by subtracting the mean from the observation, and dividing the result by the standard deviation. Formally:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = \frac{X - \mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the observation, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; the mean and standard deviation of the population from which the observation was drawn. So the mean of the standard normal distribution is 0, and its variance is 1, denoted &lt;span class=&#34;math inline&#34;&gt;\(Z ~ \sim \mathcal{N}(\mu = 0, \sigma^2 = 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From this formula, we see that &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, referred as standard score or &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score, allows to see how far away one specific observation is from the mean of all observations, with the distance expressed in standard deviations. In other words, the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score corresponds to the number of standard deviations one observation is away from the mean. A positive &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score means that the specific observation is above the mean, whereas a negative &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score means that the specific observation is below the mean. &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores are often used to compare an individual to her peers, or more generally, a measurement compared to its distribution.&lt;/p&gt;
&lt;p&gt;For instance, suppose a student scoring 60 at a statistics exam with the mean score of the class being 40, and scoring 65 at an economics exam with the mean score of the class being 80. Given the “raw” scores, one would say that the student performed better in economics than in statistics. However, taking into consideration her peers, it is clear that the student performed relatively better in statistics than in economics. Computing &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores allows to take into consideration all other students (i.e., the entire distribution) and gives a better measure of comparison. Let’s compute the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores for the two exams, assuming that the score for both exams follow a normal distribution with the following parameters:&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Statistics&lt;/th&gt;
&lt;th&gt;Economics&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Standard deviation&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;12.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Student’s score&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Statistics: &lt;span class=&#34;math inline&#34;&gt;\(Z_{stat} = \frac{60 - 40}{8} = 2.5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Economics: &lt;span class=&#34;math inline&#34;&gt;\(Z_{econ} = \frac{65 - 80}{12.5} = -1.2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the one hand, the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score for the exam in statistics is positive (&lt;span class=&#34;math inline&#34;&gt;\(Z_{stat} = 2.5\)&lt;/span&gt;) which means that she performed better than average. On the other hand, her score for the exam in economics is negative (&lt;span class=&#34;math inline&#34;&gt;\(Z_{econ} = -1.2\)&lt;/span&gt;) which means that she performed worse than average. Below an illustration of her grades in a standard normal distribution for better comparison:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the score in economics is better in absolute terms, the score in statistics is actually relatively better when comparing each score within its own distribution.&lt;/p&gt;
&lt;p&gt;Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score also enables to compare observations that would otherwise be impossible because they have different units for example. Suppose you want to compare a salary in € with a weight in kg. Without standardization, there is no way to conclude whether someone is more extreme in terms of her wage or in terms of her weight. Thanks to &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores, we can compare two values that were in the first place not comparable to each other.&lt;/p&gt;
&lt;p&gt;Final remark regarding the interpretation of a &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score: a rule of thumb is that an observation with a &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score between -3 and -2 or between 2 and 3 is considered as a rare value. An observation with a &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score smaller than -3 or larger than 3 is considered as an extremely rare value. A value with any other &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score is considered as not rare nor extremely rare.&lt;/p&gt;
&lt;div id=&#34;areas-under-the-normal-distribution-in-r-and-by-hand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Areas under the normal distribution in R and by hand&lt;/h2&gt;
&lt;p&gt;Now that we have covered the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score, we are going to use it to determine the area under the curve of a normal distribution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that there are several ways to arrive at the solution in the following exercises. You may therefore use other steps than the ones presented to obtain the same result.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;ex.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 1&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; denote a normal random variable with mean 0 and standard deviation 1, find &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We actually look for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2014.18.54.png&#34; alt=&#34;Standard normal distribution: P(Z &amp;gt; 1)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution: &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(1,
  mean = 0,
  sd = 1, # sd stands for standard deviation
  lower.tail = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1586553&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We look for the probability of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; being larger than 1 so we set the argument &lt;code&gt;lower.tail = FALSE&lt;/code&gt;. The default &lt;code&gt;lower.tail = TRUE&lt;/code&gt; would give the result for &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;lt; 1)\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(P(Z = 1) = 0\)&lt;/span&gt; so writing &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(P(Z \ge 1)\)&lt;/span&gt; is equivalent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;See that the random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; has already a mean of 0 and a standard deviation of 1, so no transformation is required. To find the probabilities by hand, we need to refer to the standard normal distribution table shown below:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2015.07.44.png&#34; alt=&#34;Standard normal distribution table (Wackerly, Mendenhall, and Scheaffer 2014).&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution table &lt;span class=&#34;citation&#34;&gt;(Wackerly, Mendenhall, and Scheaffer 2014)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the illustration at the top of the table, we see that the values inside the table correspond to the area under the normal curve &lt;strong&gt;above&lt;/strong&gt; a certain &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Since we are looking precisely at the probability above &lt;span class=&#34;math inline&#34;&gt;\(z = 1\)&lt;/span&gt; (since we look for &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;), we can simply proceed down the first (&lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;) column in the table until &lt;span class=&#34;math inline&#34;&gt;\(z = 1.0\)&lt;/span&gt;. The probability is 0.1587. Thus, &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1) = 0.1587\)&lt;/span&gt;. This is similar to what we found using R, except that values in the table are rounded to 4 digits.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 2&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; denote a normal random variable with mean 0 and standard deviation 1, find &lt;span class=&#34;math inline&#34;&gt;\(P(−1 \le Z \le 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We are looking for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2014.19.14.png&#34; alt=&#34;Standard normal distribution: P(−1 \le Z \le 1)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution: &lt;span class=&#34;math inline&#34;&gt;\(P(−1 \le Z \le 1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(1, lower.tail = TRUE) - pnorm(-1, lower.tail = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6826895&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the arguments by default for the mean and the standard deviation are &lt;code&gt;mean = 0&lt;/code&gt; and &lt;code&gt;sd = 1&lt;/code&gt;. Since this is what we need, we can omit them.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;For this exercise we proceed by steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The shaded area corresponds to the entire area under the normal curve minus the two white areas in both tails of the curve.&lt;/li&gt;
&lt;li&gt;We know that the normal distribution is symmetric.&lt;/li&gt;
&lt;li&gt;Therefore, the shaded area is the entire area under the curve minus two times the white area in the right tail of the curve, the white area in the right tail of the curve being &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;We also know that the entire area under the normal curve is 1.&lt;/li&gt;
&lt;li&gt;Thus, the shaded area is 1 minus 2 times &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(−1 \le Z \le 1) = 1 - 2 \cdot P(Z &amp;gt; 1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= 1 - 2 \cdot 0.1587 = 0.6826\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1) = 0.1587\)&lt;/span&gt; has been found in the previous exercise.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 3&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; denote a normal random variable with mean 0 and standard deviation 1, find &lt;span class=&#34;math inline&#34;&gt;\(P(0 \le Z \le 1.37)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We are looking for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2014.19.46.png&#34; alt=&#34;Standard normal distribution: P(0 \le Z \le 1.37)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution: &lt;span class=&#34;math inline&#34;&gt;\(P(0 \le Z \le 1.37)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(0, lower.tail = FALSE) - pnorm(1.37, lower.tail = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4146565&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;Again we proceed by steps for this exercise:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We know that &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 0) = 0.5\)&lt;/span&gt; since the entire area under the curve is 1, half of it is 0.5.&lt;/li&gt;
&lt;li&gt;The shaded area is half of the entire area under the curve minus the area from 1.37 to infinity.&lt;/li&gt;
&lt;li&gt;The area under the curve from 1.37 to infinity corresponds to &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.37)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Therefore, the shaded area is &lt;span class=&#34;math inline&#34;&gt;\(0.5 - P(Z &amp;gt; 1.37)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;To find &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.37)\)&lt;/span&gt;, proceed down the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; column in the table to the entry 1.3 and then across the top of the table to the column labeled .07 to read &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.37) = .0853\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Thus,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(0 \le Z \le 1.37) = P(Z &amp;gt; 0) - P(Z &amp;gt; 1.37)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ = 0.5 - 0.0853 = 0.4147\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 4&lt;/h3&gt;
&lt;p&gt;Recap the example presented in the empirical rule: Suppose that the scores of an exam in statistics given to all students in a Belgian university are known to have a normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = 67\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 9\)&lt;/span&gt;. What fraction of the scores lies between 70 and 80?&lt;/p&gt;
&lt;p&gt;We are looking for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2016.24.30.png&#34; alt=&#34;P(70 \le X \le 80) where X \sim \mathcal{N}(\mu = 67, \sigma^2 = 9^2)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(70 \le X \le 80)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X \sim \mathcal{N}(\mu = 67, \sigma^2 = 9^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(70, mean = 67, sd = 9, lower.tail = FALSE) - pnorm(80, mean = 67, sd = 9, lower.tail = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2951343&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;Remind that we are looking for &lt;span class=&#34;math inline&#34;&gt;\(P(70 \le X \le 80)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X \sim \mathcal{N}(\mu = 67, \sigma^2 = 9^2)\)&lt;/span&gt;. The random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is in its “raw” format, meaning that it has not been standardized yet since the mean is 67 and the variance is &lt;span class=&#34;math inline&#34;&gt;\(9^2\)&lt;/span&gt;. We thus need to first apply the transformation to standardize the endpoints 70 and 80 with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = \frac{X - \mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After the standardization, &lt;span class=&#34;math inline&#34;&gt;\(x = 70\)&lt;/span&gt; becomes (in terms of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, so in terms of deviation from the mean expressed in standard deviation):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z = \frac{70 - 67}{9} = 0.3333\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and &lt;span class=&#34;math inline&#34;&gt;\(x = 80\)&lt;/span&gt; becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z = \frac{80 - 67}{9} = 1.4444\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The figure above in terms of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is now in terms of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2016.37.13.png&#34; alt=&#34;P(0.3333 \le Z \le 1.4444) where Z \sim \mathcal{N}(\mu = 0, \sigma^2 = 1)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(0.3333 \le Z \le 1.4444)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal{N}(\mu = 0, \sigma^2 = 1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Finding the probability &lt;span class=&#34;math inline&#34;&gt;\(P(0.3333 \le Z \le 1.4444)\)&lt;/span&gt; is similar to exercises 1 to 3:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The shaded area corresponds to the area under the curve from &lt;span class=&#34;math inline&#34;&gt;\(z = 0.3333\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(z = 1.4444\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;In other words, the shaded area is the area under the curve from &lt;span class=&#34;math inline&#34;&gt;\(z = 0.3333\)&lt;/span&gt; to infinity minus the area under the curve from &lt;span class=&#34;math inline&#34;&gt;\(z = 1.4444\)&lt;/span&gt; to infinity.&lt;/li&gt;
&lt;li&gt;From the table, &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 0.3333) = 0.3707\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.4444) = 0.0749\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Thus:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(0.3333 \le Z \le 1.4444)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= P(Z &amp;gt; 0.3333) - P(Z &amp;gt; 1.4444)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= 0.3707 - 0.0749 = 0.2958\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The difference with the probability found using in R comes from the rounding.&lt;/p&gt;
&lt;p&gt;To conclude this exercise, we can say that, given that the mean scores is 67 and the standard deviation is 9, 29.58% of the students scored between 70 and 80.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 5&lt;/h3&gt;
&lt;p&gt;See another example in a context &lt;a href=&#34;/blog/a-guide-on-how-to-read-statistical-tables/#example&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-the-normal-distribution-so-crucial-in-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why is the normal distribution so crucial in statistics?&lt;/h1&gt;
&lt;p&gt;The normal distribution is important for three main reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some statistical hypothesis tests assume that the data follow a normal distribution&lt;/li&gt;
&lt;li&gt;The central limit theorem states that, for a large number of observations (&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; 30\)&lt;/span&gt;), no matter what is the underlying distribution of the original variable, the distribution of the sample means (&lt;span class=&#34;math inline&#34;&gt;\(\overline{X}_n\)&lt;/span&gt;) and of the sum (&lt;span class=&#34;math inline&#34;&gt;\(S_n = \sum_{i = 1}^n X_i\)&lt;/span&gt;) may be approached by a normal distribution&lt;/li&gt;
&lt;li&gt;Linear and nonlinear regression assume that the residuals are normally-distributed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is therefore useful to know how to test for normality in R, which is the topic of next sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-test-the-normality-assumption&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to test the normality assumption&lt;/h1&gt;
&lt;p&gt;As mentioned above, some statistical tests require that the data follow a normal distribution, or the result of the test may be flawed.&lt;/p&gt;
&lt;p&gt;In this section, we show 4 complementary methods to determine whether your data follow a normal distribution in R.&lt;/p&gt;
&lt;div id=&#34;histogram&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Histogram&lt;/h2&gt;
&lt;p&gt;A histogram displays the spread and shape of a distribution, so it is a good starting point to evaluate normality. Let’s have a look at the histogram of a distribution that we would expect to follow a normal distribution, the height of 1,000 adults in cm:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The normal curve with the corresponding mean and variance has been added to the histogram. The histogram follows the normal curve so the data seems to follow a normal distribution.&lt;/p&gt;
&lt;p&gt;Below the minimal code for a histogram in R with the dataset &lt;code&gt;iris&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(iris)
hist(iris$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris) +
  aes(x = Sepal.Length) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Histograms are however not sufficient, particularly in the case of small samples because the number of bins greatly change its appearance. Histograms are not recommended when the number of observations is less than 20 because it does not always correctly illustrate the distribution. See two examples below with dataset of 10 and 12 observations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Can you tell whether these datasets follow a normal distribution? Surprisingly, both follow a normal distribution!&lt;/p&gt;
&lt;p&gt;In the remaining of the article, we will use the dataset of the 12 adults. If you would like to follow my code in your own script, here is how I generated the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
dat_hist &amp;lt;- data.frame(
  value = rnorm(12, mean = 165, sd = 5)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;rnorm()&lt;/code&gt; function generates random numbers from a normal distribution (12 random numbers with a mean of 165 and standard deviation of 5 in this case). These 12 observations are then saved in the dataset called &lt;code&gt;dat_hist&lt;/code&gt; under the variable &lt;code&gt;value&lt;/code&gt;. Note that &lt;code&gt;set.seed(42)&lt;/code&gt; is important to obtain the exact same data as me.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Density plot&lt;/h2&gt;
&lt;p&gt;Density plots also provide a visual judgment about whether the data follow a normal distribution. They are similar to histograms as they also allow to analyze the spread and the shape of the distribution. However, they are a smoothed version of the histogram. Here is the density plot drawn from the dataset on the height of the 12 adults discussed above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(density(dat_hist$value))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggpubr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggpubr&amp;quot;) # package must be installed first
ggdensity(dat_hist$value,
  main = &amp;quot;Density plot of adult height&amp;quot;,
  xlab = &amp;quot;Height (cm)&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since it is hard to test for normality from histograms and density plots only, it is recommended to corroborate these graphs with a QQ-plot. QQ-plot, also known as normality plot, is the third method presented to evaluate normality.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qq-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;QQ-plot&lt;/h2&gt;
&lt;p&gt;Like histograms and density plots, QQ-plots allow to visually evaluate the normality assumption. Here is the QQ-plot drawn from the dataset on the height of the 12 adults discussed above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(car)
qqPlot(dat_hist$value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12  2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;{ggpubr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggpubr)
ggqqplot(dat_hist$value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead of looking at the spread of the data (as it is the case with histograms and density plots), with QQ-plots we only need to ascertain whether the data points follow the line (sometimes referred as Henry’s line).&lt;/p&gt;
&lt;p&gt;If points are close to the reference line and within the confidence bands, the normality assumption can be considered as met. The bigger the deviation between the points and the reference line and the more they lie outside the confidence bands, the less likely that the normality condition is met. The height of these 12 adults seem to follow a normal distribution because all points lie within the confidence bands.&lt;/p&gt;
&lt;p&gt;When facing a non-normal distribution as shown by the QQ-plot below (systematic departure from the reference line), the first step is usually to apply the logarithm transformation on the data and recheck to see whether the log-transformed data are normally distributed. Applying the logarithm transformation can be done with the &lt;code&gt;log()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that QQ-plots are also a convenient way to assess whether residuals from regression analysis follow a normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normality-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normality test&lt;/h2&gt;
&lt;p&gt;The 3 tools presented above were a visual inspection of the normality. Nonetheless, visual inspection may sometimes be unreliable so it is also possible to formally test whether the data follow a normal distribution with statistical tests. These normality tests compare the distribution of the data to a normal distribution in order to assess whether observations show an important deviation from normality.&lt;/p&gt;
&lt;p&gt;The two most common normality tests are Shapiro-Wilk’s test and Kolmogorov-Smirnov test. Both tests have the same hypotheses, that is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: the data follow a normal distribution&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: the data do not follow a normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shapiro-Wilk test is recommended for normality test as it provides better power than Kolmogorov-Smirnov test.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; In R, the Shapiro-Wilk test of normality can be done with the function &lt;code&gt;shapiro.test()&lt;/code&gt;:&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shapiro.test(dat_hist$value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dat_hist$value
## W = 0.93968, p-value = 0.4939&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt; 0.05\)&lt;/span&gt; implying that we do not reject the null hypothesis that the data follow a normal distribution. This test goes in the same direction than the QQ-plot, which showed no significant deviation from the normality (as all points lied within the confidence bands).&lt;/p&gt;
&lt;p&gt;It is important to note that, in practice, normality tests are often considered as too conservative in the sense that for large sample size (&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; 50\)&lt;/span&gt;), a small deviation from the normality may cause the normality condition to be violated. A normality test is a hypothesis test, so as the sample size increases, their capacity of detecting smaller differences increases. So as the number of observations increases, the Shapiro-Wilk test becomes very sensitive even to a small deviation from normality. As a consequence, it happens that according to the normality test the data do not follow a normal distribution although the departures from the normal distribution are negligible and the data in fact follow a normal distribution. For this reason, it is often the case that the normality condition is verified based on a combination of all methods presented in this article, that is, visual inspections (with histograms and QQ-plots) and a formal inspection (with the Shapiro-Wilk test for instance).&lt;/p&gt;
&lt;p&gt;I personally tend to prefer QQ-plots over histograms and normality tests so I do not have to bother about the sample size. This article showed the different methods that are available, your choice will of course depends on the type of your data and the context of your analyses.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope the article helped you to learn more about the normal distribution and how to test for normality in R.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-wackerly2014mathematical&#34;&gt;
&lt;p&gt;Wackerly, Dennis, William Mendenhall, and Richard L Scheaffer. 2014. &lt;em&gt;Mathematical Statistics with Applications&lt;/em&gt;. Cengage Learning.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The argument &lt;code&gt;lower.tail = TRUE&lt;/code&gt; is also the default so we could omit it as well. However, for clarity and to make sure I compute the propabilities in the correct side of the curve, I used to keep this argument explicit by writing it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The &lt;code&gt;set.seed()&lt;/code&gt; function accepts any numeric as argument. Generating random numbers (via &lt;code&gt;rnorm()&lt;/code&gt; for instance) implies that R will generates different random numbers every time you generate these random numbers (so every time you run the function &lt;code&gt;rnorm()&lt;/code&gt;). To make sure R generates the exact same numbers every time you run the function, a seed can be set with the function &lt;code&gt;set.seed()&lt;/code&gt;. Setting a seed implies that R will generate random numbers, but these numbers will always be the same as long as the seed is the same. This allows to replicate results that are based on a random generation. Change the seed if you want to generate other random values.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The Shapiro-Wilk test is based on the correlation between the sample and the corresponding normal scores.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;In R, the Kolmogorov-Smirnov test is performed with the function &lt;code&gt;ks.test()&lt;/code&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chi-square test of independence by hand</title>
      <link>/blog/chi-square-test-of-independence-by-hand/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/chi-square-test-of-independence-by-hand/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypotheses&#34;&gt;Hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-the-test-works&#34;&gt;How the test works?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#observed-frequencies&#34;&gt;Observed frequencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expected-frequencies&#34;&gt;Expected frequencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-statistic&#34;&gt;Test statistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#critical-value&#34;&gt;Critical value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion-and-interpretation&#34;&gt;Conclusion and interpretation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/chi-square-test-of-independence-by-hand_files/chi-square-test-of-independence-by-hand.jpeg&#34; alt=&#34;Photo by David Pennington&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by David Pennington&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Chi-square tests of independence test whether two &lt;a href=&#34;/blog/variable-types-and-examples/#qualitative&#34;&gt;qualitative variables&lt;/a&gt; are independent, that is, whether there exists a relationship between two categorical variables. In other words, this test is used to determine whether the values of one of the 2 qualitative variables depend on the values of the other qualitative variable.&lt;/p&gt;
&lt;p&gt;If the test shows no association between the two variables (i.e., the variables are independent), it means that knowing the value of one variable gives no information about the value of the other variable. On the contrary, if the test shows a relationship between the variables (i.e., the variables are dependent), it means that knowing the value of one variable provides information about the value of the other variable.&lt;/p&gt;
&lt;p&gt;This article focuses on how to perform a Chi-square test of independence by hand and how to interpret the results with a concrete example. To learn how to do this test in R, read the article “&lt;a href=&#34;/blog/chi-square-test-of-independence-in-r/&#34;&gt;Chi-square test of independence in R&lt;/a&gt;”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypotheses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypotheses&lt;/h1&gt;
&lt;p&gt;The Chi-square test of independence is a hypothesis test so it has a null (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) and an alternative hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; : the variables are independent, there is &lt;strong&gt;no&lt;/strong&gt; relationship between the two categorical variables. Knowing the value of one variable does not help to predict the value of the other variable&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; : the variables are dependent, there is a relationship between the two categorical variables. Knowing the value of one variable helps to predict the value of the other variable&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-the-test-works&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How the test works?&lt;/h1&gt;
&lt;p&gt;The Chi-square test of independence works by comparing the observed frequencies (so the frequencies observed in your sample) to the expected frequencies if there was no relationship between the two categorical variables (so the expected frequencies if the null hypothesis was true).&lt;/p&gt;
&lt;p&gt;If the difference between the observed frequencies and the expected frequencies is &lt;strong&gt;small&lt;/strong&gt;, we cannot reject the null hypothesis of independence and thus we cannot reject the fact that the two &lt;strong&gt;variables are not related&lt;/strong&gt;. On the other hand, if the difference between the observed frequencies and the expected frequencies is &lt;strong&gt;large&lt;/strong&gt;, we can reject the null hypothesis of independence and thus we can conclude that the two &lt;strong&gt;variables are related&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The threshold between a small and large difference is a value that comes from the Chi-square distribution (hence the name of the test). This value, referred as the critical value, depends on the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (usually set equal to 5%) and on the degrees of freedom. This critical value can be found in the statistical table of the Chi-square distribution. More on this critical value and the degrees of freedom later in the article.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;For our example, we want to determine whether there is a statistically significant association between smoking and being a professional athlete. Smoking can only be “yes” or “no” and being a professional athlete can only be “yes” or “no”. The two variables of interest are qualitative variables so we need to use a Chi-square test of independence, and the data have been collected on 28 persons.&lt;/p&gt;
&lt;p&gt;Note that we chose binary variables (binary variables = qualitative variables with two levels) for the sake of easiness, but the Chi-square test of independence can also be performed on qualitative variables with more than two levels. For instance, if the variable smoking had three levels: (i) non-smokers, (ii) moderate smokers and (iii) heavy smokers, the steps and the interpretation of the results of the test are similar than with two levels.&lt;/p&gt;
&lt;div id=&#34;observed-frequencies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Observed frequencies&lt;/h2&gt;
&lt;p&gt;Our data are summarized in the contingency table below reporting the number of people in each subgroup, totals by row, by column and the grand total:&lt;/p&gt;
&lt;table style=&#34;width:68%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt; &lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Non-smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Non-athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;expected-frequencies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expected frequencies&lt;/h2&gt;
&lt;p&gt;Remember that for the Chi-square test of independence we need to determine whether the observed counts are significantly different from the counts that we would expect if there was no association between the two variables. We have the observed counts (see the table above), so we now need to compute the expected counts in the case the variables were independent. These expected frequencies are computed for each subgroup one by one with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{expected frequencies} = \frac{\text{total # of obs. for the row} \cdot \text{total # of obs. for the column}}{\text{total number of observations}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where obs. correspond to observations. Given our table of observed frequencies above, below is the table of the expected frequencies computed for each subgroup:&lt;/p&gt;
&lt;table style=&#34;width:94%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt; &lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Non-smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(18 * 14) / 28 = 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(18 * 14) / 28 = 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Non-athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10 * 14) / 28 = 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10 * 14) / 28 = 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that the Chi-square test of independence should only be done when the &lt;strong&gt;expected&lt;/strong&gt; frequencies in all groups are equal to or greater than 5. This assumption is met for our example as the minimum number of expected frequencies is 5.
&lt;!-- If the condition is not met, the [Fisher&#39;s exact test](/blog/xxx/) is preferred. --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-statistic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Test statistic&lt;/h2&gt;
&lt;p&gt;We have the observed and expected frequencies. We now need to compare these frequencies to determine if they differ significantly. The difference between the observed and expected frequencies, referred as the test statistic (or t-stat) and denoted &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;, is computed as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\chi^2 = \sum_{i, j} \frac{\big(O_{ij} - E_{ij}\big)^2}{E_{ij}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; represents the observed frequencies and &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; the expected frequencies. We use the square of the differences between the observed and expected frequencies to make sure that negative differences are not compensated by positive differences. The formula looks more complex than what it really is, so let’s illustrate it with our example. We first compute the difference in each subgroup one by one according to the formula:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in the subgroup of athlete and non-smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(14 - 9)^2}{9} = 2.78\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in the subgroup of non-athlete and non-smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(0 - 5)^2}{5} = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in the subgroup of athlete and smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(4 - 9)^2}{9} = 2.78\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in the subgroup of non-athlete and smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(10 - 5)^2}{5} = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and then we sum them all to obtain the test statistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\chi^2 = 2.78 + 5 + 2.78 + 5 = 15.56\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critical-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Critical value&lt;/h2&gt;
&lt;p&gt;The test statistic alone is not enough to conclude for independence or dependence between the two variables. As previously mentioned, this test statistic (which in some sense is the difference between the observed and expected frequencies) must be compared to a critical value to determine whether the difference is large or small. One cannot tell that a test statistic is large or small without putting it in perspective with the critical value.&lt;/p&gt;
&lt;p&gt;If the test statistic is above the critical value, it means that the probability of observing such a difference between the observed and expected frequencies is unlikely. On the other hand, if the test statistic is below the critical value, it means that the probability of observing such a difference is likely. If it is likely to observe this difference, we cannot reject the hypothesis that the two variables are independent, otherwise we can conclude that there exists a relationship between the variables.&lt;/p&gt;
&lt;p&gt;The critical value can be found in the statistical table of the Chi-square distribution and depends on the significance level, denoted &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, and the degrees of freedom, denoted &lt;span class=&#34;math inline&#34;&gt;\(df\)&lt;/span&gt;. The significance level is usually set equal to 5%. The degrees of freedom for a Chi-square test of independence is found as follow:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[df = (\text{number of rows} - 1) \cdot (\text{number of columns} - 1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our example, the degrees of freedom is thus &lt;span class=&#34;math inline&#34;&gt;\(df = (2 - 1) \cdot (2 - 1) = 1\)&lt;/span&gt; since there are two rows and two columns in the contingency table (totals do not count as a row or column).&lt;/p&gt;
&lt;p&gt;We now have all the necessary information to find the critical value in the Chi-square table (&lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(df = 1\)&lt;/span&gt;). To find the critical value we need to look at the row &lt;span class=&#34;math inline&#34;&gt;\(df = 1\)&lt;/span&gt; and the column &lt;span class=&#34;math inline&#34;&gt;\(\chi^2_{0.050}\)&lt;/span&gt; (since &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;) in the picture below. The critical value is &lt;span class=&#34;math inline&#34;&gt;\(3.84146\)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/chi-square-test-of-independence-by-hand_files/Screenshot%202020-01-28%20at%2000.56.28.png&#34; alt=&#34;Chi-square table - Critical value for alpha = 5% and df = 1&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Chi-square table - Critical value for alpha = 5% and df = 1&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-interpretation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion and interpretation&lt;/h2&gt;
&lt;p&gt;Now that we have the test statistic and the critical value, we can compare them to check whether the null hypothesis of independence of the variables is rejected or not. In our example,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{test statistic} = 15.56 &amp;gt; \text{critical value} = 3.84146\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Like for any statistical test, when the test statistic is larger than the critical value, we can reject the null hypothesis at the specified significance level.&lt;/p&gt;
&lt;p&gt;In our case, we can therefore reject the null hypothesis of independence between the two categorical variables at the 5% significance level.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow\)&lt;/span&gt; This means that there is a significant relationship between the smoking habit and being an athlete or not. Knowing the value of one variable helps to predict the value of the other variable.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope the article helped you to perform the Chi-square test of independence by hand and interpret its results. If you would like to learn how to do this test in R, read the article “&lt;a href=&#34;/blog/chi-square-test-of-independence-in-r/&#34;&gt;Chi-square test of independence in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For readers that prefer to check the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value in order to reject or not the null hypothesis, I also created a &lt;a href=&#34;/blog/a-guide-on-how-to-read-statistical-tables/&#34;&gt;Shiny app&lt;/a&gt; to help you compute the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value given a test statistic.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chi-square test of independence in R</title>
      <link>/blog/chi-square-test-of-independence-in-r/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/chi-square-test-of-independence-in-r/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chi-square-test-of-independence-in-r&#34;&gt;Chi-square test of independence in R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion-and-interpretation&#34;&gt;Conclusion and interpretation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/chi-square-test-of-independence-in-r_files/Chi-square-test-independence-in-R.jpeg&#34; alt=&#34;Photo by Giorgio Tomassetti&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Giorgio Tomassetti&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This article explains how to perform the Chi-square test of independence in R and how to interpret its results. To learn more about how the test works and how to do it by hand, I invite you to read the article “&lt;a href=&#34;/blog/chi-square-test-of-independence-by-hand/&#34;&gt;Chi-square test of independence by hand&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;To briefly recap what have been said in that article, the Chi-square test of independence tests whether there is a relationship between two categorical variables. The null and alternative hypotheses are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; : the variables are independent, there is &lt;strong&gt;no&lt;/strong&gt; relationship between the two categorical variables. Knowing the value of one variable does not help to predict the value of the other variable&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; : the variables are dependent, there is a relationship between the two categorical variables. Knowing the value of one variable helps to predict the value of the other variable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Chi-square test of independence works by comparing the observed frequencies (so the frequencies observed in your sample) to the expected frequencies if there was no relationship between the two categorical variables (so the expected frequencies if the null hypothesis was true).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;For our example, let’s reuse the dataset introduced in the article “&lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;Descriptive statistics in R&lt;/a&gt;”. This dataset is the well-known &lt;code&gt;iris&lt;/code&gt; dataset slightly enhanced. Since there is only one categorical variable and the Chi-square test requires two categorical variables, we added the variable &lt;code&gt;size&lt;/code&gt; which corresponds to &lt;code&gt;small&lt;/code&gt; if the length of the petal is smaller than the median of all flowers, &lt;code&gt;big&lt;/code&gt; otherwise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- iris
dat$size &amp;lt;- ifelse(dat$Sepal.Length &amp;lt; median(dat$Sepal.Length),
  &amp;quot;small&amp;quot;, &amp;quot;big&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now create a contingency table of the two variables &lt;code&gt;Species&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt; with the &lt;code&gt;table()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(dat$Species, dat$size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
##              big small
##   setosa       1    49
##   versicolor  29    21
##   virginica   47     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The contingency table gives the observed number of cases in each subgroup. For instance, there is only one big setosa flower, while there are 49 small setosa flowers in the dataset.&lt;/p&gt;
&lt;p&gt;It is also a good practice to draw a barplot to visually represent the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

ggplot(dat) +
  aes(x = Species, fill = size) +
  geom_bar() +
  scale_fill_hue() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/chi-square-test-of-independence-in-r_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chi-square-test-of-independence-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chi-square test of independence in R&lt;/h2&gt;
&lt;p&gt;For this example, we are going to test in R if there is a relationship between the variables &lt;code&gt;Species&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt;. For this, the &lt;code&gt;chisq.test()&lt;/code&gt; function is used:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test &amp;lt;- chisq.test(table(dat$Species, dat$size))
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s Chi-squared test
## 
## data:  table(dat$Species, dat$size)
## X-squared = 86.035, df = 2, p-value &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything you need appears in this output: the title of the test, what variables have been used, the test statistic, the degrees of freedom and the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value of the test. You can also retrieve the &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; test statistic and the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$statistic # test statistic&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## X-squared 
##  86.03451&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$p.value # p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.078944e-19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need to find the expected frequencies, use &lt;code&gt;test$expected&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If a warning such as “Chi-squared approximation may be incorrect” appears, it means that the smallest expected frequencies is lower than 5. To avoid this issue, you can either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gather some levels (especially those with a small number of observations) to increase the number of observations in the subgroups, or&lt;/li&gt;
&lt;li&gt;use the Fisher’s exact test&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Fisher’s exact test does not require the assumption of a minimum of 5 expected counts. It can be applied in R thanks to the function &lt;code&gt;fisher.test()&lt;/code&gt;. This test is similar to the Chi-square test in terms of hypothesis and interpretation of the results. Learn more about this test in this &lt;a href=&#34;/blog/fisher-s-exact-test-in-r-independence-test-for-a-small-sample/&#34;&gt;article&lt;/a&gt; dedicated to this type of test.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-interpretation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion and interpretation&lt;/h2&gt;
&lt;p&gt;From the output and from &lt;code&gt;test$p.value&lt;/code&gt; we see that the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value is less than the significance level of 5%. Like any other statistical test, if the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value is less than the significance level, we can reject the null hypothesis.
&lt;!-- If you are not familiar with $p$-values, I invite you to read this [article](/blog/xxx/).  --&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow\)&lt;/span&gt; In our context, rejecting the null hypothesis for the Chi-square test of independence means that there is a significant relationship between the species and the size. Therefore, knowing the value of one variable helps to predict the value of the other variable.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope the article helped you to perform the Chi-square test of independence in R and interpret its results. If you would like to learn how to do this test by hand and how it works, read the article “&lt;a href=&#34;/blog/chi-square-test-of-independence-by-hand/&#34;&gt;Chi-square test of independence by hand&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Descriptive statistics in R</title>
      <link>/blog/descriptive-statistics-in-r/</link>
      <pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/descriptive-statistics-in-r/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-and-maximum&#34;&gt;Minimum and maximum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#range&#34;&gt;Range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mean&#34;&gt;Mean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#median&#34;&gt;Median&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#first-and-third-quartile&#34;&gt;First and third quartile&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#other-quantiles&#34;&gt;Other quantiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interquartile-range&#34;&gt;Interquartile range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation-and-variance&#34;&gt;Standard deviation and variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-variation&#34;&gt;Coefficient of variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mode&#34;&gt;Mode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#contingency-table&#34;&gt;Contingency table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#barplot&#34;&gt;Barplot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histogram&#34;&gt;Histogram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplot&#34;&gt;Boxplot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplot&#34;&gt;Scatterplot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qq-plot&#34;&gt;QQ-plot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#for-a-single-variable&#34;&gt;For a single variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-groups&#34;&gt;By groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plot&#34;&gt;Density plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-plot&#34;&gt;Correlation plot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/descriptive-statistics-in-r.jpeg&#34; alt=&#34;Photo by Ruthson Zimmerman&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Ruthson Zimmerman&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This article explains how to compute the main descriptive statistics in R and how to present them graphically. To learn more about the reasoning behind each descriptive statistics, how to compute them by hand and how to interpret them, read the article “&lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;Descriptive statistics by hand&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;To briefly recap what have been said in that article, descriptive statistics (in the broad sense of the term) is a branch of statistics aiming at summarizing, describing and presenting a series of values or a dataset. Descriptive statistics is often the first step and an important part in any statistical analysis. It allows to check the quality of the data and it helps to “understand” the data by having a clear overview of it. If well presented, descriptive statistics is already a good starting point for further analyses. There exists many measures to summarize a dataset. They are divided into two types:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;location measures and&lt;/li&gt;
&lt;li&gt;dispersion measures&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Location measures give an understanding about the central tendency of the data, whereas dispersion measures give an understanding about the spread of the data. In this article, we focus only on the implementation in R of the most common descriptive statistics and their visualizations (when deemed appropriate). See online or in the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;above mentioned article&lt;/a&gt; for more information about the purpose and usage of each measure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;p&gt;We use the dataset &lt;code&gt;iris&lt;/code&gt; throughout the article. This dataset is imported by default in R, you only need to load it by running &lt;code&gt;iris&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- iris # load the iris dataset and renamed it dat&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below a preview of this dataset and its structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(dat) # first 6 observations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(dat) # structure of dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &amp;quot;setosa&amp;quot;,&amp;quot;versicolor&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset contains 150 observations and 5 variables, representing the length and width of the sepal and petal and the species of 150 flowers. Length and width of the sepal and petal are numeric variables and the species is a factor with 3 levels (indicated by &lt;code&gt;num&lt;/code&gt; and &lt;code&gt;Factor w/ 3 levels&lt;/code&gt; after the name of the variables). See the &lt;a href=&#34;/blog/data-types-in-r&#34;&gt;different variables types in R&lt;/a&gt; if you need a refresh.&lt;/p&gt;
&lt;p&gt;Regarding plots, we present the default graphs and the graphs from the well-known &lt;code&gt;{ggplot2}&lt;/code&gt; package. Graphs from the &lt;code&gt;{ggplot2}&lt;/code&gt; package usually have a better look but it requires more advanced coding skills. If you need to publish or share your graphs, I suggest using &lt;code&gt;{ggplot2}&lt;/code&gt; if you can, otherwise the default graphics will do the job.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip: I recently discovered the ggplot2 builder from the &lt;code&gt;{esquisse}&lt;/code&gt; addins. See how you can easily &lt;a href=&#34;/blog/rstudio-addins-or-how-to-make-your-coding-life-easier/&#34;&gt;draw graphs from the &lt;code&gt;{ggplot2}&lt;/code&gt; package&lt;/a&gt; without having to code it yourself.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All plots displayed in this article can be customized. For instance, it is possible to edit the title, x and y-axis labels, color, etc. However, customizing plots is beyond the scope of this article so all plots are presented without any customization. Interested readers will find numerous resources online.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minimum-and-maximum&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Minimum and maximum&lt;/h1&gt;
&lt;p&gt;Minimum and maximum can be found thanks to the &lt;code&gt;min()&lt;/code&gt; and &lt;code&gt;max()&lt;/code&gt; functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively the &lt;code&gt;range()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rng &amp;lt;- range(dat$Sepal.Length)
rng&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.3 7.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;gives you the minimum and maximum directly. Note that the output of the &lt;code&gt;range()&lt;/code&gt; function is actually an object containing the minimum and maximum (in that order). This means you can actually access the minimum with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rng[1] # rng = name of the object specified above&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the maximum with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rng[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reminds us that, in R, there are often several ways to arrive at the same result. The method that uses the shortest piece of code is usually preferred as a shorter piece of code is less prone to coding errors and more readable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;range&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Range&lt;/h1&gt;
&lt;p&gt;The range can then be easily computed, as you have guessed, by subtracting the minimum from the maximum:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(dat$Sepal.Length) - min(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To my knowledge, there is no default function to compute the range. However, if you are familiar with writing functions in R
&lt;!-- (if not, see this article on [how to write a function in R](/blog/xxx/)) --&gt;
, you can create your own function to compute the range:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;range2 &amp;lt;- function(x) {
  range &amp;lt;- max(x) - min(x)
  return(range)
}

range2(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is equivalent than &lt;span class=&#34;math inline&#34;&gt;\(max - min\)&lt;/span&gt; presented above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mean&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mean&lt;/h1&gt;
&lt;p&gt;The mean can be computed with the &lt;code&gt;mean()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.843333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Tips:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if there is at least one missing value in your dataset, use &lt;code&gt;mean(dat$Sepal.Length, na.rm = TRUE)&lt;/code&gt; to compute the mean with the NA excluded. This argument can be used for most functions presented in this article, not only the mean&lt;/li&gt;
&lt;li&gt;for a truncated mean, use &lt;code&gt;mean(dat$Sepal.Length, trim = 0.10)&lt;/code&gt; and change the &lt;code&gt;trim&lt;/code&gt; argument to your needs&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;median&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Median&lt;/h1&gt;
&lt;p&gt;The median can be computed thanks to the &lt;code&gt;median()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;median(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with the &lt;code&gt;quantile()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat$Sepal.Length, 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 50% 
## 5.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;since the quantile of order 0.5 (&lt;span class=&#34;math inline&#34;&gt;\(q_{0.5}\)&lt;/span&gt;) corresponds to the median.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-and-third-quartile&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;First and third quartile&lt;/h1&gt;
&lt;p&gt;As the median, the first and third quartiles can be computed thanks to the &lt;code&gt;quantile()&lt;/code&gt; function and by setting the second argument to 0.25 or 0.75:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat$Sepal.Length, 0.25) # first quartile&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 25% 
## 5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat$Sepal.Length, 0.75) # third quartile&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75% 
## 6.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may have seen that the results above are slightly different than the results you would have found if you compute the first and third quartiles &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;by hand&lt;/a&gt;. It is normal, there are many methods to compute them (R actually has 7 methods to compute the quantiles!). However, the methods presented here and in the article “&lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;descriptive statistics by hand&lt;/a&gt;” are the easiest and most “standard” ones. Furthermore, results do not dramatically change between the two methods.&lt;/p&gt;
&lt;div id=&#34;other-quantiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other quantiles&lt;/h2&gt;
&lt;p&gt;As you have guessed, any quantile can also be computed with the &lt;code&gt;quantile()&lt;/code&gt; function. For instance, the &lt;span class=&#34;math inline&#34;&gt;\(4^{th}\)&lt;/span&gt; decile or the &lt;span class=&#34;math inline&#34;&gt;\(98^{th}\)&lt;/span&gt; percentile:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat$Sepal.Length, 0.4) # 4th decile&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 40% 
## 5.6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat$Sepal.Length, 0.98) # 98th percentile&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 98% 
## 7.7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interquartile-range&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interquartile range&lt;/h1&gt;
&lt;p&gt;The interquartile range (i.e., the difference between the first and third quartile) can be computed with the &lt;code&gt;IQR()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;IQR(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or alternatively with the &lt;code&gt;quantile()&lt;/code&gt; function again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(dat$Sepal.Length, 0.75) - quantile(dat$Sepal.Length, 0.25)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75% 
## 1.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As mentioned earlier, when possible it is usually recommended to use the shortest piece of code to arrive at the result. For this reason, the &lt;code&gt;IQR()&lt;/code&gt; function is preferred to compute the interquartile range.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-deviation-and-variance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Standard deviation and variance&lt;/h1&gt;
&lt;p&gt;The standard deviation and the variance is computed with the &lt;code&gt;sd()&lt;/code&gt; and &lt;code&gt;var()&lt;/code&gt; functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(dat$Sepal.Length) # standard deviation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8280661&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(dat$Sepal.Length) # variance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6856935&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember from this &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;article&lt;/a&gt; that the standard deviation and the variance are different whether we compute it for a sample or a population (see the difference between the two &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;here&lt;/a&gt;). In R, the standard deviation and the variance are computed as if the data represent a sample (so the denominator is &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of observations). To my knowledge, there is no function by default in R that computes the standard deviation or variance for a population.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Tip:&lt;/em&gt; to compute the standard deviation (or variance) of multiple variables at the same time, use &lt;code&gt;lapply()&lt;/code&gt; with the appropriate statistics as second argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(dat[, 1:4], sd)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Sepal.Length
## [1] 0.8280661
## 
## $Sepal.Width
## [1] 0.4358663
## 
## $Petal.Length
## [1] 1.765298
## 
## $Petal.Width
## [1] 0.7622377&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command &lt;code&gt;dat[, 1:4]&lt;/code&gt; selects the variables 1 to 4 as the fifth variable is a qualitative variable and the standard deviation cannot be computed on such type of variable. See a recap of the different &lt;a href=&#34;/blog/data-types-in-r/&#34;&gt;data types in R&lt;/a&gt; if needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;You can compute the minimum, &lt;span class=&#34;math inline&#34;&gt;\(1^{st}\)&lt;/span&gt; quartile, median, mean, &lt;span class=&#34;math inline&#34;&gt;\(3^{rd}\)&lt;/span&gt; quartile and the maximum for all numeric variables of a dataset at once using &lt;code&gt;summary()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Tip:&lt;/em&gt; if you need these descriptive statistics by group use the &lt;code&gt;by()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;by(dat, dat$Species, summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## dat$Species: setosa
##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.300   Min.   :1.000   Min.   :0.100  
##  1st Qu.:4.800   1st Qu.:3.200   1st Qu.:1.400   1st Qu.:0.200  
##  Median :5.000   Median :3.400   Median :1.500   Median :0.200  
##  Mean   :5.006   Mean   :3.428   Mean   :1.462   Mean   :0.246  
##  3rd Qu.:5.200   3rd Qu.:3.675   3rd Qu.:1.575   3rd Qu.:0.300  
##  Max.   :5.800   Max.   :4.400   Max.   :1.900   Max.   :0.600  
##        Species  
##  setosa    :50  
##  versicolor: 0  
##  virginica : 0  
##                 
##                 
##                 
## ------------------------------------------------------------ 
## dat$Species: versicolor
##   Sepal.Length    Sepal.Width     Petal.Length   Petal.Width          Species  
##  Min.   :4.900   Min.   :2.000   Min.   :3.00   Min.   :1.000   setosa    : 0  
##  1st Qu.:5.600   1st Qu.:2.525   1st Qu.:4.00   1st Qu.:1.200   versicolor:50  
##  Median :5.900   Median :2.800   Median :4.35   Median :1.300   virginica : 0  
##  Mean   :5.936   Mean   :2.770   Mean   :4.26   Mean   :1.326                  
##  3rd Qu.:6.300   3rd Qu.:3.000   3rd Qu.:4.60   3rd Qu.:1.500                  
##  Max.   :7.000   Max.   :3.400   Max.   :5.10   Max.   :1.800                  
## ------------------------------------------------------------ 
## dat$Species: virginica
##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.900   Min.   :2.200   Min.   :4.500   Min.   :1.400  
##  1st Qu.:6.225   1st Qu.:2.800   1st Qu.:5.100   1st Qu.:1.800  
##  Median :6.500   Median :3.000   Median :5.550   Median :2.000  
##  Mean   :6.588   Mean   :2.974   Mean   :5.552   Mean   :2.026  
##  3rd Qu.:6.900   3rd Qu.:3.175   3rd Qu.:5.875   3rd Qu.:2.300  
##  Max.   :7.900   Max.   :3.800   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    : 0  
##  versicolor: 0  
##  virginica :50  
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where the arguments are the name of the dataset, the grouping variable and the summary function. Follow this order, or specify the name of the arguments if you do not follow this order.&lt;/p&gt;
&lt;p&gt;If you need more descriptive statistics, use &lt;code&gt;stat.desc()&lt;/code&gt; from the package &lt;code&gt;{pastecs}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pastecs)
stat.desc(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Sepal.Length  Sepal.Width Petal.Length  Petal.Width Species
## nbr.val      150.00000000 150.00000000  150.0000000 150.00000000      NA
## nbr.null       0.00000000   0.00000000    0.0000000   0.00000000      NA
## nbr.na         0.00000000   0.00000000    0.0000000   0.00000000      NA
## min            4.30000000   2.00000000    1.0000000   0.10000000      NA
## max            7.90000000   4.40000000    6.9000000   2.50000000      NA
## range          3.60000000   2.40000000    5.9000000   2.40000000      NA
## sum          876.50000000 458.60000000  563.7000000 179.90000000      NA
## median         5.80000000   3.00000000    4.3500000   1.30000000      NA
## mean           5.84333333   3.05733333    3.7580000   1.19933333      NA
## SE.mean        0.06761132   0.03558833    0.1441360   0.06223645      NA
## CI.mean.0.95   0.13360085   0.07032302    0.2848146   0.12298004      NA
## var            0.68569351   0.18997942    3.1162779   0.58100626      NA
## std.dev        0.82806613   0.43586628    1.7652982   0.76223767      NA
## coef.var       0.14171126   0.14256420    0.4697441   0.63555114      NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can have even more statistics (i.e., skewness, kurtosis and normality test) by adding the argument &lt;code&gt;norm = TRUE&lt;/code&gt; in the previous function. Note that the variable &lt;code&gt;Species&lt;/code&gt; is not numeric, so descriptive statistics cannot be computed for this variable and NA are displayed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-variation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Coefficient of variation&lt;/h1&gt;
&lt;p&gt;The coefficient of variation can be found with &lt;code&gt;stat.desc()&lt;/code&gt; (see the line &lt;code&gt;coef.var&lt;/code&gt; in the table above) or by computing manually (remember that the coefficient of variation is the standard deviation divided by the mean):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(dat$Sepal.Length) / mean(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1417113&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mode&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mode&lt;/h1&gt;
&lt;p&gt;To my knowledge there is no function to find the mode of a variable. However, we can easily find it thanks to the functions &lt;code&gt;table()&lt;/code&gt; and &lt;code&gt;sort()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- table(dat$Sepal.Length) # number of occurrences for each unique value
sort(tab, decreasing = TRUE) # sort highest to lowest&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   5 5.1 6.3 5.7 6.7 5.5 5.8 6.4 4.9 5.4 5.6   6 6.1 4.8 6.5 4.6 5.2 6.2 6.9 7.7 
##  10   9   9   8   8   7   7   7   6   6   6   6   6   5   5   4   4   4   4   4 
## 4.4 5.9 6.8 7.2 4.7 6.6 4.3 4.5 5.3   7 7.1 7.3 7.4 7.6 7.9 
##   3   3   3   3   2   2   1   1   1   1   1   1   1   1   1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;table()&lt;/code&gt; gives the number of occurrences for each unique value, then &lt;code&gt;sort()&lt;/code&gt; with the argument &lt;code&gt;decreasing = TRUE&lt;/code&gt; displays the number of occurrences from highest to lowest. The mode of the variable &lt;code&gt;Sepal.Length&lt;/code&gt; is thus 5. This code to find the mode can also be applied to qualitative variables such as &lt;code&gt;Species&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sort(table(dat$Species), decreasing = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##     setosa versicolor  virginica 
##         50         50         50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(dat$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     setosa versicolor  virginica 
##         50         50         50&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;contingency-table&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Contingency table&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;table()&lt;/code&gt; introduced above can also be used on two qualitative variables to create a contingency table. The dataset &lt;code&gt;iris&lt;/code&gt; has only one qualitative variable so we create a new qualitative variable just for this example. We create the variable &lt;code&gt;size&lt;/code&gt; which corresponds to &lt;code&gt;small&lt;/code&gt; if the length of the petal is smaller than the median of all flowers, &lt;code&gt;big&lt;/code&gt; otherwise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat$size &amp;lt;- ifelse(dat$Sepal.Length &amp;lt; median(dat$Sepal.Length),
  &amp;quot;small&amp;quot;, &amp;quot;big&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a recap of the occurrences by size:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(dat$size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   big small 
##    77    73&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now create a contingency table of the two variables &lt;code&gt;Species&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt; with the &lt;code&gt;table()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(dat$Species, dat$size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
##              big small
##   setosa       1    49
##   versicolor  29    21
##   virginica   47     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with the &lt;code&gt;xtabs()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xtabs(~ dat$Species + dat$size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             dat$size
## dat$Species  big small
##   setosa       1    49
##   versicolor  29    21
##   virginica   47     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The contingency table gives the number of cases in each subgroup. For instance, there is only one big setosa flower, while there are 49 small setosa flowers in the dataset.&lt;/p&gt;
&lt;p&gt;To go further, we can see from the table that setosa flowers seem to be larger in size than virginica flowers. In order to check whether size is significantly associated with species, we could perform a Chi-square test of independence since both variables are categorical variables. See how to do this test &lt;a href=&#34;/blog/chi-square-test-of-independence-by-hand/&#34;&gt;by hand&lt;/a&gt; and &lt;a href=&#34;/blog/chi-square-test-of-independence-in-r/&#34;&gt;in R&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;Species&lt;/code&gt; are in rows and &lt;code&gt;size&lt;/code&gt; in column because we specified &lt;code&gt;Species&lt;/code&gt; and then &lt;code&gt;size&lt;/code&gt; in &lt;code&gt;table()&lt;/code&gt;. Change the order if you want to switch the two variables.&lt;/p&gt;
&lt;p&gt;Instead of having the frequencies (i.e.. the number of cases) you can also have the relative frequencies in each subgroup by adding the &lt;code&gt;table()&lt;/code&gt; function inside the &lt;code&gt;prop.table()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(table(dat$Species, dat$size))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
##                      big       small
##   setosa     0.006666667 0.326666667
##   versicolor 0.193333333 0.140000000
##   virginica  0.313333333 0.020000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you can also compute the percentages by row or by column by adding a second argument to the &lt;code&gt;prop.table()&lt;/code&gt; function: &lt;code&gt;1&lt;/code&gt; for row, or &lt;code&gt;2&lt;/code&gt; for column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# percentages by row:
round(prop.table(table(dat$Species, dat$size), 1), 2) # round to 2 digits with round()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
##               big small
##   setosa     0.02  0.98
##   versicolor 0.58  0.42
##   virginica  0.94  0.06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# percentages by column:
round(prop.table(table(dat$Species, dat$size), 2), 2) # round to 2 digits with round()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
##               big small
##   setosa     0.01  0.67
##   versicolor 0.38  0.29
##   virginica  0.61  0.04&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;barplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Barplot&lt;/h1&gt;
&lt;p&gt;Barplots can only be done on qualitative variables (see the difference with a quantitative variable &lt;a href=&#34;/blog/variable-types-and-examples/&#34;&gt;here&lt;/a&gt;). A barplot is a tool to visualize the distribution of a qualitative variable. We draw a barplot on the qualitative variable &lt;code&gt;size&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(table(dat$size)) # table() is mandatory&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can also draw a barplot of the relative frequencies instead of the frequencies by adding &lt;code&gt;prop.table()&lt;/code&gt; as we did earlier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(prop.table(table(dat$size)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2) # needed each time you open RStudio
# The package ggplot2 must be installed first

ggplot(dat) +
  aes(x = size) +
  geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histogram&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Histogram&lt;/h1&gt;
&lt;p&gt;A histogram gives an idea about the distribution of a quantitative variable. The idea is to break the range of values into intervals and count how many observations fall into each interval. Histograms are a bit similar to barplots, but histograms are used for quantitative variables whereas barplots are used for qualitative variables. To draw a histogram in R, use &lt;code&gt;hist()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Add the arguments &lt;code&gt;breaks =&lt;/code&gt; inside the &lt;code&gt;hist()&lt;/code&gt; function if you want to change the number of bins. A rule of thumb (known as Sturges’ law) is that the number of bins should be the rounded value of the square root of the number of observations. The dataset includes 150 observations so in this case the number of bins can be set to 12.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat) +
  aes(x = Sepal.Length) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By default, the number of bins is 30. You can change this value with &lt;code&gt;geom_histogram(bins = 12)&lt;/code&gt; for instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Boxplot&lt;/h1&gt;
&lt;p&gt;Boxplots are really useful in descriptive statistics and are often underused (mostly because it is not well understood by the public). A boxplot graphically represents the distribution of a quantitative variable by visually displaying five common location summary (minimum, median, first and third quartiles and maximum) and any observation that was classified as a suspected outlier using the interquartile range (IQR) criterion. The IQR criterion means that all observations above &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75} + 1.5 \cdot IQR\)&lt;/span&gt; and below &lt;span class=&#34;math inline&#34;&gt;\(q_{0.25} - 1.5 \cdot IQR\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; correspond to first and third quartile respectively) are considered as potential outliers by R. The minimum and maximum in the boxplot are represented without these suspected outliers. Seeing all these information on the same plot help to have a good first overview of the dispersion and the location of the data. Before drawing a boxplot of our data, see below a graph explaining the information present on a boxplot:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/boxplot.png&#34; alt=&#34;Detailed boxplot. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Detailed boxplot. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now an example with our dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boxplots are even more informative when presented side-by-side for comparing and contrasting distributions from two or more groups. For instance, we compare the length of the sepal across the different species:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(dat$Sepal.Length ~ dat$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat) +
  aes(x = Species, y = Sepal.Length) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scatterplot&lt;/h1&gt;
&lt;p&gt;Scatterplots allow to check whether there is a potential link between two quantitative variables. For instance, when drawing a scatterplot of the length of the sepal and the length of the petal:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(dat$Sepal.Length, dat$Petal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seems to be a positive association between the two variables.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat) +
  aes(x = Sepal.Length, y = Petal.Length) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As boxplots, scatterplots are even more informative when differentiating the points according to a factor, in this case the species:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat) +
  aes(x = Sepal.Length, y = Petal.Length, colour = Species) +
  geom_point() +
  scale_color_hue()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qq-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;QQ-plot&lt;/h1&gt;
&lt;div id=&#34;for-a-single-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;For a single variable&lt;/h2&gt;
&lt;p&gt;In order to check the normality assumption of a variable (normality means that the data follow a normal distribution, also known as a Gaussian distribution), we usually use histograms and/or QQ-plots.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; See an article discussing about the &lt;a href=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/&#34;&gt;normal distribution and how to evaluate the normality assumption in R&lt;/a&gt; if you need a refresh on that subject. Histograms have been presented earlier, so here is how to draw a QQ-plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Draw points on the qq-plot:
qqnorm(dat$Sepal.Length)
# Draw the reference line:
qqline(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or a QQ-plot with confidence bands with the &lt;code&gt;qqPlot()&lt;/code&gt; function from the &lt;code&gt;{car}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(car) # package must be installed first
qqPlot(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 132 118&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If points are close to the reference line (sometimes referred as Henry’s line) and within the confidence bands, the normality assumption can be considered as met. The bigger the deviation between the points and the reference line and the more they lie outside the confidence bands, the less likely that the normality condition is met. The variable &lt;code&gt;Sepal.Length&lt;/code&gt; does not seem to follow a normal distribution because several points lie outside the confidence bands. When facing a non-normal distribution, the first step is usually to apply the logarithm transformation on the data and recheck to see whether the log-transformed data are normally distributed. Applying the logarithm transformation can be done with the &lt;code&gt;log()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggpubr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggpubr)
ggqqplot(dat$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-groups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;By groups&lt;/h2&gt;
&lt;p&gt;For some statistical tests, the normality assumption is required in all groups. One solution is to draw a QQ-plot for each group by manually splitting the dataset into different groups and then draw a QQ-plot for each subset of the data (with the methods shown above). Another (easier) solution is to draw a QQ-plot for each group automatically with the argument &lt;code&gt;groups =&lt;/code&gt; in the function &lt;code&gt;qqPlot()&lt;/code&gt; from the &lt;code&gt;{car}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqPlot(dat$Sepal.Length, groups = dat$size)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(
  sample = Sepal.Length, data = dat,
  col = size, shape = size
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is also possible to differentiate groups by only shape or color. For this, remove one of the argument &lt;code&gt;col&lt;/code&gt; or &lt;code&gt;shape&lt;/code&gt; in the &lt;code&gt;qplot()&lt;/code&gt; function above.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Density plot&lt;/h1&gt;
&lt;p&gt;Density plot is a smoothed version of the histogram and is used in the same concept, that is, to represent the distribution of a numeric variable. The functions &lt;code&gt;plot()&lt;/code&gt; and &lt;code&gt;density()&lt;/code&gt; are used together to draw a density plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(density(dat$Sepal.Length))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat) +
  aes(x = Sepal.Length) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-in-r_files/figure-html/unnamed-chunk-48-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Correlation plot&lt;/h1&gt;
&lt;p&gt;The last type of descriptive plot is a correlation plot, also called a correlogram. This type of graph is more complex than the ones presented above, so it is detailed in a separate article. See &lt;a href=&#34;/blog/correlogram-in-r-how-to-highlight-the-most-correlated-variables-in-a-dataset&#34;&gt;how to draw a correlogram to highlight the most correlated variables in a dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you to do descriptive statistics in R. If you would like to do the same by hand or understand what these statistics represent, read the article “&lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;Descriptive statistics by hand&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Normality tests such as Shapiro-Wilk or Kolmogorov-Smirnov tests can also be used to test whether the data follow a normal distribution or not. However, in practice, normality tests are often considered as too conservative in the sense that for large sample size, a small deviation from the normality may cause the normality condition to be violated. For this reason, it is often the case that the normality condition is verified based on a combination of visual inspections (with histograms and QQ-plots) and formal test (Shapiro-Wilk test for instance).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Descriptive statistics by hand</title>
      <link>/blog/descriptive-statistics-by-hand/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/descriptive-statistics-by-hand/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location-versus-dispersion-measures&#34;&gt;Location versus dispersion measures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location&#34;&gt;Location&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-and-maximum&#34;&gt;Minimum and maximum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mean&#34;&gt;Mean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#median&#34;&gt;Median&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#odd-number-of-observations&#34;&gt;Odd number of observations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#even-number-of-observations&#34;&gt;Even number of observations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mean-vs.median&#34;&gt;Mean vs. median&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#st-and-3rd-quartiles&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1^{st}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(3^{rd}\)&lt;/span&gt; quartiles&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#q_0.25-q_0.75-and-q_0.5&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q_{0.5}\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-note-on-deciles-and-percentiles&#34;&gt;A note on deciles and percentiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mode&#34;&gt;Mode&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mode-for-qualitative-variables&#34;&gt;Mode for qualitative variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dispersion&#34;&gt;Dispersion&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#range&#34;&gt;Range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation&#34;&gt;Standard deviation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation-for-a-population&#34;&gt;Standard deviation for a population&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation-for-a-sample&#34;&gt;Standard deviation for a sample&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance&#34;&gt;Variance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#variance-for-a-population&#34;&gt;Variance for a population&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance-for-a-sample&#34;&gt;Variance for a sample&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation-vs.variance&#34;&gt;Standard deviation vs. variance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#notations&#34;&gt;Notations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interquartile-range&#34;&gt;Interquartile range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-variation&#34;&gt;Coefficient of variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-variation-vs.standard-deviation&#34;&gt;Coefficient of variation vs. standard deviation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/descriptive-statistics-by-hand.jpeg&#34; alt=&#34;Photo by Pop &amp;amp; Zebra&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Pop &amp;amp; Zebra&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This article explains how to compute the main descriptive statistics by hand and how to interpret them. To learn how to compute these measures in R, read the article “&lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;Descriptive statistics in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;Descriptive statistics (in the broad sense of the term) is a branch of statistics aiming at summarizing, describing and presenting a series of values or a dataset. Long series of values without any preparation or without any summary measures are often not informative due to the difficulty of recognizing any pattern in the data. Below an example with the height (in cm) of a population of 100 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt;, &lt;em&gt;173.9&lt;/em&gt;, &lt;em&gt;190.1&lt;/em&gt;, &lt;em&gt;174.1&lt;/em&gt;, &lt;em&gt;195.2&lt;/em&gt;, &lt;em&gt;174.4&lt;/em&gt;, &lt;em&gt;188&lt;/em&gt;, &lt;em&gt;197.9&lt;/em&gt;, &lt;em&gt;161.1&lt;/em&gt;, &lt;em&gt;172.2&lt;/em&gt;, &lt;em&gt;173.7&lt;/em&gt;, &lt;em&gt;181.4&lt;/em&gt;, &lt;em&gt;172.2&lt;/em&gt;, &lt;em&gt;148.4&lt;/em&gt;, &lt;em&gt;150.6&lt;/em&gt;, &lt;em&gt;188.2&lt;/em&gt;, &lt;em&gt;171.9&lt;/em&gt;, &lt;em&gt;157.2&lt;/em&gt;, &lt;em&gt;173.3&lt;/em&gt;, &lt;em&gt;187.1&lt;/em&gt;, &lt;em&gt;194&lt;/em&gt;, &lt;em&gt;170.7&lt;/em&gt;, &lt;em&gt;172.4&lt;/em&gt;, &lt;em&gt;157.4&lt;/em&gt;, &lt;em&gt;179.6&lt;/em&gt;, &lt;em&gt;168.6&lt;/em&gt;, &lt;em&gt;179.6&lt;/em&gt;, &lt;em&gt;182&lt;/em&gt;, &lt;em&gt;185.4&lt;/em&gt;, &lt;em&gt;168.9&lt;/em&gt;, &lt;em&gt;180&lt;/em&gt;, &lt;em&gt;157.8&lt;/em&gt;, &lt;em&gt;167.2&lt;/em&gt;, &lt;em&gt;166.5&lt;/em&gt;, &lt;em&gt;150.9&lt;/em&gt;, &lt;em&gt;175.4&lt;/em&gt;, &lt;em&gt;177.1&lt;/em&gt;, &lt;em&gt;171.4&lt;/em&gt;, &lt;em&gt;182.6&lt;/em&gt;, &lt;em&gt;167.7&lt;/em&gt;, &lt;em&gt;161.3&lt;/em&gt;, &lt;em&gt;179.3&lt;/em&gt;, &lt;em&gt;166.9&lt;/em&gt;, &lt;em&gt;189.4&lt;/em&gt;, &lt;em&gt;170.7&lt;/em&gt;, &lt;em&gt;181.6&lt;/em&gt;, &lt;em&gt;178.2&lt;/em&gt;, &lt;em&gt;167.2&lt;/em&gt;, &lt;em&gt;190.8&lt;/em&gt;, &lt;em&gt;181.4&lt;/em&gt;, &lt;em&gt;175.9&lt;/em&gt;, &lt;em&gt;177.8&lt;/em&gt;, &lt;em&gt;181.8&lt;/em&gt;, &lt;em&gt;175.9&lt;/em&gt;, &lt;em&gt;145.1&lt;/em&gt;, &lt;em&gt;177.8&lt;/em&gt;, &lt;em&gt;171.3&lt;/em&gt;, &lt;em&gt;176.9&lt;/em&gt;, &lt;em&gt;180.8&lt;/em&gt;, &lt;em&gt;189&lt;/em&gt;, &lt;em&gt;167.7&lt;/em&gt;, &lt;em&gt;188&lt;/em&gt;, &lt;em&gt;178.4&lt;/em&gt;, &lt;em&gt;185.4&lt;/em&gt;, &lt;em&gt;184.2&lt;/em&gt;, &lt;em&gt;182.2&lt;/em&gt;, &lt;em&gt;164.6&lt;/em&gt;, &lt;em&gt;174.1&lt;/em&gt;, &lt;em&gt;181.2&lt;/em&gt;, &lt;em&gt;165.5&lt;/em&gt;, &lt;em&gt;169.6&lt;/em&gt;, &lt;em&gt;180.8&lt;/em&gt;, &lt;em&gt;182.7&lt;/em&gt;, &lt;em&gt;179.6&lt;/em&gt;, &lt;em&gt;166.1&lt;/em&gt;, &lt;em&gt;164&lt;/em&gt;, &lt;em&gt;190.1&lt;/em&gt;, &lt;em&gt;177.6&lt;/em&gt;, &lt;em&gt;175.9&lt;/em&gt;, &lt;em&gt;173.8&lt;/em&gt;, &lt;em&gt;163.1&lt;/em&gt;, &lt;em&gt;181.1&lt;/em&gt;, &lt;em&gt;172.8&lt;/em&gt;, &lt;em&gt;173.2&lt;/em&gt;, &lt;em&gt;184.3&lt;/em&gt;, &lt;em&gt;183.2&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Facing this series, it is hard (not to say impossible) for anyone to understand the data and have a clear view of the size of these adults in a reasonable amount of time. Descriptive statistics allow to summarize, and thus have a better overview of the data. Of course, by summarizing data through one or several measures, some information will inevitably be lost. However, in many cases it is generally better to lose some information but in return gain an overview.&lt;/p&gt;
&lt;p&gt;Descriptive statistics is often the first step and an important part in any statistical analysis. It allows to check the quality of the data by detecting potential outliers (i.e., data points that appear to be separated from the rest of the data), collection or encoding errors. It also helps to “understand” the data and if well presented, descriptive statistics is already a good starting point for further analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;location-versus-dispersion-measures&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Location versus dispersion measures&lt;/h1&gt;
&lt;p&gt;Several different measures (called statistics if we are analyzing a sample) are used to summarize the data. Some of them give an understanding about the location of the data, others give and understanding about the dispersion of the data. In practice, both types of measures are often used together in order to summarize the data in the most concise but complete way. We illustrate this point with the graph below, representing the height (in cm) of 100 persons divided into two groups (50 persons in each group):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The black line corresponds to the mean. The mean height (in cm) is similar in both groups. However, it is clear that the dispersion of heights are very different in the two groups. For this reason, location or dispersion measures are often not enough if presented individually and it is a good practice to present several statistics from both types of measures.&lt;/p&gt;
&lt;p&gt;In the following sections, we detail the most common location and dispersion measures and illustrate them with examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;location&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Location&lt;/h1&gt;
&lt;p&gt;Location measures allow to see “where” the data are located, around which values. In other words, location measures give an understanding on what is the central tendency, the “position” of the data as a whole. It includes the following statistics (others exist but we focus on the most common ones):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;minimum&lt;/li&gt;
&lt;li&gt;maximum&lt;/li&gt;
&lt;li&gt;mean&lt;/li&gt;
&lt;li&gt;median&lt;/li&gt;
&lt;li&gt;first quartile&lt;/li&gt;
&lt;li&gt;third quartile&lt;/li&gt;
&lt;li&gt;mode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We detail and compute by hand each of them in the following sections.&lt;/p&gt;
&lt;div id=&#34;minimum-and-maximum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Minimum and maximum&lt;/h2&gt;
&lt;p&gt;Minimum (&lt;span class=&#34;math inline&#34;&gt;\(min\)&lt;/span&gt;) and maximum (&lt;span class=&#34;math inline&#34;&gt;\(max\)&lt;/span&gt;) are simply the lowest and largest values, respectively. Given the height (in cm) of a sample of 6 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt; and &lt;em&gt;173.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The minimum is 169.4 cm and the maximum is 188.7 cm. These two basic statistics give a clear idea about the size of the smallest and tallest of these 6 adults.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean&lt;/h2&gt;
&lt;p&gt;The mean, also known as average, is probably the most common statistics. It gives an idea on what is the average value, that is, the central value of the data or in other words the center of gravity. The mean is found by summing all values and dividing this sum by the number of observations (denoted &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[mean = \bar{x} = \frac{\text{sum of all values}}{\text{number of values}} = \frac{1}{n}\sum^{n}_{i = 1} x_i\]&lt;/span&gt;
Below a visual representation of the mean:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/mean.png&#34; alt=&#34;Mean. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Mean. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Given our sample of 6 adults presented above, the mean is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{x} = \frac{188.7 + 169.4 + 178.6 + 181.3 + 179 + 173.9}{6} = 178.4833\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the mean size, that is, the average size of our sample of 6 adults is 178.48 cm (rounded to 2 decimals).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;median&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Median&lt;/h2&gt;
&lt;p&gt;The median is another measure of location so it also gives an idea about the central tendency of the data. The interpretation of the median is that there are as many observations below as above the median. In other words, 50% of the observations lie below the median, and 50% of the observations lie above the median. Below a visual representation of the median:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/median.png&#34; alt=&#34;Median. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Median. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The easiest way to compute the median is by first sorting the data from lowest to highest (i.e., in ascending order) then take the middle point as the median. From the sorted values, for an odd number of observations, the middle point is easy to find: it is the value with as many observations below as above. Still from the sorted values, for an even number of observations, the middle point is exactly between the two middle values. Formally, after sorting, the median is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; (number of observations) is odd: &lt;span class=&#34;math display&#34;&gt;\[med(x) = x_{\frac{n+1}{2}}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is even: &lt;span class=&#34;math display&#34;&gt;\[med(x) = \frac{1}{2}\big(x_{\frac{n}{2}} + x_{\frac{n}{2} + 1}\big)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the subscript of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; denotes the numbering of the sorted data. The formulas look harder than they really are, so let’s see with two concrete examples.&lt;/p&gt;
&lt;div id=&#34;odd-number-of-observations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Odd number of observations&lt;/h3&gt;
&lt;p&gt;Given the height of a sample of 7 adults taken from the 100 adults presented in the introduction:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We first sort the order from lowest to highest:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt; and &lt;em&gt;188.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given that the number of observations &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is odd (since &lt;span class=&#34;math inline&#34;&gt;\(n = 7\)&lt;/span&gt;), the median is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[med(x) = x_\frac{7 + 1}{2} = x_4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we take the fourth value from the sorted values, which corresponds to 166.4. In conclusion, the median size of these 7 adults is 166.4 cm. As you can see, there are 3 observations below 166.4 and 3 observations above 166.4 cm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;even-number-of-observations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Even number of observations&lt;/h3&gt;
&lt;p&gt;Now let’s see when the number of observations is even, which is slightly more complicated than when the number of observations is odd. Given the height of a sample of 6 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt; and &lt;em&gt;173.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We sort the values in ascending order:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;173.9&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt; and &lt;em&gt;188.7&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given that the number of observations &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is even (since &lt;span class=&#34;math inline&#34;&gt;\(n = 6\)&lt;/span&gt;), the median is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[med(x) = \frac{1}{2}\big(x_{\frac{6}{2}} + x_{\frac{6}{2} + 1}\big) = \frac{1}{2}\big(x_{3} + x_{4}\big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we sum the third and fourth values from the sorted values and divide this sum by 2 (which is equivalent than taking the mean of these two middle values):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{2}(178.6 + 179) = 178.8\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the median size of these 6 adults is 178.8 cm. Again, remark that there are as many observations below as above 178.8 cm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-vs.median&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean vs. median&lt;/h2&gt;
&lt;p&gt;Although the mean and median are often relatively close to each other they should not be confused since they both have advantages and disadvantages in different contexts. Besides the fact that almost everyone knows (or at least have heard about) the mean, it has the advantage that it gives a unique picture for each different series of data. However, it has the disadvantage that the mean is sensible to outliers (i.e., extreme values). On the other hand, the advantage of the median is that it is resistant to outliers and the inconvenient is that it may be the exact same value for very different series of data (so not unique to the data).&lt;/p&gt;
&lt;p&gt;To illustrate the “sensible to outlier” argument, consider 3 friends in a bar comparing their salaries. Their salaries are &lt;em&gt;1800&lt;/em&gt;, &lt;em&gt;2000&lt;/em&gt; and &lt;em&gt;2100&lt;/em&gt;€, for an average (mean) salary of &lt;em&gt;1967&lt;/em&gt;€. A friend of them (who happens to be friend with Bill Gates as well) joins them in the bar. Their salaries are now &lt;em&gt;1800&lt;/em&gt;, &lt;em&gt;2000&lt;/em&gt;, &lt;em&gt;2100&lt;/em&gt; and &lt;em&gt;1000000&lt;/em&gt;€. The average salary of the 4 friends is now &lt;em&gt;251475&lt;/em&gt;€, compared to &lt;em&gt;1967&lt;/em&gt;€ without the rich friend. Although it is statistically correct to say that the average salary of the 4 friends is &lt;em&gt;251475&lt;/em&gt;€, you will concede that this measure does not represent a fair image of the salaries of the 4 friends, as 3 of them earn much less than the average salary. As we have just seen, the mean is sensible to outliers. On the other hand, if we report the medians, we see that the median salary of the 3 first friends is &lt;em&gt;2000&lt;/em&gt;€, and the median salary of the 4 friends is &lt;em&gt;2050&lt;/em&gt;€. As you can see with this example, the median is not sensible to outliers and for series with such extreme value(s), the median is more appropriate compared to the mean as it often gives a better representation of the data. (&lt;em&gt;Note:&lt;/em&gt; this example also shows how a large majority of people earn less than the average salary reported in the news. This is however beyond the scope of the article.)&lt;/p&gt;
&lt;p&gt;Given the previous example, one may then choose to always use the median instead of the mean. However, the median has it own inconvenient which the mean does not have: the median is less unique and less specific to its underlying data than the mean. Consider the following data, representing the grades of 5 students taking a statistics and economics exam:&lt;/p&gt;
&lt;table style=&#34;width:51%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;studentID&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;economics&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;statistics&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The median of the grades is the same in economics and statistics (median = &lt;em&gt;10&lt;/em&gt;). Therefore, had we computed only the medians, we could have concluded that the students performed as well in economics as in statistics. However, although the medians are exactly the same for both classes, it is clear that students performed better in economics than in statistics. In fact, the mean of the grades in economics is &lt;em&gt;13.6&lt;/em&gt; and the mean of the grades in statistics is &lt;em&gt;8.6&lt;/em&gt;. What we have just shown here is that the median is based only on one single value, the middle value, or on the two middle values if there are an even number of observations, while the mean is based on all values (and thus includes more information). The median is therefore not sensible to outliers, but it is also not unique (i.e., not specific) to different series of data, whereas the mean is much more likely to be different and unique for different series of data. This difference in terms of specificity and uniqueness between the two measures may make the mean more useful for data with no outlier.&lt;/p&gt;
&lt;p&gt;In conclusion, depending on the context and the data, it is often more interesting to report the mean or the median, or both. As a last remark regarding the comparison between the two most important location measures, note that when the mean and median are equal, the distribution of your data can often be considered to follow a normal distribution (also referred as Gaussian distribution).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;st-and-3rd-quartiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(1^{st}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(3^{rd}\)&lt;/span&gt; quartiles&lt;/h2&gt;
&lt;p&gt;The first and third quartiles are similar to the median in the sense that they also divide the observations into two parts, except that these parts are not equal. Remind that the median divides the data into two equal parts (with 50% of the observations below and 50% above the median). The first quartile cuts the observations such that there are 25% of the observations &lt;strong&gt;below&lt;/strong&gt; and thus 75% &lt;strong&gt;above&lt;/strong&gt; the first quartile. The third quartile, as you have guessed by now, represents the value with 75% of the observations below it and thus 25% of the observations above it. There exists several methods to compute the first and third quartile (which sometimes give slight differences, R for instance uses a different method), but here is I believe the easiest one when computing these statistics by hand:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;sort the data in ascending order&lt;/li&gt;
&lt;li&gt;compute &lt;span class=&#34;math inline&#34;&gt;\(0.25 \cdot n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0.75 \cdot n\)&lt;/span&gt; (i.e., 0.25 and 0.75 times the number of observations)&lt;/li&gt;
&lt;li&gt;round up these two numbers to the next whole number&lt;/li&gt;
&lt;li&gt;these two numbers represent the rank of the first and third quartile (in the sorted series), respectively&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The steps are the same for both an odd and even number of observations. Here is an example with the following series, representing the height in cm of 9 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We first order from lowest to highest:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt; and &lt;em&gt;188.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are 9 observations so &lt;span class=&#34;math display&#34;&gt;\[0.25 \cdot 9 = 2.25\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[0.75 \cdot 9 = 6.75\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Rounding up to the whole number gives 3 and 7, which represent the rank of the first and third quartiles, respectively. Therefore, the first quartile is 163.9 cm and the third quartile is 181.5 cm.&lt;/p&gt;
&lt;p&gt;In conclusion, 25% of adults are less than 163.9 cm tall (and thus 75% of them are more than 163.9 cm tall), while 75% of adults are less than 181.5 cm tall (and thus 25% of them are more than 181.5 cm tall).&lt;/p&gt;
&lt;div id=&#34;q_0.25-q_0.75-and-q_0.5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q_{0.5}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Note that the first quartile is denoted &lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt; and the third quartile is denoted &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; stands for quartile). As you can see, the median is actually the second quartile and for this reason it is also sometimes denoted &lt;span class=&#34;math inline&#34;&gt;\(q_{0.5}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-note-on-deciles-and-percentiles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A note on deciles and percentiles&lt;/h3&gt;
&lt;p&gt;Deciles and percentiles are similar to quartiles except that they cuts the data in 10 and 100 equal parts. For instance, the &lt;span class=&#34;math inline&#34;&gt;\(4^{th}\)&lt;/span&gt; decile (&lt;span class=&#34;math inline&#34;&gt;\(q_{0.4}\)&lt;/span&gt;) is the value such that there are 40% of the observations below it and thus 60% of the observations above it. Furthermore, the &lt;span class=&#34;math inline&#34;&gt;\(98^{th}\)&lt;/span&gt; percentile (&lt;span class=&#34;math inline&#34;&gt;\(q_{0.98}\)&lt;/span&gt;, also sometimes denoted &lt;span class=&#34;math inline&#34;&gt;\(P98\)&lt;/span&gt;) is the value such that there are 98% of the observations below it and thus 2% of the observations above it. Percentiles are often used for the weight and height of babies, giving precise information to the parents about where their child stands compared to other children of the same age.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mode&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mode&lt;/h2&gt;
&lt;p&gt;The mode of a series is the value that appears most often. In other words, it is the value that has the highest number of occurrences. Given the height of 9 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;170&lt;/em&gt;, &lt;em&gt;168&lt;/em&gt;, &lt;em&gt;171&lt;/em&gt;, &lt;em&gt;170&lt;/em&gt;, &lt;em&gt;182&lt;/em&gt;, &lt;em&gt;165&lt;/em&gt;, &lt;em&gt;170&lt;/em&gt;, &lt;em&gt;189&lt;/em&gt; and &lt;em&gt;167&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mode is 170 because it is the most common value with 3 occurrences. All other values appear only once. In conclusion, most adults of this sample are 170 cm tall. Note that it is possible that a series has no mode (e.g., &lt;em&gt;4&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt; and &lt;em&gt;10&lt;/em&gt;) or more than one mode (e.g., &lt;em&gt;4&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt; and &lt;em&gt;11&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Data with two modes are often called bimodal and data with more than two modes are often called multimodal, as opposed to series with one mode which are referred as unimodal.&lt;/p&gt;
&lt;div id=&#34;mode-for-qualitative-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mode for qualitative variables&lt;/h3&gt;
&lt;p&gt;Unlike the previous descriptive statistics (i.e., min, max, mean, median, first and third quartile) that can only be computed for quantitative variables, the mode can be computed for quantitative &lt;strong&gt;and&lt;/strong&gt; qualitative variables (see a recap of the different &lt;a href=&#34;/blog/variable-types-and-examples/&#34;&gt;types of variables&lt;/a&gt; if you do not remember the difference). Given the eye color of the 9 adults presented above:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;blue&lt;/em&gt;, &lt;em&gt;blue&lt;/em&gt;, &lt;em&gt;blue&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt; and &lt;em&gt;green&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mode is brown, so most adults of this sample have brown eyes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dispersion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dispersion&lt;/h1&gt;
&lt;p&gt;All previous descriptive statistics helps to get a sense of the location and position of the data. We now present the most common dispersion measures, which help to get a sense of the dispersion and the variability of the data (to which extent a distribution is squeezed or stretched):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;range&lt;/li&gt;
&lt;li&gt;standard deviation&lt;/li&gt;
&lt;li&gt;variance&lt;/li&gt;
&lt;li&gt;interquartile range&lt;/li&gt;
&lt;li&gt;coefficient of variation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As for location measures, we detail and compute by hand each of these statistics one by one.&lt;/p&gt;
&lt;div id=&#34;range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Range&lt;/h2&gt;
&lt;p&gt;The range is the difference between the maximum and the minimum value:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[range = max - min\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given the height (in cm) of our sample of 6 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt; and &lt;em&gt;173.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The range is 188.7 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 169.4 &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; 19.3 cm. The advantage of the range is that it is extremely easy to compute it and it gives a precise idea on what are the possible values in the data. The disadvantage is that it relies on the two most extreme values only.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-deviation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard deviation&lt;/h2&gt;
&lt;p&gt;The standard deviation is the most common dispersion measure in statistics. Like the mean for the location measures, if we have to present one statistics which summarizes the spread of the data, it is usually the standard deviation. As its name suggests, the standard deviation tells what is the “normal” deviation of the data. It actually computes the average deviation from the &lt;strong&gt;mean&lt;/strong&gt;. The larger the standard deviation, the more scattered the data are. On the contrary, the smaller the standard deviation, the more the data are centred around the mean. Below a visual representation of the standard deviation:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/standard-deviation.png&#34; alt=&#34;Standard deviation. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard deviation. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The standard deviation is a bit more complex than the previous statistics in the sense that there are two formulas depending on whether we face a sample or a population. A population includes all members from a specified group, all possible outcomes or measurements that are of interest. A sample consists of some observations drawn from the population, so a part or a subset of the population. For instance, the population may be “&lt;strong&gt;all&lt;/strong&gt; people living in Belgium” and the sample may be “&lt;strong&gt;some&lt;/strong&gt; people living in Belgium”. Read this article on &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;the difference between population and sample&lt;/a&gt; if you want to learn more.&lt;/p&gt;
&lt;div id=&#34;standard-deviation-for-a-population&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standard deviation for a population&lt;/h3&gt;
&lt;p&gt;The standard deviation for a population, denoted &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma = \sqrt{\frac{1}{n}\sum^n_{i = 1}(x_i - \mu)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As you can see from the formula, the standard deviation is actually the average deviation of the data from their mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Note the square for the difference between the observations and the mean to avoid that negative differences are compensated by positive differences.&lt;/p&gt;
&lt;p&gt;For the sake of easiness, imagine a population of only 3 adults (the steps are the same with a large population, the computation is just longer). Below their heights (in cm):&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mean is 172.6 (rounded to 1 decimal). The standard deviation is thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma = \sqrt{\frac{1}{3} \big[(160.4 - 172.6)^2 + (175.8 - 172.6)^2 + (181.5 - 172.6)^2 \big]}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sigma = 8.91\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the standard deviation for the heights of these 3 adults is 8.91 cm. This means that, on average, the height of the adults in this population deviates from the mean by 8.91 cm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-deviation-for-a-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standard deviation for a sample&lt;/h3&gt;
&lt;p&gt;The standard deviation for a sample is similar to the standard deviation for a population except that we divide by &lt;span class=&#34;math inline&#34;&gt;\(n -1\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and it is denoted &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{1}{n-1}\sum^n_{i = 1}(x_i - \bar{x})^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Imagine now that the 3 adults presented in the previous section is a sample instead of a population:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mean is still 172.6 (rounded to 1 decimal) since the mean is the same whether it is a population or a sample. The standard deviation is now:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{1}{3 - 1} \big[(160.4 - 172.6)^2 + (175.8 - 172.6)^2 + (181.5 - 172.6)^2 \big]}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[s = 10.92\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the standard deviation for the heights of these 3 adults is 10.92 cm. The interpretation is the same than for a population.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance&lt;/h2&gt;
&lt;p&gt;The variance is simply the square of the standard deviation. Put it another way, the standard deviation is the square root of the variance. We also distinguish between the variance for a population and for a sample.&lt;/p&gt;
&lt;div id=&#34;variance-for-a-population&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance for a population&lt;/h3&gt;
&lt;p&gt;The variance for a population, denoted &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2 = \frac{1}{n}\sum^n_{i = 1}(x_i - \mu)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the formula for variance is the same than for standard deviation, except that the square root is removed for the variance. Remember the heights of our population of 3 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The standard deviation was 8.91 cm, so the variance of the height of these adults is &lt;span class=&#34;math inline&#34;&gt;\(8.91^2 = 79.39\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(cm^2\)&lt;/span&gt; (see below why the unit of a variance is &lt;span class=&#34;math inline&#34;&gt;\(unit^2\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-for-a-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance for a sample&lt;/h3&gt;
&lt;p&gt;Again, the variance for a sample is similar to the variance for a population except that we divide by &lt;span class=&#34;math inline&#34;&gt;\(n -1\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and it is denoted &lt;span class=&#34;math inline&#34;&gt;\(s^2\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s^2 = \frac{1}{n-1}\sum^n_{i = 1}(x_i - \bar{x})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Imagine again that the 3 adults in the previous section is a sample instead of a population:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The standard deviation for this sample was 10.92 cm, so the variance of the height of these adults is 119.14 &lt;span class=&#34;math inline&#34;&gt;\(cm^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-deviation-vs.variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard deviation vs. variance&lt;/h2&gt;
&lt;p&gt;Standard deviation and variance are often used interchangeably and both quantify the spread of a given dataset by measuring how far the observations are from their mean. However, the standard deviation can be more easily interpreted because the unit for the standard deviation is the same than the unit of measurement of the data (while it is the &lt;span class=&#34;math inline&#34;&gt;\(unit^2\)&lt;/span&gt; for the variance). Following our example of adult heights in cm, the standard deviation is measured in cm while the variance is measured in &lt;span class=&#34;math inline&#34;&gt;\(cm^2\)&lt;/span&gt;. The fact that the standard deviation keeps the same unit than the initial unit of measurement makes it more interpretable and thus often more used in practice.&lt;/p&gt;
&lt;div id=&#34;notations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notations&lt;/h3&gt;
&lt;p&gt;For completeness, below a table showing the different notations for variance and standard deviation in case of population and sample:&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Population&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sample&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Standard deviation&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(s^2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interquartile-range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interquartile range&lt;/h2&gt;
&lt;p&gt;Remember the first &lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt; and third quartile &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; presented earlier. The interquartile range is another measure of dispersion of the data, using the quartiles. It is the difference between the third and first quartile:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IQR = q_{0.75} - q_{0.25}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Considering the height of the 9 adults presented in the section about the first and third quartile:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The first quartile was 163.9 cm and the third quartile was 181.5 cm. The IQR is thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IQR = 181.5 - 163.9 = 17.6\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the interquartile range is 17.6 cm. The interquartile range is actually the range (since it is the difference between a higher and a lower value) of the middle data. The graph below may help to understand better the IQR and the quartiles:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/IQR-quartiles.png&#34; alt=&#34;IQR, first and third quartile. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;IQR, first and third quartile. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-variation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficient of variation&lt;/h2&gt;
&lt;p&gt;The last dispersion measure is the coefficient of variation. The coefficient of variation, denoted &lt;span class=&#34;math inline&#34;&gt;\(CV\)&lt;/span&gt;, is the standard deviation divided by the mean. Formally:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[CV = \frac{s}{\bar{x}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consider the height of a sample of 4 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mean is &lt;span class=&#34;math inline&#34;&gt;\(\bar{x} =\)&lt;/span&gt; 170.35 cm and the standard deviation is &lt;span class=&#34;math inline&#34;&gt;\(s =\)&lt;/span&gt; 9.95 cm. (Find the same values as an exercise!) The coefficient of variation is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[CV = \frac{9.95 \text{ cm}}{170.35 \text{ cm}} = 0.058 = 5.8\%\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the coefficient of variation is 5.8%. Note that, as a rule of thumb, a coefficient of variation greater than 15% usually means that the data are heterogeneous while a coefficient of variation equal to or less than 15% means that the data are homogeneous. Given that the coefficient of variation equals 5.8% in this case, we can conclude that these 4 adults are homogeneous in terms of height.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-variation-vs.standard-deviation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficient of variation vs. standard deviation&lt;/h2&gt;
&lt;p&gt;Although the coefficient of variation is rather unknown to the public, it is, in fact, worth presenting when making descriptive statistics.&lt;/p&gt;
&lt;p&gt;The standard deviation should always be understood in the context of the mean of the data and is dependent on its unit. The standard deviation has the advantage that it tells by how far on average the data is from the mean in terms of unit in which the data has been measured. Standard deviation is useful when considering variables with same units and approximately same means. However, standard deviation becomes less useful when comparing variables with different units or widely different means. For instance, a variable with a standard deviation of 10 cm cannot be compared to a variable with a standard deviation of 10€ to conclude which one is the most dispersed.&lt;/p&gt;
&lt;p&gt;The coefficient of variation is a ratio of two statistics with the same units. It has thus no unit and is independent of the unit in which the data has been measured. Being unit-free, coefficients of variation computed on datasets or variables with different units or widely different means can be compared to conclude, in fine, which data or variables is more (or less) dispersed. For instance, consider a sample of 10 women with their heights in cm and their salaries in €. The coefficients of variation are 0.032 and 0.061 respectively for the height and the salary. We can conclude that, relative to their respective average, their salaries vary more than their heights for these women (since the coefficient of variation is larger for the salary compared to the coefficient variation for the height).&lt;/p&gt;
&lt;p&gt;This concludes a relatively long article, thanks for reading! I hope the article helped you to understand and compute the different descriptive statistics by hand. If you would like to learn how to compute these measures in R, read the article “&lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;Descriptive statistics in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What is the difference between population and sample?</title>
      <link>/blog/what-is-the-difference-between-population-and-sample/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/what-is-the-difference-between-population-and-sample/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/what-is-the-difference-between-population-and-sample_files/difference-between-population-and-sample.jpeg&#34; alt=&#34;Photo by Roland Denes&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Roland Denes&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;People often fail to properly distinguish between population and sample. It is however essential in any statistical analysis, starting from &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;descriptive statistics&lt;/a&gt; with different formulas for variance and standard deviation depending on whether we face a sample or a population. Moreover, the branch of statistics called &lt;a href=&#34;/blog/a-shiny-app-for-inferential-statistics-by-hand/&#34;&gt;inferential statistics&lt;/a&gt; is often defined as the science of drawing conclusions about a population from observations made on a representative sample of that population. It is therefore crucial to properly distinguish between the two concepts. So, what exactly is the difference between population and sample?&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/what-is-the-difference-between-population-and-sample_files/population-sample.png&#34; alt=&#34;Population versus sample. Source: towardsdatascience.com&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Population versus sample. Source: towardsdatascience.com&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A population includes &lt;strong&gt;all members&lt;/strong&gt; from a specified group, all possible outcomes or measurements that are of interest. The exact population will depend on the scope of the study. For example, say you would like to know whether there is an association between job performance and the amount of home working hours per week in the specific case of Belgian data scientists. In this case, the population might be Belgian data scientists. However, if the scope of the study is more narrow (e.g., the study focuses on french-speaking Belgian data scientists who live at least 30km away from their workplace), then the population will be more specific and include only workers who meet the criteria. The point is that the population should only include people to whom the results will apply.&lt;/p&gt;
&lt;p&gt;A sample consists of some observations drawn from the population, so a part or a &lt;strong&gt;subset of the population&lt;/strong&gt;. The sample is the group of elements who actually participated in the study.&lt;/p&gt;
&lt;p&gt;Members and elements are defined in the broad sense of the term. It may be human. For instance, the population may be “&lt;strong&gt;all&lt;/strong&gt; people living in Belgium” and the sample may be “&lt;strong&gt;some&lt;/strong&gt; people living in Belgium”. It can be anything else too. Say you are testing the effect of a new fertilizer on crop yield. All the crop fields represent your population, whereas the 10 crop fields you tested correspond to your sample. Since a sample is a subset of a population, a sample is always smaller than the population.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Note that, a population must not necessarily be large. It might be the case that you study such a narrow population (e.g., first-year male bachelor students from your university who passed the statistics exam in June and for whom their parents have been divorced for more than 5 years), that the size of the population is actually rather small.&lt;/p&gt;
&lt;p&gt;As mentioned at the beginning of this article, one of the main concern in statistics is being able to draw conclusions about a population from a representative sample. Why using a sample of the population and not directly the population? In general it is almost always impossible to carry out measurements for the entire study population because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the population is too large. Example: the population of pregnant women. If we want to take measurements on all pregnant women in the world, it will most likely either take too long or cost too much&lt;/li&gt;
&lt;li&gt;the population is virtual. In this case “virtual” population is understood as a “hypothetical” population: it is unlimited in size. Example: for an experimental study, we focus on men with prostate cancer treated with a new treatment. We do not know how many people will be treated, so the population varies, is infinite and uncountable at the present time, and therefore virtual&lt;/li&gt;
&lt;li&gt;the population is not easily reachable. Example: the population of homeless persons in Belgium&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For these reasons, measurements are made on a subgroup of observations from the population, i.e., on a sample of our population. These measures are then used to draw conclusions about the population of interest. With an appropriate methodology, the results obtained on a sample are often almost as accurate as those that would be obtained on the entire population.&lt;/p&gt;
&lt;p&gt;Of course, the sample must be selected to be representative of the population under study. If participants are included in a study on a voluntary basis, there is a serious concern that the resulting sample may not be representative of the population. It may be the case that volunteers are different in terms of the parameter&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; of interest, leading to a selection bias. Another selection bias can occur when, for instance, a researcher collects citizens’ wage, by the means of internet. It might be the case that people having access to internet have different wages than people who do not have access.&lt;/p&gt;
&lt;p&gt;The gold standard to select a sample representative of the population under study is by selecting a &lt;strong&gt;random&lt;/strong&gt; sample. A random sample is a sample selected at random from the population so that each member of the population has an equal chance of being selected. A random sample is usually an unbiased sample, that is, a sample whose randomness is not in doubt.&lt;/p&gt;
&lt;p&gt;In some situations (e.g., in medicine) it is complicated or even impossible to obtain a random sample of the population. In such cases, it will be important to consider how representative the resulting sample will be. Last but not least, a paired sample is a sample in which groups (often pairs) of experimental units are linked together by the same experimental conditions. For example, one may measure the hours of sleep for 20 individuals before taking a sleeping pill, and then repeat the measurements on the same individuals after they have taken a sleeping pill. The two measurements for each individual (hours of sleep before and after the sleeping pill) are of course related. Statistical tools accounting for a relation between the samples exist and should be preferred in that case.&lt;/p&gt;
&lt;p&gt;To summarize, the sample is the group of individuals who participated in the study and the population is the broader group to whom the results will apply. Measurements on the entire population is too complex or impossible, so representative samples are used to draw conclusions about the population. Samples based on a random selection are often the most representative samples.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope the article helped you to understand the difference between population and sample.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;There can be, however, many different samples from the same population. All samples combined together can therefore be larger than the population. This is beyond the scope of this article, and at the moment we assume there is only one sample from a specified population.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The tools used to describe a population are called parameters, whereas the tools used to describe a sample are referred as statistics. See the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;most common statistics for a sample&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Shiny app for inferential statistics by hand</title>
      <link>/blog/a-shiny-app-for-inferential-statistics-by-hand/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/a-shiny-app-for-inferential-statistics-by-hand/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/a-shiny-app-for-inferential-statistics_files/Screenshot%202020-02-04%20at%2011.36.38.png&#34; alt=&#34;A Shiny app for inferential statistics: hypothesis tests and confidence intervals&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;A Shiny app for inferential statistics: hypothesis tests and confidence intervals&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Statistics is divided into four main branches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Descriptive statistics&lt;/li&gt;
&lt;li&gt;Inferential statistics&lt;/li&gt;
&lt;li&gt;Predictive analysis&lt;/li&gt;
&lt;li&gt;Exploratory analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Descriptive statistics provide a summary of the data; it helps explaining the data in a concise way without losing too much information. Data can be summarized numerically or graphically. See &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;descriptive statistics by hand&lt;/a&gt; or &lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;in R&lt;/a&gt; to learn more about this branch of statistics.&lt;/p&gt;
&lt;p&gt;The branch of predictive analysis aims at predicting a dependent variable based on one or several independent variables. Depending on the type of data to be predicted, it often encompasses methods such as regression or classification.&lt;/p&gt;
&lt;p&gt;Exploratory analyses focus on using graphical approaches to delve into the data and identify the relationships that exist between the different variables in the dataset. They are therefore more akin to data visualization.&lt;/p&gt;
&lt;p&gt;Inferential statistics uses a random sample of data taken from a population to make inferences, i.e., to draw conclusions about the population (see the &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;difference between population and sample&lt;/a&gt;). In other words, information from the sample is used to make generalizations about the parameter of interest in the population. The two major tools in inferential statistics are confidence intervals and hypothesis tests. Here is a Shiny app which helps you to use these two tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-201/&#34;&gt;Statistics-201&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This Shiny app focuses on confidence intervals and hypothesis tests for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 and 2 means (with unpaired and paired samples)&lt;/li&gt;
&lt;li&gt;1 and 2 proportions&lt;/li&gt;
&lt;li&gt;1 and 2 variances&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the entire code in case you would like to enhance it (see an example on how to use this app after the embedded code):&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/AntoineSoetewey/296d78c473561254eaaff60395488fa6.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Note that the link may not work if the app has hit the monthly usage limit. Try again later if that is the case.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;how-to-use-this-app&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to use this app?&lt;/h1&gt;
&lt;p&gt;Follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the app via this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-201/&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Choose the parameter(s) you want to do inference for (i.e., mean(s), proportion(s) or variance(s))&lt;/li&gt;
&lt;li&gt;Write your data in Sample. Observations are separated by a comma and the decimal is a point&lt;/li&gt;
&lt;li&gt;Set the null and alternative hypothesis&lt;/li&gt;
&lt;li&gt;Select the significance level (most of the time &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the results panel (on the right side or below depending on the size of your screen), you will see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a recap of your sample together with some appropriate descriptive statistics&lt;/li&gt;
&lt;li&gt;the confidence interval&lt;/li&gt;
&lt;li&gt;the hypothesis test&lt;/li&gt;
&lt;li&gt;the interpretation&lt;/li&gt;
&lt;li&gt;and an illustration of the hypothesis test&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All formulas, steps and computations to arrive at the final results are also provided.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope you will find this app useful to do inferential statistics and in particular confidence interval and hypothesis testing by hand.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fshiny%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Shiny app for simple linear regression by hand and in R</title>
      <link>/blog/a-shiny-app-for-simple-linear-regression-by-hand-and-in-r/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/a-shiny-app-for-simple-linear-regression-by-hand-and-in-r/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/a-shiny-app-for-simple-linear-regression_files/Screenshot%202020-02-04%20at%2011.45.09.png&#34; alt=&#34;A Shiny app to perform simple linear regression (by hand and in R)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;A Shiny app to perform simple linear regression (by hand and in R)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Simple linear regression is a statistical method to summarize and study relationships between two variables. When more than two variables are of interest, it is referred as multiple linear regression.&lt;/p&gt;
&lt;p&gt;In this article, we focus only on a Shiny app which allows to perform simple linear regression by hand and in R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-202/&#34;&gt;Statistics-202&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the entire code in case you would like to enhance it (see how to use this app after the embedded code):&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/AntoineSoetewey/dfff4a22ce1a35b898d559c065084733.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;For further details about what is linear regression and when it is used, please see the numerous resources on the topic available in textbooks and online.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that the link may not work if the app has hit the monthly usage limit. Try again later if that is the case.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;how-to-use-this-app&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to use this app?&lt;/h1&gt;
&lt;p&gt;Follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the app via this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-202/&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Enter your data in the x and y fields. The x field corresponds to the independent variable, while the y field corresponds to the dependent variable&lt;/li&gt;
&lt;li&gt;If you do not want to display the confidence interval around the regression line, uncheck the checkbox under Plot&lt;/li&gt;
&lt;li&gt;Change the x and y-axis labels for the regression plot if needed&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the results panel (on the right side or below depending on the size of your screen), you will see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a recap of your dataset together with some appropriate descriptive statistics&lt;/li&gt;
&lt;li&gt;the estimates &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and the regression model computed by hand&lt;/li&gt;
&lt;li&gt;the results of the model computed in R&lt;/li&gt;
&lt;li&gt;the regression plot with some key measures&lt;/li&gt;
&lt;li&gt;and the interpretations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All formulas, steps and computations to arrive at the final results are also provided. Note that it is your responsibility to check the validity of your linear model. This app only serves you to compute the results of the linear model given the data but it does not check whether the assumptions are met. Last but not least, you can download a report of the results by clicking on the Download button. You can choose the format of the report (i.e., HTML, PDF or Word) and whether you want to include the R code or not.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope you will find this app useful to do simple linear regression by hand and in R.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fshiny%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A guide on how to read statistical tables</title>
      <link>/blog/a-guide-on-how-to-read-statistical-tables/</link>
      <pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/a-guide-on-how-to-read-statistical-tables/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/a-guide-on-how-to-read-statistics-table_files/Probability-distributions-statsandr.com.png&#34; alt=&#34;Shiny app to compute probabilities for the main probability distributions&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Shiny app to compute probabilities for the main probability distributions&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Below a Shiny app to help you read the main statistical tables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-101/&#34;&gt;Statistics-101&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This Shiny app helps you to compute probabilities for the main probability distributions.&lt;/p&gt;
&lt;p&gt;Here is the entire code in case you would like to enhance it (see an example on how to use this app after the embedded code):&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/AntoineSoetewey/b0cbddf52fc42afe379551536f223fb7.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Note that the link may not work if the app has hit the monthly usage limit. Try again later if that is the case.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;how-to-use-this-app&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to use this app?&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the app via this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-101/&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Choose the distribution&lt;/li&gt;
&lt;li&gt;Set the parameter(s) of the distribution (the parameters depend of course on the chosen distribution)&lt;/li&gt;
&lt;li&gt;Select whether you want to find the lower tail, upper tail or an interval&lt;/li&gt;
&lt;li&gt;Choose the value of x&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the right panel (or below depending on the size of your screen) you will see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a recap of the data you just entered&lt;/li&gt;
&lt;li&gt;the numerical solution (i.e., the probability)&lt;/li&gt;
&lt;li&gt;a visualization of the solution&lt;/li&gt;
&lt;li&gt;the probability density function together with the mean, the standard deviation and the variance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;Here is an example with the most common distribution: the &lt;strong&gt;normal distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Imagine the following problem: The cost of weekly maintenance and repair of a business has been observed over a long period of time and turns out to be distributed according to a normal distribution with an average of 402€ and a standard deviation of 22€. Having set a budget of 439€ for next week, what is the probability that the cost exceeds this budget?&lt;/p&gt;
&lt;p&gt;To solve this problem, follow these steps in the app:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose the normal distribution, as it is said that the costs follow a normal distribution&lt;/li&gt;
&lt;li&gt;Set the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; equal to 402, as it is said that the average cost is 402€&lt;/li&gt;
&lt;li&gt;In the statement, the standard deviation is given (and not the variance) so select “Standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;” and set it equal to 22&lt;/li&gt;
&lt;li&gt;We are asked what is the probability the the cost &lt;strong&gt;exceeds&lt;/strong&gt; the budget. Therefore, we look for the probability &lt;strong&gt;above&lt;/strong&gt; a certain x, so select upper tail &lt;span class=&#34;math inline&#34;&gt;\(P(X &amp;gt; x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;We are now asked to find the probability that the cost exceeds 439€, so set x equal to 439&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The solution panel gives a recap of the data:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X ∼ \mathcal{N}(\mu = 402, \sigma^2 = 484)\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(484 = 22^2\)&lt;/span&gt;, and the solution: &lt;span class=&#34;math display&#34;&gt;\[P(X &amp;gt; 439) = P(Z &amp;gt; 1.68) = 0.0463\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{X - \mu}{\sigma} = \frac{439 - 402}{22} = 1.68\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z ∼ \mathcal{N}(\mu = 0, \sigma^2 = 1)\)&lt;/span&gt; (known as the standard normal distribution). Thus, the probability that the cost next week exceeds the budget of 439€ is 0.0463, or 4.63%.&lt;/p&gt;
&lt;p&gt;It also shows the normal distribution (with &lt;span class=&#34;math inline&#34;&gt;\(\mu = 402\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 484\)&lt;/span&gt;) with the shaded area corresponding to the probability we are looking for. It then gives some details about the density function, the mean, the standard deviation and the variance.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope you will find this app useful to compute probabilities for the main distributions.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fshiny%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Variable types and examples</title>
      <link>/blog/variable-types-and-examples/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/variable-types-and-examples/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#big-picture&#34;&gt;Big picture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantitative&#34;&gt;Quantitative&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#discrete&#34;&gt;Discrete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous&#34;&gt;Continuous&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qualitative&#34;&gt;Qualitative&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nominal&#34;&gt;Nominal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ordinal&#34;&gt;Ordinal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variable-transformations&#34;&gt;Variable transformations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#from-continuous-to-discrete&#34;&gt;From continuous to discrete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#from-quantitative-to-qualitative&#34;&gt;From quantitative to qualitative&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-notes&#34;&gt;Additional notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This article presents the different variable types from a statistical point of view. To learn about the different data types in R, read “&lt;a href=&#34;/blog/data-types-in-r/&#34;&gt;Data types in R&lt;/a&gt;”.&lt;/p&gt;
&lt;div id=&#34;big-picture&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Big picture&lt;/h1&gt;
&lt;p&gt;In statistics, variables are classified into 4 different types:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph {\n\n\n\n\n  \&#34;1\&#34; [label = \&#34;Variable\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Variable\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;2\&#34; [label = \&#34;Qualitative\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Qualitative\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;3\&#34; [label = \&#34;Nominal\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Nominal\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;4\&#34; [label = \&#34;Ordinal\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Ordinal\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;5\&#34; [label = \&#34;Quantitative\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Quantitative\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;6\&#34; [label = \&#34;Discrete\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Discrete\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;7\&#34; [label = \&#34;Continuous\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Continuous\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;1\&#34;-&gt;\&#34;2\&#34; \n  \&#34;1\&#34;-&gt;\&#34;5\&#34; \n  \&#34;2\&#34;-&gt;\&#34;3\&#34; \n  \&#34;2\&#34;-&gt;\&#34;4\&#34; \n  \&#34;5\&#34;-&gt;\&#34;6\&#34; \n  \&#34;5\&#34;-&gt;\&#34;7\&#34; \n}&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;quantitative&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantitative&lt;/h1&gt;
&lt;p&gt;A quantitative variable is a variable that reflects a notion of magnitude, that is, if the values it can take are numbers. A quantitative variable represents thus a measure and is numerical.&lt;/p&gt;
&lt;p&gt;Quantitative variables are divided into two types: discrete and continuous.&lt;/p&gt;
&lt;div id=&#34;discrete&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Discrete&lt;/h2&gt;
&lt;p&gt;Quantitative discrete variables are variables for which the values it can take are &lt;strong&gt;countable&lt;/strong&gt; and have a &lt;strong&gt;finite number of possibilities&lt;/strong&gt;. The values are often (but not always) integers. Here are some examples of discrete variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of children per family&lt;/li&gt;
&lt;li&gt;Number of students in a class&lt;/li&gt;
&lt;li&gt;Number of citizens of a country&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even if it would take a long time to count the citizens of a large country, it is still doable. Moreover, for all examples, the number of possibilities is finite. Whatever the number of children in a family, it will never be 3.58 or 7.912 so the number of possibilities is a finite number and thus countable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous&lt;/h2&gt;
&lt;p&gt;On the other hand, continuous variables are variables for which the values are &lt;strong&gt;not countable&lt;/strong&gt; and have an &lt;strong&gt;infinite number of possibilities&lt;/strong&gt;. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Weight&lt;/li&gt;
&lt;li&gt;Height&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For simplicity, we usually referred to years, kilograms (or pounds) and centimeters (or feet and inches) for age, weight and height respectively. However, a 28-year-old man could actually be 28 years, 7 months, 16 days, 3 hours, 4 minutes, 5 seconds, 31 milliseconds, 9 nanoseconds old. For all measurements, we usually stop at a standard level of granularity, but nothing (except our measurement tools) prevents us from going deeper, leading to an infinite number of potential values. The fact that the values can take an infinite number of possibilities makes it uncountable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;qualitative&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Qualitative&lt;/h1&gt;
&lt;p&gt;In opposition to quantitative variables, qualitative variables (also referred as categorical variables or factors) are variables that are not numerical and which values fits into categories. In other words, a qualitative variable is a variable which takes as its values modalities, categories or even levels, in contrast to quantitative variables which measure a quantity on each individual.&lt;/p&gt;
&lt;p&gt;Qualitative variables are divided into two types: nominal and ordinal.&lt;/p&gt;
&lt;div id=&#34;nominal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nominal&lt;/h2&gt;
&lt;p&gt;A nominal variable is a qualitative variable where no ordering is possible or implied in the levels. For example, the variable gender is nominal because there is no order in the levels female/male. Eye color is another example of a nominal variable because there is no order among blue, brown or green eyes. A nominal variable can have between two levels (e.g., do you smoke? Yes/No or what is your gender? Female/Male) and a large number of levels (what is your college major? Each major is a level in that case).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ordinal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ordinal&lt;/h2&gt;
&lt;p&gt;On the other hand, an ordinal variable is a qualitative variable with an order implied in the levels. For instance, the variable “severity of road accidents” is ordinal because there is a clear order in the levels light/moderate/fatal.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variable transformations&lt;/h1&gt;
&lt;p&gt;There exists two main variable transformations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;From a continuous to a discrete variable&lt;/li&gt;
&lt;li&gt;From a quantitative to a qualitative variable&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;from-continuous-to-discrete&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From continuous to discrete&lt;/h2&gt;
&lt;p&gt;Let’s say we are interested in babies’ ages. The data collected is the age of the babies, so a continuous variable. However, we may work with only the number of weeks since birth and thus transforming the age into a discrete variable. The variable age remains a continuous variable but the variable we are working on (i.e., the number of weeks since birth) is a discrete variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-quantitative-to-qualitative&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From quantitative to qualitative&lt;/h2&gt;
&lt;p&gt;Let’s say we are interested in the Body Mass Index (BMI). For this, a researcher collects data on height and weight of individuals and computes the BMI. The BMI is a continuous variable but the researcher may want to turn it into a qualitative variable by categorizing individuals below a certain threshold as underweighted, above a certain threshold as overweighted and the rest as normal weight. The BMI is a continuous variable but it has been transformed to another variable, which is now a qualitative (ordinal) variable.&lt;/p&gt;
&lt;p&gt;Same goes for age when age is transformed to an ordinal variable with levels such as minors, adults and seniors. It is also often the case (especially in surveys) that the variable salary (continuous) is transformed into an ordinal variable with different range of salaries (e.g., &amp;lt; 1000€, 1000 - 2000€, &amp;gt; 2000€).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-notes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Additional notes&lt;/h1&gt;
&lt;p&gt;The reason why we often class variables into different types is because not all statistical analyses can be performed on all variable types. For instance, it is impossible to compute the mean of the variable “hair color” as you cannot add brown and blond hair. On the other hand, finding the mode of a continuous variable does not really make any sense because most of the time there will not be two exact same values, so there will be no mode. And even in the case there is a mode, there will be very few observations with this value. As an example, try finding the mode of the height of the students in your class. If you are lucky, a couple of students will have the same size. However, most of the time, every student will have a different size (especially if the measurements include several decimals) and thus there will be no mode. To see what kind of analysis is possible on each type of variable, see “&lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;Descriptive statistics by hand&lt;/a&gt;” or “&lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;Descriptive statistics in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;Last but not least, in datasets it is very often the case that numbers are assigned to qualitative variables. For instance, a study may assign the number “1” to women and the number “2” to men (or “0” to the answer “No” and “1” to the answer “Yes”). Despite the numerical classification, the variable gender is still a qualitative variable and not a discrete variable as it may look. The numerical classification is used only for data analysis. It is indeed easier to write “1” or “2” instead of “women” or “men”, and thus less prone to encoding errors. If you face this kind of setup, do not forget to transform your variable into the right type before performing any statistical analyses.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you to understand the different types of variable. If you would like to learn more about the different data types in R, read the article “&lt;a href=&#34;/blog/data-types-in-r/&#34;&gt;Data types in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>