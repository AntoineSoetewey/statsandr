<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Stats and R</title>
    <link>/tags/statistics/</link>
    <description>Recent content in Statistics on Stats and R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 05 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The complete guide to clustering analysis: k-means and hierarchical clustering by hand and in R</title>
      <link>/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-clustering-analysis&#34;&gt;What is clustering analysis?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#application-1-computing-distances&#34;&gt;Application 1: Computing distances&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#solution&#34;&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-clustering&#34;&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#application-2-k-means-clustering&#34;&gt;Application 2: &lt;em&gt;k&lt;/em&gt;-means clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kmeans-with-2-groups&#34;&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 2 groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quality-of-a-k-means-partition&#34;&gt;Quality of a &lt;em&gt;k&lt;/em&gt;-means partition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nstart-for-several-initial-centers&#34;&gt;&lt;code&gt;nstart&lt;/code&gt; for several initial centers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kmeans-with-3-groups&#34;&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 3 groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#manual-application-and-verification-in-r&#34;&gt;Manual application and verification in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#solution-by-hand&#34;&gt;Solution by hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solution-in-r&#34;&gt;Solution in R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-clustering&#34;&gt;Hierarchical clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#application-3-hierarchical-clustering&#34;&gt;Application 3: hierarchical clustering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-1&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solution-by-hand-1&#34;&gt;Solution by hand&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#single-linkage&#34;&gt;Single linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complete-linkage&#34;&gt;Complete linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-linkage&#34;&gt;Average linkage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#solution-in-r-1&#34;&gt;Solution in R&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#single-linkage-1&#34;&gt;Single linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complete-linkage-1&#34;&gt;Complete linkage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-linkage-1&#34;&gt;Average linkage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-means-versus-hierarchical-clustering&#34;&gt;&lt;em&gt;k&lt;/em&gt;-means versus hierarchical clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r-statsandr.com.jpeg&#34; alt=&#34;Photo by Nikola Johnny Mirkovic&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Nikola Johnny Mirkovic&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-clustering-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is clustering analysis?&lt;/h1&gt;
&lt;p&gt;Clustering analysis is a form of exploratory data analysis in which observations are divided into different groups that share common characteristics.&lt;/p&gt;
&lt;p&gt;The purpose of cluster analysis (also known as classification) is to construct groups (or classes or &lt;em&gt;clusters&lt;/em&gt;) while ensuring the following property: &lt;strong&gt;within a group&lt;/strong&gt; the observations must be as &lt;strong&gt;similar&lt;/strong&gt; as possible, while observations belonging to &lt;strong&gt;different groups&lt;/strong&gt; must be as &lt;strong&gt;different&lt;/strong&gt; as possible.&lt;/p&gt;
&lt;p&gt;There are two main types of classification:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/li&gt;
&lt;li&gt;Hierarchical clustering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first is generally used when the &lt;strong&gt;number of classes is fixed&lt;/strong&gt; in advance, while the second is generally used for an &lt;strong&gt;unknown number of classes&lt;/strong&gt; and helps to determine this optimal number. Both methods are illustrated below through applications by hand and in R. Note that for hierarchical clustering, only the &lt;em&gt;ascending&lt;/em&gt; classification is presented in this article.&lt;/p&gt;
&lt;p&gt;Clustering algorithms use the &lt;strong&gt;distance&lt;/strong&gt; in order to separate observations into different groups. Therefore, before diving into the presentation of the two classification methods, a reminder exercise on how to compute distances between points is presented.&lt;/p&gt;
&lt;div id=&#34;application-1-computing-distances&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application 1: Computing distances&lt;/h2&gt;
&lt;p&gt;Let a data set containing the points &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{a} = (0, 0)&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{b} = (1, 0)&amp;#39;\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{c} = (5, 5)&amp;#39;\)&lt;/span&gt;. Compute the matrix of Euclidean distances between the points by hand and in R.&lt;/p&gt;
&lt;div id=&#34;solution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solution&lt;/h3&gt;
&lt;p&gt;The points are as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We create the points in R
a &amp;lt;- c(0, 0)
b &amp;lt;- c(1, 0)
c &amp;lt;- c(5, 5)

X &amp;lt;- rbind(a, b, c) # a, b and c are combined per row
colnames(X) &amp;lt;- c(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;) # rename columns

X # display the points&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y
## a 0 0
## b 1 0
## c 5 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the Pythagorean theorem, we will remember that the distance between 2 points &lt;span class=&#34;math inline&#34;&gt;\((x_a, y_a)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((x_b, y_b)\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^2\)&lt;/span&gt; is given by &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\)&lt;/span&gt;. So for instance, for the distance between the points &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{b} = (1, 0)&amp;#39;\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{c} = (5, 5)&amp;#39;\)&lt;/span&gt; presented in the statement above, we have :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\sqrt{(x_b - x_c)^2 + (y_b - y_c)^2} = \sqrt{(1-5)^2 + (0-5)^2} = 6.403124
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can proceed similarly for all pairs of points to find the distance matrix by hand. In R, the &lt;code&gt;dist()&lt;/code&gt; function allows you to find the distance of points in a matrix or dataframe in a very simple way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The distance is found using the dist() function:
distance &amp;lt;- dist(X, method = &amp;quot;euclidean&amp;quot;)
distance # display the distance matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          a        b
## b 1.000000         
## c 7.071068 6.403124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the argument &lt;code&gt;method = &amp;quot;euclidean&amp;quot;&lt;/code&gt; is not mandatory because the Euclidean method is the default one.&lt;/p&gt;
&lt;p&gt;The distance matrix resulting from the &lt;code&gt;dist()&lt;/code&gt; function gives the distance between the different points. The Euclidean distance between the points &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{b}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{c}\)&lt;/span&gt; is 6.403124, which corresponds to what we found above via the Pythagorean formula.&lt;/p&gt;
&lt;p&gt;Now that the distance has been presented, let’s see how to perform clustering analysis with the k-means algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;k-means-clustering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/h1&gt;
&lt;p&gt;The first form of classification is the method called &lt;em&gt;&lt;em&gt;k&lt;/em&gt;-means clustering&lt;/em&gt; or the mobile center algorithm. As a reminder, this method aims at partitioning &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations into &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; clusters in which each observation belongs to the cluster with the closest average, serving as a prototype of the cluster. It is presented below.&lt;/p&gt;
&lt;div id=&#34;application-2-k-means-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application 2: &lt;em&gt;k&lt;/em&gt;-means clustering&lt;/h2&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;For this exercise, the &lt;code&gt;Eurojobs.csv&lt;/code&gt; database available &lt;a href=&#34;/blog/data/Eurojobs.csv&#34;&gt;here&lt;/a&gt; is used.&lt;/p&gt;
&lt;p&gt;This database contains the percentage of the population employed in different industries in 26 European countries in 1979. It contains 10 variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Country&lt;/code&gt; - the name of the country (identifier)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Agr&lt;/code&gt; - % of workforce employed in agriculture&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Min&lt;/code&gt; - % in mining&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Man&lt;/code&gt; - % in manufacturing&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PS&lt;/code&gt; - % in power supplies industries&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Con&lt;/code&gt; - % in construction&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SI&lt;/code&gt; - % in service industries&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Fin&lt;/code&gt; - % in finance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SPS&lt;/code&gt; - % in social and personal services&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TC&lt;/code&gt; - % in transportation and communications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We first import the dataset. See &lt;a href=&#34;/blog/how-to-import-an-excel-file-in-rstudio&#34;&gt;how to import data into R&lt;/a&gt; if you need a reminder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Import data
Eurojobs &amp;lt;- read.csv(
  file = &amp;quot;data/Eurojobs.csv&amp;quot;,
  sep = &amp;quot;,&amp;quot;, dec = &amp;quot;.&amp;quot;, header = TRUE
)
head(Eurojobs) # head() is used to display only the first 6 observations&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Country  Agr Min  Man  PS  Con   SI Fin  SPS  TC
## 1    Belgium  3.3 0.9 27.6 0.9  8.2 19.1 6.2 26.6 7.2
## 2    Denmark  9.2 0.1 21.8 0.6  8.3 14.6 6.5 32.2 7.1
## 3     France 10.8 0.8 27.5 0.9  8.9 16.8 6.0 22.6 5.7
## 4 W. Germany  6.7 1.3 35.8 0.9  7.3 14.4 5.0 22.3 6.1
## 5    Ireland 23.2 1.0 20.7 1.3  7.5 16.8 2.8 20.8 6.1
## 6      Italy 15.9 0.6 27.6 0.5 10.0 18.1 1.6 20.1 5.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that there is a numbering before the first variable &lt;code&gt;Country&lt;/code&gt;. For more clarity, we will replace this numbering by the country. To do this, we add the argument &lt;code&gt;row.names = 1&lt;/code&gt; in the import function &lt;code&gt;read.csv()&lt;/code&gt; to specify that the first column corresponds to the row names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Eurojobs &amp;lt;- read.csv(
  file = &amp;quot;data/Eurojobs.csv&amp;quot;,
  sep = &amp;quot;,&amp;quot;, dec = &amp;quot;.&amp;quot;, header = TRUE, row.names = 1
)
Eurojobs # displays dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Agr Min  Man  PS  Con   SI  Fin  SPS  TC
## Belgium         3.3 0.9 27.6 0.9  8.2 19.1  6.2 26.6 7.2
## Denmark         9.2 0.1 21.8 0.6  8.3 14.6  6.5 32.2 7.1
## France         10.8 0.8 27.5 0.9  8.9 16.8  6.0 22.6 5.7
## W. Germany      6.7 1.3 35.8 0.9  7.3 14.4  5.0 22.3 6.1
## Ireland        23.2 1.0 20.7 1.3  7.5 16.8  2.8 20.8 6.1
## Italy          15.9 0.6 27.6 0.5 10.0 18.1  1.6 20.1 5.7
## Luxembourg      7.7 3.1 30.8 0.8  9.2 18.5  4.6 19.2 6.2
## Netherlands     6.3 0.1 22.5 1.0  9.9 18.0  6.8 28.5 6.8
## United Kingdom  2.7 1.4 30.2 1.4  6.9 16.9  5.7 28.3 6.4
## Austria        12.7 1.1 30.2 1.4  9.0 16.8  4.9 16.8 7.0
## Finland        13.0 0.4 25.9 1.3  7.4 14.7  5.5 24.3 7.6
## Greece         41.4 0.6 17.6 0.6  8.1 11.5  2.4 11.0 6.7
## Norway          9.0 0.5 22.4 0.8  8.6 16.9  4.7 27.6 9.4
## Portugal       27.8 0.3 24.5 0.6  8.4 13.3  2.7 16.7 5.7
## Spain          22.9 0.8 28.5 0.7 11.5  9.7  8.5 11.8 5.5
## Sweden          6.1 0.4 25.9 0.8  7.2 14.4  6.0 32.4 6.8
## Switzerland     7.7 0.2 37.8 0.8  9.5 17.5  5.3 15.4 5.7
## Turkey         66.8 0.7  7.9 0.1  2.8  5.2  1.1 11.9 3.2
## Bulgaria       23.6 1.9 32.3 0.6  7.9  8.0  0.7 18.2 6.7
## Czechoslovakia 16.5 2.9 35.5 1.2  8.7  9.2  0.9 17.9 7.0
## E. Germany      4.2 2.9 41.2 1.3  7.6 11.2  1.2 22.1 8.4
## Hungary        21.7 3.1 29.6 1.9  8.2  9.4  0.9 17.2 8.0
## Poland         31.1 2.5 25.7 0.9  8.4  7.5  0.9 16.1 6.9
## Rumania        34.7 2.1 30.1 0.6  8.7  5.9  1.3 11.7 5.0
## USSR           23.7 1.4 25.8 0.6  9.2  6.1  0.5 23.6 9.3
## Yugoslavia     48.7 1.5 16.8 1.1  4.9  6.4 11.3  5.3 4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(Eurojobs) # displays the number of rows and columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 26  9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a “clean” dataset of 26 observations and 9 variables on which we can base the classification. Note that in this case it is not necessary to standardize the data because they are all expressed in the same unit (in percentage). If this was not the case, we would have had to standardize the data via the &lt;code&gt;scale()&lt;/code&gt; function (do not forget it otherwise your results may be completly different!). The so-called &lt;em&gt;k&lt;/em&gt;-means clustering is done via the &lt;code&gt;kmeans()&lt;/code&gt; function. We apply the classification with 2 classes and then 3 classes as examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kmeans-with-2-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 2 groups&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- kmeans(Eurojobs, centers = 2)

# displays the class determined by
# the model for all observations:
print(model$cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Belgium        Denmark         France     W. Germany        Ireland 
##              1              1              1              1              2 
##          Italy     Luxembourg    Netherlands United Kingdom        Austria 
##              1              1              1              1              1 
##        Finland         Greece         Norway       Portugal          Spain 
##              1              2              1              2              2 
##         Sweden    Switzerland         Turkey       Bulgaria Czechoslovakia 
##              1              1              2              2              1 
##     E. Germany        Hungary         Poland        Rumania           USSR 
##              1              2              2              2              2 
##     Yugoslavia 
##              2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the argument &lt;code&gt;centers = 2&lt;/code&gt; is used to set the number of clusters, determined in advance. In this exercise the number of clusters has been determined arbitrarily. This number of clusters should be determined according to the context and goal of your analysis. Calling &lt;code&gt;print(model$cluster)&lt;/code&gt; or &lt;code&gt;model$cluster&lt;/code&gt; is the same. This output specifies the group (i.e., 1 or 2) to which each country belongs to.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quality-of-a-k-means-partition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quality of a &lt;em&gt;k&lt;/em&gt;-means partition&lt;/h3&gt;
&lt;p&gt;The quality of a &lt;em&gt;k&lt;/em&gt;-means partition is found by calculating the percentage of the &lt;em&gt;TSS&lt;/em&gt; “explained” by the partition using the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\dfrac{\operatorname{BSS}}{\operatorname{TSS}} \times 100\%
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;BSS&lt;/em&gt; and &lt;em&gt;TSS&lt;/em&gt; stands for &lt;em&gt;Between Sum of Squares&lt;/em&gt; and &lt;em&gt;Total Sum of Squares&lt;/em&gt;, respectively. The higher the percentage, the better the score (and thus the quality) because it means that &lt;em&gt;BSS&lt;/em&gt; is large and/or &lt;em&gt;WSS&lt;/em&gt; is small.&lt;/p&gt;
&lt;p&gt;Here is how you can check the quality of the partition in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BSS and TSS are extracted from the model and stored
(BSS &amp;lt;- model$betweenss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4823.535&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(TSS &amp;lt;- model$totss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9299.59&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We calculate the quality of the partition
BSS / TSS * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.86826&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The quality of the partition is 51.87%. This value has no real interpretation in absolute terms except that a higher quality means a higher explained percentage. However, it is more insightful when it is compared to the quality of other partitions (with the same number of clusters!) in order to determine the best partition among the ones considered.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nstart-for-several-initial-centers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;nstart&lt;/code&gt; for several initial centers&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;k&lt;/em&gt;-means algorithm uses a random set of initial points to arrive at the final classification. Due to the fact that the initial centers are randomly chosen, the same command &lt;code&gt;kmeans(Eurojobs, centers = 2)&lt;/code&gt; may give slightly different results every time it is run, and thus slight differences in the quality of the partitions. The &lt;code&gt;nstart&lt;/code&gt; argument in the &lt;code&gt;kmeans()&lt;/code&gt; function allows to run the algorithm several times with different initial centers, in order to obtain a potentially better partition:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model2 &amp;lt;- kmeans(Eurojobs, centers = 2, nstart = 10)
100 * model2$betweenss / model2$totss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 54.2503&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Depending on the initial random choices, this new partition will be better or not compared to the first one. In our example, the partition is better as the quality increased to 54.25%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kmeans-with-3-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;kmeans()&lt;/code&gt; with 3 groups&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model3 &amp;lt;- kmeans(Eurojobs, centers = 3)
BSS3 &amp;lt;- model3$betweenss
TSS3 &amp;lt;- model3$totss
BSS3 / TSS3 * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 74.59455&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It can be seen that the classification into three groups allows for a higher explained percentage and a higher quality. This will always be the case: with more classes, the partition will be finer, and the &lt;em&gt;BSS&lt;/em&gt; contribution will be higher. On the other hand, the “model” will be more complex, requiring more classes. In the extreme case where &lt;em&gt;k = n&lt;/em&gt; (each observation is a singleton class), we have &lt;em&gt;BSS = TSS&lt;/em&gt;, but the partition has lost all interest.&lt;/p&gt;
&lt;p&gt;Now that the &lt;em&gt;k&lt;/em&gt;-means clustering has been detailed in R, see how to do the algorithm by hand in the following sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;manual-application-and-verification-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Manual application and verification in R&lt;/h3&gt;
&lt;p&gt;Perform &lt;strong&gt;by hand&lt;/strong&gt; the &lt;em&gt;k&lt;/em&gt;-means algorithm for the points shown in the graph below, with &lt;em&gt;k&lt;/em&gt; = 2 and with the points &lt;em&gt;i&lt;/em&gt; = 5 and &lt;em&gt;i&lt;/em&gt; = 6 as initial centers. Compute the quality of the partition you just found and then &lt;strong&gt;check&lt;/strong&gt; your answers &lt;strong&gt;in R&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Assume that the variables have the same units so there is no need to scale the data.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;solution-by-hand&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Solution by hand&lt;/h4&gt;
&lt;p&gt;Step 1. Here are the coordinates of the 6 points:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
point
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
x
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
y
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
7
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
9
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
7
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
8
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;And the initial centers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group 1: point 5 with center &lt;em&gt;(9, 7)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Group 2: point 6 with center &lt;em&gt;(6, 8)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 2. Compute the distance matrix point by point with the Pythagorean theorem. Remind that the distance between the point &lt;em&gt;a&lt;/em&gt; and the point &lt;em&gt;b&lt;/em&gt; is found with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to two decimals):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4     5
## 2  3.61                        
## 3  5.10  2.24                  
## 4  7.28  5.66  3.61            
## 5  4.47  5.39  7.62 10.82      
## 6  5.10  3.61  5.66  9.22  3.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 3. Based on the distance matrix computed in step 2, we can put each point to its closest group and compute the coordinates of the center.&lt;/p&gt;
&lt;p&gt;We first put each point in its closest group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;point 1 is closer to point 5 than to point 6 because the distance between points 1 and 5 is 4.47 while the distance between points 1 and 6 is 5.10&lt;/li&gt;
&lt;li&gt;point 2 is closer to point 6 than to point 5 because the distance between points 2 and 5 is 5.39 while the distance between points 2 and 6 is 3.61&lt;/li&gt;
&lt;li&gt;point 3 is closer to point 6 than to point 5 because the distance between points 3 and 5 is 7.62 while the distance between points 3 and 6 is 5.66&lt;/li&gt;
&lt;li&gt;point 4 is closer to point 6 than to point 5 because the distance between points 4 and 5 is 10.82 while the distance between points 4 and 6 is 9.22&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that computing the distances between each point and the points 5 and 6 is sufficient. There is no need to compute the distance between the points 1 and 2 for example, as we compare each point to the initial centers (which are points 5 and 6).&lt;/p&gt;
&lt;p&gt;We then compute the coordinates of the centers of the two groups by taking the mean of the coordinates &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group 1 includes the points 5 and 1 with &lt;em&gt;(8, 5)&lt;/em&gt; as center (&lt;span class=&#34;math inline&#34;&gt;\(8 = \frac{9+7}{2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(5 = \frac{7+3}{2}\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Group 2 includes the points 6, 2, 3 and 4 with &lt;em&gt;(3, 4.5)&lt;/em&gt; as center (&lt;span class=&#34;math inline&#34;&gt;\(3 = \frac{6+4+2+0}{4}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(4.5 = \frac{8+5+4+1}{4}\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We thus have:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
center
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5 &amp;amp; 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(8, 5)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
cluster 2
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6, 2, 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
(3, 4.5)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Step 4. We make sure that the allocation is optimal by checking that each point is in the nearest cluster. The distance between a point and the center of a cluster is again computed thanks to the Pythagorean theorem. Thus, we have:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-scde{color:#009901;text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-yi9q{color:#009901;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 2
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
2.24
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4.27
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
1.12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6.08
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
1.12
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
8.94
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
4.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
2.24
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-yi9q&#34;&gt;
3.61
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
4.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The minimum distance between the points and the two clusters is colored in green.&lt;/p&gt;
&lt;p&gt;We check that each point is in the correct group (i.e., the closest cluster). According to the distance in the table above, point 6 seems to be closer to the cluster 1 than to the cluster 2. Therefore, the allocation is not optimal and point 6 should be reallocated to cluster 1.&lt;/p&gt;
&lt;p&gt;Step 5. We compute again the centers of the clusters after this reallocation. The centers are found by taking the mean of the coordinates &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; of the points belonging to the cluster. We thus have:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
center
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1, 5 &amp;amp; 6
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(7.33, 6)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
cluster 2
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
2, 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0lax&#34;&gt;
(2, 3.33)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;where, for instance, 3.33 is simply &lt;span class=&#34;math inline&#34;&gt;\(\frac{5+4+1}{3}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Step 6. Repeat step 4 until the allocation is optimal. If the allocation is optimal, the algorithm stops. In our example:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-scde{color:#009901;text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
Distance to cluster 2
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
3.02
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5.01
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
3.48
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
2.61
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5.69
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
0.67
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
8.87
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
3.07
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
1.95
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
7.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-scde&#34;&gt;
2.4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5.08
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;All points are correctly allocated to its nearest cluster, so the allocation is optimal and the algorithm stops.&lt;/p&gt;
&lt;p&gt;Step 7. State the final partition and the centers. In our example:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
points
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
center
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1, 5 &amp;amp; 6
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(7.33, 6)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
cluster 2
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2, 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
(2, 3.33)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We are now going to compute the quality of the partition we just found. Remember that we need to compute the BSS and TSS to find the quality. Below the steps to compute the quality of this partition by &lt;em&gt;k&lt;/em&gt;-means, based on this summary table:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-0a7q{border-color:#000000;text-align:left;vertical-align:middle}
.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34; colspan=&#34;3&#34;&gt;
cluster 1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34; colspan=&#34;3&#34;&gt;
cluster 2
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
point
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
x
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
y
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
point
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
x
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
y
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
7
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
9
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
7
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
8
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
mean
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
7.33
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
6
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3.33
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Step 1. Compute the overall mean of the &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;y&lt;/em&gt; coordinates:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\overline{\overline{x}} = \frac{7+4+2+0+9+6+3+5+4+1+7+8}{12} = 4.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Step 2. Compute TSS and WSS:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[TSS = (7-4.67)^2 + (4-4.67)^2 + (2-4.67)^2 + (0-4.67)^2 \\+ (9-4.67)^2 + (6-4.67)^2 + (3-4.67)^2 + (5-4.67)^2 \\ + (4-4.67)^2 + (1-4.67)^2 + (7-4.67)^2 + (8-4.67)^2 = 88.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Regarding WSS, it is splitted for cluster 1 and for cluster 2. For cluster 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[WSS[1] = (7-7.33)^2 + (9 - 7.33)^2 + (6 - 7.33)^2 \\ + (3-6)^2 + (7-6)^2 + (8-6)^2 = 18.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For cluster 2:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[WSS[2] = (4-2)^2 + (2-2)^2 + (0-2)^2 \\ + (5-3.33)^2 + (4-3.33)^2 + (1-3.33)^2 = 16.67\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And &lt;span class=&#34;math display&#34;&gt;\[WSS = WSS[1] + WSS[2] = 18.67 + 16.67 = 35.34\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To find the BSS:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[BSS = TSS - WSS = 88.67-35.34 = 53.33\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Finally, the quality of the partition is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Quality = \frac{BSS}{TSS} = \frac{53.33}{88.67} = 0.6014\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the quality of the partition is 60.14%.&lt;/p&gt;
&lt;p&gt;We are now going to verify all these solutions (the partition, the final centers and the quality) in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-in-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Solution in R&lt;/h4&gt;
&lt;p&gt;As you can imagine, the solution in R us much shorter and requires much less computation on the user side. We first need to enter the data as a matrix or dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- matrix(c(7, 3, 4, 5, 2, 4, 0, 1, 9, 7, 6, 8),
  nrow = 6, byrow = TRUE
)
X # display the coordinates of the points&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    7    3
## [2,]    4    5
## [3,]    2    4
## [4,]    0    1
## [5,]    9    7
## [6,]    6    8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now perform the &lt;em&gt;k&lt;/em&gt;-means via the &lt;code&gt;kmeans()&lt;/code&gt; function with the point 5 and 6 as initial centers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# take rows 5 and 6 of the X matrix as initial centres
res.k &amp;lt;- kmeans(X, centers = X[c(5, 6), ], algorithm = &amp;quot;Lloyd&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike in the previous application with the dataset &lt;code&gt;Eurojobs.csv&lt;/code&gt; where the initial centers are randomly chosen by R, in this second application we want to specify which points are going to be the two initial centers. For this, we need to set &lt;code&gt;centers = X[c(5,6), ]&lt;/code&gt; to indicate that that there are 2 centers, and that they are going to be the points 5 and 6.&lt;/p&gt;
&lt;p&gt;The reason for adding the argument &lt;code&gt;algorithm = &amp;quot;Lloyd&amp;quot;&lt;/code&gt; can be found in the usage of the R function &lt;code&gt;kmeans()&lt;/code&gt;. In fact, there are several variants of the &lt;em&gt;k&lt;/em&gt;-means algorithm. The default choice is the &lt;span class=&#34;citation&#34;&gt;Hartigan and Wong (1979)&lt;/span&gt; version, which is more sophisticated than the basic version detailed in the solution by hand. By using the original version of &lt;span class=&#34;citation&#34;&gt;Lloyd (1982)&lt;/span&gt;, we find the same solution in R and by hand. For more information, you can consult the documentation of the &lt;code&gt;kmeans()&lt;/code&gt; function (via &lt;code&gt;?kmeans&lt;/code&gt;) and read the articles mentioned.&lt;/p&gt;
&lt;p&gt;The solution is then found by extracting&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the partition:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res.k$cluster&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 2 2 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Points 1, 5 and 6 belong to cluster 1, points 2, 3 and 4 belong to cluster 2.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the coordinates of the final centers:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We extract the coordinates of the 2 final centers, rounded to 2 decimals
round(res.k$centers, digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [,1] [,2]
## 1 7.33 6.00
## 2 2.00 3.33&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;and then the quality of the partition:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res.k$betweenss / res.k$totss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6015038&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 3 results are equal to what we found by hand (except the quality which is slightly different due to rounding).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-clustering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hierarchical clustering&lt;/h1&gt;
&lt;p&gt;Remind that the difference with the partition by &lt;em&gt;k&lt;/em&gt;-means is that for hierarchical clustering, the number of classes is &lt;strong&gt;not&lt;/strong&gt; specified in advance. Hierarchical clustering will help to determine the optimal number of clusters.&lt;/p&gt;
&lt;p&gt;In this article, the 3 main types of hierarchical clustering are discussed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single linkage (minimum distance)&lt;/li&gt;
&lt;li&gt;Complete linkage (maximum distance)&lt;/li&gt;
&lt;li&gt;Average linkage (average distance)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the following sections, we perform these 3 algorithms by hand and verify the results in R.&lt;/p&gt;
&lt;div id=&#34;application-3-hierarchical-clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application 3: hierarchical clustering&lt;/h2&gt;
&lt;div id=&#34;data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;Using the data from the graph and the table below, perform &lt;strong&gt;by hand&lt;/strong&gt; the 3 algorithms (single, complete and average linkage). Then &lt;strong&gt;check&lt;/strong&gt; your answers &lt;strong&gt;in R&lt;/strong&gt; and draw the dendrograms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      V1    V2
## 1  2.03  0.06
## 2 -0.64 -0.10
## 3 -0.42 -0.53
## 4 -0.36  0.07
## 5  1.14  0.37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-by-hand-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solution by hand&lt;/h3&gt;
&lt;p&gt;Step 1. For all 3 algorithms, we first need to compute the distance matrix between the 5 points thanks to the Pythagorean theorem. Remind that the distance between the point &lt;em&gt;a&lt;/em&gt; and the point &lt;em&gt;b&lt;/em&gt; is found with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to three decimals):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;single-linkage&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Single linkage&lt;/h4&gt;
&lt;p&gt;Step 2. From the distance matrix computed in step 1, we see that the &lt;strong&gt;smallest distance&lt;/strong&gt; = 0.328 between points 2 and 4. 0.328 corresponds to the first height (more on this later when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp;amp; 4, 3 and 5. The new distances between the points 2 &amp;amp; 4 and all other points are now:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
3
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.390
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.520
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.483
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.530
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.801
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To construct this new distance matrix, proceed point by point:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the inital distance matrix (found in step 1), which was 2.520&lt;/li&gt;
&lt;li&gt;same goes for the distance between points 1 and 5 and points 3 and 5, the distances are the same than in the initial distance matrix since the points have not changed&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 4 has changed since points 2 &amp;amp; 4 are now together&lt;/li&gt;
&lt;li&gt;since we are applying the &lt;strong&gt;single linkage&lt;/strong&gt; criterion, the new distance between points 1 and 2 &amp;amp; 4 corresponds to the &lt;strong&gt;minimum distance&lt;/strong&gt; between the distance between points 1 and 2 and the distance between points 1 and 4&lt;/li&gt;
&lt;li&gt;initial distance between points 1 and 2 is 2.675 and initial distance between points 1 and 4 is 2.390&lt;/li&gt;
&lt;li&gt;therefore, the minimum distance between these two distances is 2.390&lt;/li&gt;
&lt;li&gt;2.390 is thus the new distance between points 1 and 2 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;we apply the same process for points 3 and 2 &amp;amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The minimum distance between these 2 distances is 0.483 so the new distance between points 3 and 2 &amp;amp; 4 is 0.483&lt;/li&gt;
&lt;li&gt;follow the same process for all other points&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 3. Based on the distance matrix in step 2, the smallest distance is 0.483 between points 3 and 2 &amp;amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp;amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp;amp; 3 &amp;amp; 4. The groups are thus: 1, 2 &amp;amp; 3 &amp;amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.390
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.530
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;points 1 and 5 have not change, so the distance between these two points are the same than in previous step&lt;/li&gt;
&lt;li&gt;from step 2 we see that the distance between points 1 and 2 &amp;amp; 4 is 2.390 and the distance between points 1 and 3 is 2.520&lt;/li&gt;
&lt;li&gt;since we apply the single linkage criterion, we take the minimum distance, which is 2.390&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.390&lt;/li&gt;
&lt;li&gt;same process for points 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp;amp; 5. The groups are thus: 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.530
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the only distance left to compute is the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;from the previous step we see that the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is 2.390 and the distance between points 5 and 2 &amp;amp; 3 &amp;amp; 4 is 1.530&lt;/li&gt;
&lt;li&gt;since we apply the single linkage criterion, we take the minimum distance, which is 1.530&lt;/li&gt;
&lt;li&gt;the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 is thus 1.530&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 5. The final combination of points is the combination of points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4, with a final height of 1.530. Heights are used to draw the dendrogram in the final step.&lt;/p&gt;
&lt;p&gt;Step 6. Draw the dendrogram thanks to the combination of points and the heights found above. Remember that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the first combination of points was between points 2 and 4, with a height of 0.328&lt;/li&gt;
&lt;li&gt;the second combination was between points 3 and 2 &amp;amp; 4 with a height of 0.483&lt;/li&gt;
&lt;li&gt;the third combination was between points 1 and 5 with a height of 0.942&lt;/li&gt;
&lt;li&gt;the final combination was between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 with a height of 1.530&lt;/li&gt;
&lt;li&gt;this is exactly what is illustrated in the following dendrogram:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complete-linkage&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complete linkage&lt;/h4&gt;
&lt;p&gt;Complete linkage is quite similar to single linkage, except that instead of taking the smallest distance when computing the new distance between points that have been grouped, the &lt;strong&gt;maximum distance&lt;/strong&gt; is taken.&lt;/p&gt;
&lt;p&gt;The steps to perform the hierarchical clustering with the complete linkage (maximum) are detailed below.&lt;/p&gt;
&lt;p&gt;Step 1. Step 1 is exactly the same than for single linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 2. From the distance matrix computed in step 1, we see that the &lt;strong&gt;smallest distance&lt;/strong&gt; = 0.328 between points 2 and 4. It is important to note that even if we apply the complete linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp;amp; 4, 3 and 5. The new distances between the points 2 &amp;amp; 4 and all other points are now:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
3
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.675
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.520
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.603
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.841
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.801
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To construct this new distance matrix, proceed point by point:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the inital distance matrix (found in step 1), which was 2.520&lt;/li&gt;
&lt;li&gt;same goes for the distance between points 1 and 5 and points 3 and 5, the distances are the same than in the initial distance matrix since the points have not changed&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 4 has changed since points 2 &amp;amp; 4 are now together&lt;/li&gt;
&lt;li&gt;since we are applying the &lt;strong&gt;complete linkage&lt;/strong&gt; criterion, the new distance between points 1 and 2 &amp;amp; 4 corresponds to the &lt;strong&gt;maximum distance&lt;/strong&gt; between the distance between points 1 and 2 and the distance between points 1 and 4&lt;/li&gt;
&lt;li&gt;initial distance between points 1 and 2 is 2.675 and initial distance between points 1 and 4 is 2.390&lt;/li&gt;
&lt;li&gt;therefore, the maximum distance between these two distances is 2.675&lt;/li&gt;
&lt;li&gt;2.675 is thus the new distance between points 1 and 2 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;we apply the same process for points 3 and 2 &amp;amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The maximum distance between these 2 distances is 0.603 so the new distance between points 3 and 2 &amp;amp; 4 is 0.603&lt;/li&gt;
&lt;li&gt;follow the same process for all other points&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 3. Based on the distance matrix in step 2, the smallest distance is 0.603 between points 3 and 2 &amp;amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp;amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp;amp; 3 &amp;amp; 4. The groups are thus: 1, 2 &amp;amp; 3 &amp;amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.675
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.841
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;points 1 and 5 have not change, so the distance between these two points are the same than in previous step&lt;/li&gt;
&lt;li&gt;from step 2 we see that the distance between points 1 and 2 &amp;amp; 4 is 2.675 and the distance between points 1 and 3 is 2.520&lt;/li&gt;
&lt;li&gt;since we apply the complete linkage criterion, we take the maximum distance, which is 2.675&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.675&lt;/li&gt;
&lt;li&gt;same process for points 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp;amp; 5. The groups are thus: 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.675
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the only distance left to compute is the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;from the previous step we see that the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is 2.675 and the distance between points 5 and 2 &amp;amp; 3 &amp;amp; 4 is 1.841&lt;/li&gt;
&lt;li&gt;since we apply the complete linkage criterion, we take the maximum distance, which is 2.675&lt;/li&gt;
&lt;li&gt;the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.675&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 5. The final combination of points is the combination of points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4, with a final height of 2.675. Heights are used to draw the dendrogram in the final step.&lt;/p&gt;
&lt;p&gt;Step 6. Draw the dendrogram thanks to the combination of points and the heights found above. Remember that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the first combination of points was between points 2 and 4, with a height of 0.328&lt;/li&gt;
&lt;li&gt;the second combination was between points 3 and 2 &amp;amp; 4 with a height of 0.603&lt;/li&gt;
&lt;li&gt;the third combination was between points 1 and 5 with a height of 0.942&lt;/li&gt;
&lt;li&gt;the final combination was between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 with a height of 2.675&lt;/li&gt;
&lt;li&gt;this is exactly what is illustrated in the following dendrogram:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-linkage&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Average linkage&lt;/h4&gt;
&lt;p&gt;With the average linkage criterion, it is not the minimum nor the maximum distance that is taken when computing the new distance between points that have been grouped, but it is, as you guessed by now, the &lt;strong&gt;average distance&lt;/strong&gt; between the points.&lt;/p&gt;
&lt;p&gt;The steps to perform the hierarchical clustering with the average linkage are detailed below.&lt;/p&gt;
&lt;p&gt;Step 1. Step 1 is exactly the same than for single and complete linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Step 2. From the distance matrix computed in step 1, we see that the &lt;strong&gt;smallest distance&lt;/strong&gt; = 0.328 between points 2 and 4. It is important to note that even if we apply the average linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp;amp; 4, 3 and 5. The new distances between the points 2 &amp;amp; 4 and all other points are now:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
3
&lt;/th&gt;
&lt;th class=&#34;tg-0pky&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.5325
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
3
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
2.520
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.543
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.6855
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
1.801
&lt;/td&gt;
&lt;td class=&#34;tg-0pky&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;To construct this new distance matrix, proceed point by point:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the inital distance matrix (found in step 1), which was 2.520&lt;/li&gt;
&lt;li&gt;same goes for the distance between points 1 and 5 and points 3 and 5, the distances are the same than in the initial distance matrix since the points have not changed&lt;/li&gt;
&lt;li&gt;the distance between points 1 and 2 &amp;amp; 4 has changed since points 2 &amp;amp; 4 are now together&lt;/li&gt;
&lt;li&gt;since we are applying the &lt;strong&gt;average linkage&lt;/strong&gt; criterion, the new distance between points 1 and 2 &amp;amp; 4 corresponds to the &lt;strong&gt;average distance&lt;/strong&gt; between the distance between points 1 and 2 and the distance between points 1 and 4&lt;/li&gt;
&lt;li&gt;initial distance between points 1 and 2 is 2.675 and initial distance between points 1 and 4 is 2.390&lt;/li&gt;
&lt;li&gt;therefore, the average distance between these two distances is &lt;span class=&#34;math inline&#34;&gt;\(\frac{2.675 + 2.390}{2} = 2.5325\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;2.5325 is thus the new distance between points 1 and 2 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;we apply the same process for points 3 and 2 &amp;amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The average distance between these 2 distances is 0.543 so the new distance between points 3 and 2 &amp;amp; 4 is 0.543&lt;/li&gt;
&lt;li&gt;follow the same process for all other points&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 3. Based on the distance matrix in step 2, the smallest distance is 0.543 between points 3 and 2 &amp;amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp;amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp;amp; 3 &amp;amp; 4. The groups are thus: 1, 2 &amp;amp; 3 &amp;amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
5
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.528333
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0.942
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1.724
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;points 1 and 5 have not change, so the distance between these two points are the same than in previous step&lt;/li&gt;
&lt;li&gt;from step 2 we see that the distance between points 1 and 2 &amp;amp; 4 is 2.5325 and the distance between points 1 and 3 is 2.520&lt;/li&gt;
&lt;li&gt;since we apply the average linkage criterion, we take the average distance&lt;/li&gt;
&lt;li&gt;however, we have to take into the consideration that there are 2 points in the group 2 &amp;amp; 4, while only one point in the group 3&lt;/li&gt;
&lt;li&gt;the average distance for the distance between 1 and 2 &amp;amp; 3 &amp;amp; 4 is thus: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(2 \cdot 2.5325) + (1 \cdot 2.520)}{3} = 2.528333\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;same process for points 5 and 2 &amp;amp; 3 &amp;amp; 4: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(2 \cdot 1.6855) + (1 \cdot 1.801)}{3} = 1.724\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp;amp; 5. The groups are thus: 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:&lt;/p&gt;
&lt;center&gt;
&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/th&gt;
&lt;th class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
1 &amp;amp; 5
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2 &amp;amp; 3 &amp;amp; 4
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
2.126167
&lt;/td&gt;
&lt;td class=&#34;tg-cly1&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the only distance left to compute is the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4&lt;/li&gt;
&lt;li&gt;from the previous step we see that the distance between points 1 and 2 &amp;amp; 3 &amp;amp; 4 is 2.528333 and the distance between points 5 and 2 &amp;amp; 3 &amp;amp; 4 is 1.724&lt;/li&gt;
&lt;li&gt;since we apply the average linkage criterion, we take the average distance, which is &lt;span class=&#34;math inline&#34;&gt;\(\frac{2.528333 + 1.724}{2} = 2.126167\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the distance between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 is thus 2.126167&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Step 5. The final combination of points is the combination of points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4, with a final height of 2.126167. Heights are used to draw the dendrogram in the final step.&lt;/p&gt;
&lt;p&gt;Step 6. Draw the dendrogram thanks to the combination of points and the heights found above. Remember that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the first combination of points was between points 2 and 4, with a height of 0.328&lt;/li&gt;
&lt;li&gt;the second combination was between points 3 and 2 &amp;amp; 4 with a height of 0.543&lt;/li&gt;
&lt;li&gt;the third combination was between points 1 and 5 with a height of 0.942&lt;/li&gt;
&lt;li&gt;the final combination was between points 1 &amp;amp; 5 and 2 &amp;amp; 3 &amp;amp; 4 with a height of 2.126167&lt;/li&gt;
&lt;li&gt;this is exactly what is illustrated in the following dendrogram:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-in-r-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solution in R&lt;/h3&gt;
&lt;p&gt;To perform the hierarchical clustering with any of the 3 criterion in R, we first need to enter the data (in this case as a matrix format, but it can also be performed on dataframes):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- matrix(c(2.03, 0.06, -0.64, -0.10, -0.42, -0.53, -0.36, 0.07, 1.14, 0.37),
  nrow = 5, byrow = TRUE
)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;single-linkage-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Single linkage&lt;/h4&gt;
&lt;p&gt;We can apply the hierarchical clustering with the single linkage criterion thanks to the &lt;code&gt;hclust()&lt;/code&gt; function with the argument &lt;code&gt;method = &amp;quot;single&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering: single linkage
hclust &amp;lt;- hclust(dist(X), method = &amp;quot;single&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;hclust()&lt;/code&gt; function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix &lt;code&gt;X&lt;/code&gt; corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the &lt;code&gt;dist()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We can now plot the dendrogram to check our results by hand found above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.&lt;/p&gt;
&lt;p&gt;Remember that hierarchical clustering is used to determine the optimal number of clusters. This optimal number of clusters can be determined thanks to the dendrogram. For this, we usually look at the largest difference of heights:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/dendrogram-single-linkage.png&#34; alt=&#34;How to determine the number of clusters from a dendrogram? Take the largest difference of heights and count how many vertical lines you see&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;How to determine the number of clusters from a dendrogram? Take the largest difference of heights and count how many vertical lines you see&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp;amp; 3 &amp;amp; 4 with the group 1 &amp;amp; 5. To determine the optimal number of clusters, simply count how many vertical lines you see within this largest difference. In our case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the &lt;code&gt;rect.hclust()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &amp;quot;blue&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we could also determine the optimal number of cluster thanks to a barplot the heights (stored in &lt;code&gt;$height&lt;/code&gt; of the clustering output):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;barplot(hclust$height,
  names.arg = (nrow(X) - 1):1
) # show the number of cluster below each bars&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, look for the largest jump of heights. In our case, the largest jump is from 1 to 2 classes. Therefore, the optimal number of classes is 2.&lt;/p&gt;
&lt;p&gt;Note that determining the number of clusters using the dendrogram or barplot is not a strict rule. You can also consider the &lt;em&gt;Silhouette plot&lt;/em&gt;, &lt;em&gt;elbow plot&lt;/em&gt; or some numerical measures such as Dunn’s index, Hubert’s gamma, etc., which show the variation of the error with the number of clusters (&lt;em&gt;k&lt;/em&gt;), and you choose the value of &lt;em&gt;k&lt;/em&gt; where the error is smallest. However, these methods are outside the scope of this course and the method presented with the dendrogram is sufficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complete-linkage-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complete linkage&lt;/h4&gt;
&lt;p&gt;We can apply the hierarchical clustering with the complete linkage criterion thanks to the &lt;code&gt;hclust()&lt;/code&gt; function with the argument &lt;code&gt;method = &amp;quot;complete&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering: complete linkage
hclust &amp;lt;- hclust(dist(X), method = &amp;quot;complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;hclust()&lt;/code&gt; function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix &lt;code&gt;X&lt;/code&gt; corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the &lt;code&gt;dist()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We can now plot the dendrogram to check our results by hand found above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.&lt;/p&gt;
&lt;p&gt;Like for the single linkage, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp;amp; 3 &amp;amp; 4 with the group 1 &amp;amp; 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the &lt;code&gt;rect.hclust()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &amp;quot;blue&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-linkage-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Average linkage&lt;/h4&gt;
&lt;p&gt;We can apply the hierarchical clustering with the average linkage criterion thanks to the &lt;code&gt;hclust()&lt;/code&gt; function with the argument &lt;code&gt;method = &amp;quot;average&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering: average linkage
hclust &amp;lt;- hclust(dist(X), method = &amp;quot;average&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;hclust()&lt;/code&gt; function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix &lt;code&gt;X&lt;/code&gt; corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the &lt;code&gt;dist()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We can now plot the dendrogram to check our results by hand found above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.&lt;/p&gt;
&lt;p&gt;Like for the single and complete linkages, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp;amp; 3 &amp;amp; 4 with the group 1 &amp;amp; 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the &lt;code&gt;rect.hclust()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &amp;quot;blue&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;k-means-versus-hierarchical-clustering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;em&gt;k&lt;/em&gt;-means versus hierarchical clustering&lt;/h1&gt;
&lt;p&gt;Choosing between &lt;em&gt;k&lt;/em&gt;-means and hierarchical clustering is not always easy. If you have a good reason to think that there is a specific number of clusters in your dataset (for example if you would like to distinguish diseased and healty patients depending on some characteristics but you do not know which group patients belong to), you should probably opt for the &lt;em&gt;k&lt;/em&gt;-means clustering as this technique is used when the number of groups is specified in advance. If you do not have any reason to believe there is a certain number of groups in your dataset (for instance in marketing when trying to distinguish clients without any prior belief on the number of different types of customers), then you should probably opt for the hierarchical clustering to determine in how many clusters your data should be divided.&lt;/p&gt;
&lt;p&gt;In order to help you make your choice note that, on the one hand, with a large number of variables, &lt;em&gt;k&lt;/em&gt;-means may be computationally faster than hierarchical clustering if the number of clusters is small. On the other hand, the result of a hierarchical clustering is a structure that is more informative and interpretable than the unstructured set of flat clusters returned by &lt;em&gt;k&lt;/em&gt;-means. Therefore, it is easier to determine the optimal number of clusters by looking at the dendrogram of a hierarchical clustering, instead of predicting this optimal number in advance in case of &lt;em&gt;k&lt;/em&gt;-means.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you understand the different clustering methods and how to compute them by hand and in R.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-jaaw28m&#34;&gt;
&lt;p&gt;Hartigan, J. A., and M. A. Wong. 1979. “A K-Means Clustering Algorithm.” &lt;em&gt;Applied Statistics&lt;/em&gt; 28: 100–108.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lloyd1982least&#34;&gt;
&lt;p&gt;Lloyd, Stuart. 1982. “Least Squares Quantization in Pcm.” &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 28 (2). IEEE: 129–37.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inferential statistics: confidence intervals and hypothesis tests explained in 4 easy steps</title>
      <link>/blog/inferential-statistics-confidence-intervals-and-hypothesis-tests-explained-in-4-easy-steps/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/inferential-statistics-confidence-intervals-and-hypothesis-tests-explained-in-4-easy-steps/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypothesis-tests-and-confidence-intervals-why-and-when&#34;&gt;Hypothesis tests and confidence intervals: why and when?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;xxx add image and put the link in the YAML&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Remember that &lt;a href=&#34;/blog/descriptive-statistics-by-hand&#34;&gt;descriptive statistics&lt;/a&gt; is a branch of statistics aiming at describing and summarizing a set of data in the best possible manner, that is, by reducing them down to a few meaningful key measures and visualizations (with as little loss of information as possible). In other words, descriptive statistics helps to have a better understanding and a clear image about a set of observations thanks to summary statistics and graphics. With descriptive statistics, there is no uncertainty because we describe only the group of observations that we decided to work and no attempt is made to generalize the observed characteristics to another or larger group of observations.&lt;/p&gt;
&lt;p&gt;Inferential statistics is another branch of statistics that uses a random sample of data taken from a population to make inferences, i.e., to draw conclusions about the population (see the &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;difference between population and sample&lt;/a&gt;). In other words, information from the sample is used to make generalizations about the parameter of interest in the population. Inferential statistics includes two important tools: hypothesis tests and confidence intervals.&lt;/p&gt;
&lt;p&gt;As part of my teaching assistant position, I quickly realized that students often struggle to compute confidence intervals, perform hypothesis tests and interpret the results. It seems to me that students often encounter difficulties because this branch of statistics is rather unclear and abstract to them. I believe the main reason why it looks abstract to them is because they do not understand the final goal of inferential statistics, that is, the why behind these tools. They often perform hypothesis tests and confidence intervals by simply following the steps presented in another example whitout understanding the reasoning behind it, as they would follow a cooking recipe because they must prepare food, but not because they actually want to prepare &lt;em&gt;good&lt;/em&gt; food.&lt;/p&gt;
&lt;p&gt;For this reason, I though it would be useful to write an article on the goal of hypothesis tests and confidence intervals (the why), in which context they should be used (the when), how they work (in 4 easy steps) and how to interpret the results (as statistical results are meaningless without proper interpretation). Like anything else in statistics, it becomes much easier when we understand what we are trying to demonstrate before knowing how to compute it.&lt;/p&gt;
&lt;p&gt;Inferential statistics can be applied to many parameters. Nonetheless, in order to keep this article easy and accessible to people from diverse backgrounds, I focus on hypothesis tests and confidences intervals applied to the 3 main parameters: &lt;strong&gt;mean, proportion and variance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you are familiar with these two tools, below are 3 articles that may be of interest to you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/blog/xxx&#34;&gt;Hypothesis tests and confidence intervals for one and two means (independent and paired samples)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/blog/xxx&#34;&gt;Hypothesis tests and confidence intervals for one and two proportions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/blog/xxx&#34;&gt;Hypothesis tests and confidence intervals for one and two variances&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These 3 articles focus on the practical application of the two tools of inferential statistics by hand and in R. The present article focuses on the same topic but from a theoritical perspective in order to lay the foundations of hypothesis testing and confidence interval. I believe that grasping the concepts behind these tools from a theoritical perspective is a great help in applying them in practice.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-tests-and-confidence-intervals-why-and-when&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypothesis tests and confidence intervals: why and when?&lt;/h1&gt;
&lt;p&gt;Unlike descriptive statistics where we have the data on all&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you to understand better how to perform hypothesis tests and construct confidence intervals by hand.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Do my data follow a normal distribution? A note on the most widely used distribution and how to test for normality in R</title>
      <link>/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-a-normal-distribution&#34;&gt;What is a normal distribution?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#empirical-rule&#34;&gt;Empirical rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probabilities-and-standard-normal-distribution&#34;&gt;Probabilities and standard normal distribution&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#areas-under-the-normal-distribution-in-r-and-by-hand&#34;&gt;Areas under the normal distribution in R and by hand&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-1&#34;&gt;Ex. 1&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-2&#34;&gt;Ex. 2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r-1&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand-1&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-3&#34;&gt;Ex. 3&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r-2&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand-2&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-4&#34;&gt;Ex. 4&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-r-3&#34;&gt;In R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#by-hand-3&#34;&gt;By hand&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex.-5&#34;&gt;Ex. 5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-is-the-normal-distribution-so-crucial-in-statistics&#34;&gt;Why is the normal distribution so crucial in statistics?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-test-the-normality-assumption&#34;&gt;How to test the normality assumption&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histogram&#34;&gt;Histogram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plot&#34;&gt;Density plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qq-plot&#34;&gt;QQ-plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normality-test&#34;&gt;Normality test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-is-a-normal-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is a normal distribution?&lt;/h1&gt;
&lt;p&gt;The normal distribution is a function that defines how a set of measurements is distributed around the center of these measurements (i.e., the mean). Many natural phenomena in real life can be approximated by a bell-shaped frequency distribution known as the normal distribution or the Gaussian distribution.&lt;/p&gt;
&lt;p&gt;The normal distribution is a mount-shaped, unimodal and symmetric distribution where most measurements gather around the mean. Moreover, the further a measure deviates from the mean, the lower the probability of occurring. In this sense, for a given variable, it is common to find values close to the mean, but less and less likely to find values as we move away from the mean. Last but not least, since the normal distribution is symmetric around its mean, extreme values in both tails of the distribution are equivalently unlikely. For instance, given that adult height follows a normal distribution, most adults are close to the average height and extremely short adults occur as infrequently as extremely tall adults.&lt;/p&gt;
&lt;p&gt;In this article, the focus is on understanding the normal distribution, the associated empirical rule, its parameters and how to compute &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores to find probabilities under the curve (illustrated with examples). As it is a requirement in some statistical tests, we also show 4 complementary methods to test the normality assumption in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-rule&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Empirical rule&lt;/h1&gt;
&lt;p&gt;Data possessing an approximately normal distribution have a definite variation, as expressed by the following empirical rule:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu \pm \sigma\)&lt;/span&gt; includes approximately 68% of the observations&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu \pm 2 \cdot \sigma\)&lt;/span&gt; includes approximately 95% of the observations&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu \pm 3 \cdot \sigma\)&lt;/span&gt; includes almost all of the observations (99.7% to be more precise)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/empirical-rule-normal-distribution.png&#34; alt=&#34;Normal distribution &amp;amp; empirical rule (68-95-99.7% rule)&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Normal distribution &amp;amp; empirical rule (68-95-99.7% rule)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; correspond to the population mean and population standard deviation, respectively.&lt;/p&gt;
&lt;p&gt;The empirical rule, also known as the 68-95-99.7% rule, is illustrated by the following 2 examples. Suppose that the scores of an exam in statistics given to all students in a Belgian university are known to have, approximately, a normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = 67\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 9\)&lt;/span&gt;. It can then be deduced that approximately 68% of the scores are between 58 and 76, that approximately 95% of the scores are between 49 and 85, and that almost all of the scores (99.7%) are between 40 and 94. Thus, knowing the mean and the standard deviation gives us a fairly good picture of the distribution of scores. Now suppose that a single university student is randomly selected from those who took the exam. What is the probability that her score will be between 49 and 85? Based on the empirical rule, we find that 0.95 is a reasonable answer to this probability question.&lt;/p&gt;
&lt;p&gt;The utility and value of the empirical rule are due to the common occurrence of approximately normal distributions of measurements in nature. For example, IQ, shoe size, height, birth weight, etc. are approximately normally-distributed. You will find that approximately 95% of these measurements will be within &lt;span class=&#34;math inline&#34;&gt;\(2\sigma\)&lt;/span&gt; of their mean &lt;span class=&#34;citation&#34;&gt;(Wackerly, Mendenhall, and Scheaffer 2014)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parameters&lt;/h1&gt;
&lt;p&gt;Like many probability distributions, the shape and probabilities of the normal distribution is defined entirely by some parameters. The normal distribution has two parameters: (i) the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#mean&#34;&gt;mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;&lt;/a&gt; and (ii) the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#variance&#34;&gt;variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/a&gt; (i.e., the square of the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/#standard-deviation&#34;&gt;standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;&lt;/a&gt;). The mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; locates the center of the distribution, that is, the central tendency of the observations, and the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; defines the width of the distribution, that is, the spread of the observations.&lt;/p&gt;
&lt;p&gt;The mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; can take on any finite value (i.e., &lt;span class=&#34;math inline&#34;&gt;\(-\infty &amp;lt; \mu &amp;lt; \infty\)&lt;/span&gt;), whereas the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; can assume any positive finite value (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 &amp;gt; 0\)&lt;/span&gt;). The shape of the normal distribution changes based on these two parameters. Since there is an infinite number of combinations of the mean and variance, there is an infinite number of normal distributions, and thus an infinite number of forms.&lt;/p&gt;
&lt;p&gt;For instance, see how the shapes of the normal distributions vary when the two parameters change:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see on the second graph, when the variance (or the standard deviation) decreases, the observations are closer to the mean. On the contrary, when the variance (or standard deviation) increases, it is more likely that observations will be further away from the mean.&lt;/p&gt;
&lt;p&gt;A random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; which follows a normal distribution with a mean of 430 and a variance of 17 is denoted &lt;span class=&#34;math inline&#34;&gt;\(X ~ \sim \mathcal{N}(\mu = 430, \sigma^2 = 17)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We have seen that, although different normal distributions have different shapes, all normal distributions have common characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They are symmetric, 50% of the population is above the mean and 50% of the population is below the mean&lt;/li&gt;
&lt;li&gt;The mean, median and mode are equal&lt;/li&gt;
&lt;li&gt;The empirical rule detailed earlier is applicable to all normal distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;probabilities-and-standard-normal-distribution&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Probabilities and standard normal distribution&lt;/h1&gt;
&lt;p&gt;Probabilities and quantiles for random variables with normal distributions are easily found using R via the functions &lt;code&gt;pnorm()&lt;/code&gt; and &lt;code&gt;qnorm()&lt;/code&gt;. Probabilities associated with a normal distribution can also be found using this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-101/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shiny app&lt;/a&gt;. However, before computing probabilities, we need to learn more about the standard normal distribution and the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score.&lt;/p&gt;
&lt;p&gt;Although there are infinitely many normal distributions (since there is a normal distribution for every combination of mean and variance), we need only one table to find the probabilities under the normal curve: the &lt;strong&gt;standard normal distribution&lt;/strong&gt;. The normal standard distribution is a special case of the normal distribution where the mean is equal to 0 and the variance is equal to 1. A normal random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can always be transformed to a standard normal random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, a process known as “scaling” or “standardization”, by subtracting the mean from the observation, and dividing the result by the standard deviation. Formally:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = \frac{X - \mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the observation, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; the mean and standard deviation of the population from which the observation was drawn. So the mean of the standard normal distribution is 0, and its variance is 1, denoted &lt;span class=&#34;math inline&#34;&gt;\(Z ~ \sim \mathcal{N}(\mu = 0, \sigma^2 = 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From this formula, we see that &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, referred as standard score or &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score, allows to see how far away one specific observation is from the mean of all observations, with the distance expressed in standard deviations. In other words, the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score corresponds to the number of standard deviations one observation is away from the mean. A positive &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score means that the specific observation is above the mean, whereas a negative &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score means that the specific observation is below the mean. &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores are often used to compare an individual to her peers, or more generally, a measurement compared to its distribution.&lt;/p&gt;
&lt;p&gt;For instance, suppose a student scoring 60 at a statistics exam with the mean score of the class being 40, and scoring 65 at an economics exam with the mean score of the class being 80. Given the “raw” scores, one would say that the student performed better in economics than in statistics. However, taking into consideration her peers, it is clear that the student performed relatively better in statistics than in economics. Computing &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores allows to take into consideration all other students (i.e., the entire distribution) and gives a better measure of comparison. Let’s compute the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores for the two exams, assuming that the score for both exams follow a normal distribution with the following parameters:&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Statistics&lt;/th&gt;
&lt;th&gt;Economics&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Standard deviation&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;12.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Student’s score&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Statistics: &lt;span class=&#34;math inline&#34;&gt;\(Z_{stat} = \frac{60 - 40}{8} = 2.5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Economics: &lt;span class=&#34;math inline&#34;&gt;\(Z_{econ} = \frac{65 - 80}{12.5} = -1.2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the one hand, the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score for the exam in statistics is positive (&lt;span class=&#34;math inline&#34;&gt;\(Z_{stat} = 2.5\)&lt;/span&gt;) which means that she performed better than average. On the other hand, her score for the exam in economics is negative (&lt;span class=&#34;math inline&#34;&gt;\(Z_{econ} = -1.2\)&lt;/span&gt;) which means that she performed worse than average. Below an illustration of her grades in a standard normal distribution for better comparison:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the score in economics is better in absolute terms, the score in statistics is actually relatively better when comparing each score within its own distribution.&lt;/p&gt;
&lt;p&gt;Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score also enables to compare observations that would otherwise be impossible because they have different units for example. Suppose you want to compare a salary in € with a weight in kg. Without standardization, there is no way to conclude whether someone is more extreme in terms of her wage or in terms of her weight. Thanks to &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; scores, we can compare two values that were in the first place not comparable to each other.&lt;/p&gt;
&lt;p&gt;Final remark regarding the interpretation of a &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score: a rule of thumb is that an observation with a &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score between -3 and -2 or between 2 and 3 is considered as a rare value. An observation with a &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score smaller than -3 or larger than 3 is considered as an extremely rare value. A value with any other &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score is considered as not rare nor extremely rare.&lt;/p&gt;
&lt;div id=&#34;areas-under-the-normal-distribution-in-r-and-by-hand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Areas under the normal distribution in R and by hand&lt;/h2&gt;
&lt;p&gt;Now that we have covered the &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; score, we are going to use it to determine the area under the curve of a normal distribution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that there are several ways to arrive at the solution in the following exercises. You may therefore use other steps than the ones presented to obtain the same result.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;ex.-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 1&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; denote a normal random variable with mean 0 and standard deviation 1, find &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We actually look for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2014.18.54.png&#34; alt=&#34;Standard normal distribution: P(Z &amp;gt; 1)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution: &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(1,
  mean = 0,
  sd = 1, # sd stands for standard deviation
  lower.tail = FALSE
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1586553&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We look for the probability of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; being larger than 1 so we set the argument &lt;code&gt;lower.tail = FALSE&lt;/code&gt;. The default &lt;code&gt;lower.tail = TRUE&lt;/code&gt; would give the result for &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;lt; 1)\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(P(Z = 1) = 0\)&lt;/span&gt; so writing &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(P(Z \ge 1)\)&lt;/span&gt; is equivalent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;See that the random variable &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; has already a mean of 0 and a standard deviation of 1, so no transformation is required. To find the probabilities by hand, we need to refer to the standard normal distribution table shown below:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2015.07.44.png&#34; alt=&#34;Standard normal distribution table (Wackerly, Mendenhall, and Scheaffer 2014).&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution table &lt;span class=&#34;citation&#34;&gt;(Wackerly, Mendenhall, and Scheaffer 2014)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the illustration at the top of the table, we see that the values inside the table correspond to the area under the normal curve &lt;strong&gt;above&lt;/strong&gt; a certain &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. Since we are looking precisely at the probability above &lt;span class=&#34;math inline&#34;&gt;\(z = 1\)&lt;/span&gt; (since we look for &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;), we can simply proceed down the first (&lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;) column in the table until &lt;span class=&#34;math inline&#34;&gt;\(z = 1.0\)&lt;/span&gt;. The probability is 0.1587. Thus, &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1) = 0.1587\)&lt;/span&gt;. This is similar to what we found using R, except that values in the table are rounded to 4 digits.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 2&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; denote a normal random variable with mean 0 and standard deviation 1, find &lt;span class=&#34;math inline&#34;&gt;\(P(−1 \le Z \le 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We are looking for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2014.19.14.png&#34; alt=&#34;Standard normal distribution: P(−1 \le Z \le 1)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution: &lt;span class=&#34;math inline&#34;&gt;\(P(−1 \le Z \le 1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(1, lower.tail = TRUE) - pnorm(-1, lower.tail = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6826895&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the arguments by default for the mean and the standard deviation are &lt;code&gt;mean = 0&lt;/code&gt; and &lt;code&gt;sd = 1&lt;/code&gt;. Since this is what we need, we can omit them.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;For this exercise we proceed by steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The shaded area corresponds to the entire area under the normal curve minus the two white areas in both tails of the curve.&lt;/li&gt;
&lt;li&gt;We know that the normal distribution is symmetric.&lt;/li&gt;
&lt;li&gt;Therefore, the shaded area is the entire area under the curve minus two times the white area in the right tail of the curve, the white area in the right tail of the curve being &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;We also know that the entire area under the normal curve is 1.&lt;/li&gt;
&lt;li&gt;Thus, the shaded area is 1 minus 2 times &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1)\)&lt;/span&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(−1 \le Z \le 1) = 1 - 2 \cdot P(Z &amp;gt; 1) = 1 - 2 \cdot 0.1587 = 0.6826\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1) = 0.1587\)&lt;/span&gt; has been found in the previous exercise.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 3&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; denote a normal random variable with mean 0 and standard deviation 1, find &lt;span class=&#34;math inline&#34;&gt;\(P(0 \le Z \le 1.37)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We are looking for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2014.19.46.png&#34; alt=&#34;Standard normal distribution: P(0 \le Z \le 1.37)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard normal distribution: &lt;span class=&#34;math inline&#34;&gt;\(P(0 \le Z \le 1.37)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(0, lower.tail = FALSE) - pnorm(1.37, lower.tail = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4146565&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;Again we proceed by steps for this exercise:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We know that &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 0) = 0.5\)&lt;/span&gt; since the entire area under the curve is 1, half of it is 0.5.&lt;/li&gt;
&lt;li&gt;The shaded area is half of the entire area under the curve minus the area from 1.37 to infinity.&lt;/li&gt;
&lt;li&gt;The area under the curve from 1.37 to infinity corresponds to &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.37)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Therefore, the shaded area is &lt;span class=&#34;math inline&#34;&gt;\(0.5 - P(Z &amp;gt; 1.37)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;To find &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.37)\)&lt;/span&gt;, proceed down the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; column in the table to the entry 1.3 and then across the top of the table to the column labeled .07 to read &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.37) = .0853\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Thus,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(0 \le Z \le 1.37) = P(Z &amp;gt; 0) - P(Z &amp;gt; 1.37) = 0.5 - 0.0853 = 0.4147\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 4&lt;/h3&gt;
&lt;p&gt;Recap the example presented in the empirical rule: Suppose that the scores of an exam in statistics given to all students in a Belgian university are known to have a normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu = 67\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 9\)&lt;/span&gt;. What fraction of the scores lies between 70 and 80?&lt;/p&gt;
&lt;p&gt;We are looking for the shaded area in the following figure:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2016.24.30.png&#34; alt=&#34;P(70 \le X \le 80) where X \sim \mathcal{N}(\mu = 67, \sigma^2 = 9^2)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(70 \le X \le 80)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X \sim \mathcal{N}(\mu = 67, \sigma^2 = 9^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-r-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;In R&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(70, mean = 67, sd = 9, lower.tail = FALSE) - pnorm(80, mean = 67, sd = 9, lower.tail = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2951343&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;by-hand-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;By hand&lt;/h4&gt;
&lt;p&gt;Remind that we are looking for &lt;span class=&#34;math inline&#34;&gt;\(P(70 \le X \le 80)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X \sim \mathcal{N}(\mu = 67, \sigma^2 = 9^2)\)&lt;/span&gt;. The random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is in its “raw” format, meaning that it has not been standardized yet since the mean is 67 and the variance is &lt;span class=&#34;math inline&#34;&gt;\(9^2\)&lt;/span&gt;. We thus need to first apply the transformation to standardize the endpoints 70 and 80 with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Z = \frac{X - \mu}{\sigma}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After the standardization, &lt;span class=&#34;math inline&#34;&gt;\(x = 70\)&lt;/span&gt; becomes (in terms of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, so in terms of deviation from the mean expressed in standard deviation):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z = \frac{70 - 67}{9} = 0.3333\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and &lt;span class=&#34;math inline&#34;&gt;\(x = 80\)&lt;/span&gt; becomes:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[z = \frac{80 - 67}{9} = 1.4444\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The figure above in terms of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is now in terms of &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/Screenshot%202020-01-30%20at%2016.37.13.png&#34; alt=&#34;P(0.3333 \le Z \le 1.4444) where Z \sim \mathcal{N}(\mu = 0, \sigma^2 = 1)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(0.3333 \le Z \le 1.4444)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z \sim \mathcal{N}(\mu = 0, \sigma^2 = 1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Finding the probability &lt;span class=&#34;math inline&#34;&gt;\(P(0.3333 \le Z \le 1.4444)\)&lt;/span&gt; is similar to exercises 1 to 3:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The shaded area corresponds to the area under the curve from &lt;span class=&#34;math inline&#34;&gt;\(z = 0.3333\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(z = 1.4444\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;In other words, the shaded area is the area under the curve from &lt;span class=&#34;math inline&#34;&gt;\(z = 0.3333\)&lt;/span&gt; to infinity minus the area under the curve from &lt;span class=&#34;math inline&#34;&gt;\(z = 1.4444\)&lt;/span&gt; to infinity.&lt;/li&gt;
&lt;li&gt;From the table, &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 0.3333) = 0.3707\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(P(Z &amp;gt; 1.4444) = 0.0749\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Thus:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(0.3333 \le Z \le 1.4444) = P(Z &amp;gt; 0.3333) - P(Z &amp;gt; 1.4444) = 0.3707 - 0.0749 = 0.2958\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The difference with the probability found using in R comes from the rounding.&lt;/p&gt;
&lt;p&gt;To conclude this exercise, we can say that, given that the mean scores is 67 and the standard deviation is 9, 29.58% of the students scored between 70 and 80.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ex.-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ex. 5&lt;/h3&gt;
&lt;p&gt;See another example in a context &lt;a href=&#34;/blog/a-guide-on-how-to-read-statistical-tables/#example&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-the-normal-distribution-so-crucial-in-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why is the normal distribution so crucial in statistics?&lt;/h1&gt;
&lt;p&gt;The normal distribution is important for three main reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some statistical hypothesis tests assume that the data follow a normal distribution&lt;/li&gt;
&lt;li&gt;The central limit theorem states that, for a large number of observations (&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; 30\)&lt;/span&gt;), no matter what is the underlying distribution of the original variable, the distribution of the sample means (&lt;span class=&#34;math inline&#34;&gt;\(\overline{X}_n\)&lt;/span&gt;) and of the sum (&lt;span class=&#34;math inline&#34;&gt;\(S_n = \sum_{i = 1}^n X_i\)&lt;/span&gt;) may be approached by a normal distribution&lt;/li&gt;
&lt;li&gt;Linear and nonlinear regression assume that the residuals are normally-distributed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is therefore useful to know how to test for normality in R, which is the topic of next sections.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-test-the-normality-assumption&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to test the normality assumption&lt;/h1&gt;
&lt;p&gt;As mentioned above, some statistical tests require that the data follow a normal distribution, or the result of the test may be flawed.&lt;/p&gt;
&lt;p&gt;In this section, we show 4 complementary methods to determine whether your data follow a normal distribution in R.&lt;/p&gt;
&lt;div id=&#34;histogram&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Histogram&lt;/h2&gt;
&lt;p&gt;A histogram displays the spread and shape of a distribution, so it is a good starting point to evaluate normality. Let’s have a look at the histogram of a distribution that we would expect to follow a normal distribution, the height of 1,000 adults in cm:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The normal curve with the corresponding mean and variance has been added to the histogram. The histogram follows the normal curve so the data seems to follow a normal distribution.&lt;/p&gt;
&lt;p&gt;Below the minimal code for a histogram in R with the dataset &lt;code&gt;iris&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(iris)
hist(iris$Sepal.Length)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggplot2}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iris) +
  aes(x = Sepal.Length) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Histograms are however not sufficient, particularly in the case of small samples because the number of bins greatly change its appearance. Histograms are not recommended when the number of observations is less than 20 because it does not always correctly illustrate the distribution. See two examples below with dataset of 10 and 12 observations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Can you tell whether these datasets follow a normal distribution? Surprisingly, both follow a normal distribution!&lt;/p&gt;
&lt;p&gt;In the remaining of the article, we will use the dataset of the 12 adults. If you would like to follow my code in your own script, here is how I generated the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
dat_hist &amp;lt;- data.frame(
  value = rnorm(12, mean = 165, sd = 5)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;rnorm()&lt;/code&gt; function generates random numbers from a normal distribution (12 random numbers with a mean of 165 and standard deviation of 5 in this case). These 12 observations are then saved in the dataset called &lt;code&gt;dat_hist&lt;/code&gt; under the variable &lt;code&gt;value&lt;/code&gt;. Note that &lt;code&gt;set.seed(42)&lt;/code&gt; is important to obtain the exact same data as me.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Density plot&lt;/h2&gt;
&lt;p&gt;Density plots also provide a visual judgment about whether the data follow a normal distribution. They are similar to histograms as they also allow to analyze the spread and the shape of the distribution. However, they are a smoothed version of the histogram. Here is the density plot drawn from the dataset on the height of the 12 adults discussed above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(density(dat_hist$value))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;{ggpubr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggpubr&amp;quot;) # package must be installed first
ggdensity(dat_hist$value,
  main = &amp;quot;Density plot of adult height&amp;quot;,
  xlab = &amp;quot;Height (cm)&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since it is hard to test for normality from histograms and density plots only, it is recommended to corroborate these graphs with a QQ-plot. QQ-plot, also known as normality plot, is the third method presented to evaluate normality.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qq-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;QQ-plot&lt;/h2&gt;
&lt;p&gt;Like histograms and density plots, QQ-plots allow to visually evaluate the normality assumption. Here is the QQ-plot drawn from the dataset on the height of the 12 adults discussed above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(car)
qqPlot(dat_hist$value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12  2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;{ggpubr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggpubr)
ggqqplot(dat_hist$value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead of looking at the spread of the data (as it is the case with histograms and density plots), with QQ-plots we only need to ascertain whether the data points follow the line (sometimes referred as Henry’s line).&lt;/p&gt;
&lt;p&gt;If points are close to the reference line and within the confidence bands, the normality assumption can be considered as met. The bigger the deviation between the points and the reference line and the more they lie outside the confidence bands, the less likely that the normality condition is met. The height of these 12 adults seem to follow a normal distribution because all points lie within the confidence bands.&lt;/p&gt;
&lt;p&gt;When facing a non-normal distribution as shown by the QQ-plot below (systematic departure from the reference line), the first step is usually to apply the logarithm transformation on the data and recheck to see whether the log-transformed data are normally distributed. Applying the logarithm transformation can be done with the &lt;code&gt;log()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that QQ-plots are also a convenient way to assess whether residuals from regression analysis follow a normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;normality-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normality test&lt;/h2&gt;
&lt;p&gt;The 3 tools presented above were a visual inspection of the normality. Nonetheless, visual inspection may sometimes be unreliable so it is also possible to formally test whether the data follow a normal distribution with statistical tests. These normality tests compare the distribution of the data to a normal distribution in order to assess whether observations show an important deviation from normality.&lt;/p&gt;
&lt;p&gt;The two most common normality tests are Shapiro-Wilk’s test and Kolmogorov-Smirnov test. Both tests have the same hypotheses, that is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;: the data follow a normal distribution&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;: the data do not follow a normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Shapiro-Wilk test is recommended for normality test as it provides better power than Kolmogorov-Smirnov test.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; In R, the Shapiro-Wilk test of normality can be done with the function &lt;code&gt;shapiro.test()&lt;/code&gt;:&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shapiro.test(dat_hist$value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Shapiro-Wilk normality test
## 
## data:  dat_hist$value
## W = 0.93968, p-value = 0.4939&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output, we see that the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt; 0.05\)&lt;/span&gt; implying that we do not reject the null hypothesis that the data follow a normal distribution. This test goes in the same direction than the QQ-plot, which showed no significant deviation from the normality (as all points lied within the confidence bands).&lt;/p&gt;
&lt;p&gt;It is important to note that, in practice, normality tests are often considered as too conservative in the sense that for large sample size (&lt;span class=&#34;math inline&#34;&gt;\(n &amp;gt; 50\)&lt;/span&gt;), a small deviation from the normality may cause the normality condition to be violated. A normality test is a hypothesis test, so as the sample size increases, their capacity of detecting smaller differences increases. So as the number of observations increases, the Shapiro-Wilk test becomes very sensitive even to a small deviation from normality. As a consequence, it happens that according to the normality test the data do not follow a normal distribution although the departures from the normal distribution are negligible and the data in fact follow a normal distribution. For this reason, it is often the case that the normality condition is verified based on a combination of all methods presented in this article, that is, visual inspections (with histograms and QQ-plots) and a formal inspection (with the Shapiro-Wilk test for instance).&lt;/p&gt;
&lt;p&gt;I personally tend to prefer QQ-plots over histograms and normality tests so I do not have to bother about the sample size. This article showed the different methods that are available, your choice will of course depends on the type of your data and the context of your analyses.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope the article helped you to learn more about the normal distribution and how to test for normality in R.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-wackerly2014mathematical&#34;&gt;
&lt;p&gt;Wackerly, Dennis, William Mendenhall, and Richard L Scheaffer. 2014. &lt;em&gt;Mathematical Statistics with Applications&lt;/em&gt;. Cengage Learning.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The argument &lt;code&gt;lower.tail = TRUE&lt;/code&gt; is also the default so we could omit it as well. However, for clarity and to make sure I compute the propabilities in the correct side of the curve, I used to keep this argument explicit by writing it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The &lt;code&gt;set.seed()&lt;/code&gt; function accepts any numeric as argument. Generating random numbers (via &lt;code&gt;rnorm()&lt;/code&gt; for instance) implies that R will generates different random numbers every time you generate these random numbers (so every time you run the function &lt;code&gt;rnorm()&lt;/code&gt;). To make sure R generates the exact same numbers every time you run the function, a seed can be set with the function &lt;code&gt;set.seed()&lt;/code&gt;. Setting a seed implies that R will generate random numbers, but these numbers will always be the same as long as the seed is the same. This allows to replicate results that are based on a random generation. Change the seed if you want to generate other random values.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The Shapiro-Wilk test is based on the correlation between the sample and the corresponding normal scores.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;In R, the Kolmogorov-Smirnov test is performed with the function &lt;code&gt;ks.test()&lt;/code&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Chi-square test of independence by hand</title>
      <link>/blog/chi-square-test-of-independence-by-hand/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/chi-square-test-of-independence-by-hand/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypotheses&#34;&gt;Hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-the-test-works&#34;&gt;How the test works?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#observed-frequencies&#34;&gt;Observed frequencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expected-frequencies&#34;&gt;Expected frequencies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-statistic&#34;&gt;Test statistic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#critical-value&#34;&gt;Critical value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion-and-interpretation&#34;&gt;Conclusion and interpretation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/chi-square-test-of-independence-by-hand_files/chi-square-test-of-independence-by-hand.jpeg&#34; alt=&#34;Photo by David Pennington&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by David Pennington&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Chi-square tests of independence test whether two &lt;a href=&#34;/blog/variable-types-and-examples/#qualitative&#34;&gt;qualitative variables&lt;/a&gt; are independent, that is, whether there exists a relationship between two categorical variables. In other words, this test is used to determine whether the values of one of the 2 qualitative variables depend on the values of the other qualitative variable.&lt;/p&gt;
&lt;p&gt;If the test shows no association between the two variables (i.e., the variables are independent), it means that knowing the value of one variable gives no information about the value of the other variable. On the contrary, if the test shows a relationship between the variables (i.e., the variables are dependent), it means that knowing the value of one variable provides information about the value of the other variable.&lt;/p&gt;
&lt;p&gt;This article focuses on how to perform a Chi-square test of independence by hand and how to interpret the results with a concrete example. To learn how to do this test in R, read the article “&lt;a href=&#34;/blog/chi-square-test-of-independence-in-r/&#34;&gt;Chi-square test of independence in R&lt;/a&gt;”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypotheses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hypotheses&lt;/h1&gt;
&lt;p&gt;The Chi-square test of independence is a hypothesis test so it has a null (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) and an alternative hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt; : the variables are independent, there is &lt;strong&gt;no&lt;/strong&gt; relationship between the two categorical variables. Knowing the value of one variable does not help to predict the value of the other variable&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; : the variables are dependent, there is a relationship between the two categorical variables. Knowing the value of one variable helps to predict the value of the other variable&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-the-test-works&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How the test works?&lt;/h1&gt;
&lt;p&gt;The Chi-square test of independence works by comparing the observed frequencies (so the frequencies observed in your sample) to the expected frequencies if there was no relationship between the two categorical variables (so the expected frequencies if the null hypothesis was true).&lt;/p&gt;
&lt;p&gt;If the difference between the observed frequencies and the expected frequencies is &lt;strong&gt;small&lt;/strong&gt;, we cannot reject the null hypothesis of independence and thus we cannot reject the fact that the two &lt;strong&gt;variables are not related&lt;/strong&gt;. On the other hand, if the difference between the observed frequencies and the expected frequencies is &lt;strong&gt;large&lt;/strong&gt;, we can reject the null hypothesis of independence and thus we can conclude that the two &lt;strong&gt;variables are related&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The threshold between a small and large difference is a value that comes from the Chi-square distribution (hence the name of the test). This value, referred as the critical value, depends on the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (usually set equal to 5%) and on the degrees of freedom. This critical value can be found in the statistical table of the Chi-square distribution. More on this critical value and the degrees of freedom later in the article.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;For our example, we want to determine whether there is a statistically significant association between smoking and being a professional athlete. Smoking can only be “yes” or “no” and being a professional athlete can only be “yes” or “no”. The two variables of interest are qualitative variables so we need to use a Chi-square test of independence, and the data have been collected on 28 persons.&lt;/p&gt;
&lt;p&gt;Note that we chose binary variables (binary variables = qualitative variables with two levels) for the sake of easiness, but the Chi-square test of independence can also be performed on qualitative variables with more than two levels. For instance, if the variable smoking had three levels: (i) non-smokers, (ii) moderate smokers and (iii) heavy smokers, the steps and the interpretation of the results of the test are similar than with two levels.&lt;/p&gt;
&lt;div id=&#34;observed-frequencies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Observed frequencies&lt;/h2&gt;
&lt;p&gt;Our data are summarized in the contingency table below reporting the number of people in each subgroup, totals by row, by column and the grand total:&lt;/p&gt;
&lt;table style=&#34;width:68%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt; &lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Non-smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Non-athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;expected-frequencies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expected frequencies&lt;/h2&gt;
&lt;p&gt;Remember that for the Chi-square test of independence we need to determine whether the observed counts are significantly different from the counts that we would expect if there was no association between the two variables. We have the observed counts (see the table above), so we now need to compute the expected counts in the case the variables were independent. These expected frequencies are computed for each subgroup one by one with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{expected frequencies} = \frac{\text{total # of obs. for the row} \cdot \text{total # of obs. for the column}}{\text{total number of observations}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where obs. correspond to observations. Given our table of observed frequencies above, below is the table of the expected frequencies computed for each subgroup:&lt;/p&gt;
&lt;table style=&#34;width:94%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt; &lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Non-smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Smoker&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(18 * 14) / 28 = 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(18 * 14) / 28 = 9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Non-athlete&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10 * 14) / 28 = 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(10 * 14) / 28 = 5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note that the Chi-square test of independence should only be done when the &lt;strong&gt;expected&lt;/strong&gt; frequencies in all groups are equal to or greater than 5. This assumption is met for our example as the minimum number of expected frequencies is 5.
&lt;!-- If the condition is not met, the [Fisher&#39;s exact test](/blog/xxx/) is preferred. --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-statistic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Test statistic&lt;/h2&gt;
&lt;p&gt;We have the observed and expected frequencies. We now need to compare these frequencies to determine if they differ significantly. The difference between the observed and expected frequencies, referred as the test statistic (or t-stat) and denoted &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt;, is computed as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\chi^2 = \sum_{i, j} \frac{\big(O_{ij} - E_{ij}\big)^2}{E_{ij}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(O\)&lt;/span&gt; represents the observed frequencies and &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; the expected frequencies. We use the square of the differences between the observed and expected frequencies to make sure that negative differences are not compensated by positive differences. The formula looks more complex than what it really is, so let’s illustrate it with our example. We first compute the difference in each subgroup one by one according to the formula:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in the subgroup of athlete and non-smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(14 - 9)^2}{9} = 2.78\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in the subgroup of non-athlete and non-smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(0 - 5)^2}{5} = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in the subgroup of athlete and smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(4 - 9)^2}{9} = 2.78\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;in the subgroup of non-athlete and smoker: &lt;span class=&#34;math inline&#34;&gt;\(\frac{(10 - 5)^2}{5} = 5\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and then we sum them all to obtain the test statistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\chi^2 = 2.78 + 5 + 2.78 + 5 = 15.56\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critical-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Critical value&lt;/h2&gt;
&lt;p&gt;The test statistic alone is not enough to conclude for independence or dependence between the two variables. As previously mentioned, this test statistic (which in some sense is the difference between the observed and expected frequencies) must be compared to a critical value to determine whether the difference is large or small. One cannot tell that a test statistic is large or small without putting it in perspective with the critical value.&lt;/p&gt;
&lt;p&gt;If the test statistic is above the critical value, it means that the probability of observing such a difference between the observed and expected frequencies is unlikely. On the other hand, if the test statistic is below the critical value, it means that the probability of observing such a difference is likely. If it is likely to observe this difference, we cannot reject the hypothesis that the two variables are independent, otherwise we can conclude that there exists a relationship between the variables.&lt;/p&gt;
&lt;p&gt;The critical value can be found in the statistical table of the Chi-square distribution and depends on the significance level, denoted &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, and the degrees of freedom, denoted &lt;span class=&#34;math inline&#34;&gt;\(df\)&lt;/span&gt;. The significance level is usually set equal to 5%. The degrees of freedom for a Chi-square test of independence is found as follow:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[df = (\text{number of rows} - 1) \cdot (\text{number of columns} - 1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our example, the degrees of freedom is thus &lt;span class=&#34;math inline&#34;&gt;\(df = (2 - 1) \cdot (2 - 1) = 1\)&lt;/span&gt; since there are two rows and two columns in the contingency table (totals do not count as a row or column).&lt;/p&gt;
&lt;p&gt;We now have all the necessary information to find the critical value in the Chi-square table (&lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(df = 1\)&lt;/span&gt;). To find the critical value we need to look at the row &lt;span class=&#34;math inline&#34;&gt;\(df = 1\)&lt;/span&gt; and the column &lt;span class=&#34;math inline&#34;&gt;\(\chi^2_{0.050}\)&lt;/span&gt; (since &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;) in the picture below. The critical value is &lt;span class=&#34;math inline&#34;&gt;\(3.84146\)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/chi-square-test-of-independence-by-hand_files/Screenshot%202020-01-28%20at%2000.56.28.png&#34; alt=&#34;Chi-square table - Critical value for alpha = 5% and df = 1&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Chi-square table - Critical value for alpha = 5% and df = 1&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-interpretation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion and interpretation&lt;/h2&gt;
&lt;p&gt;Now that we have the test statistic and the critical value, we can compare them to check whether the null hypothesis of independence of the variables is rejected or not. In our example,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{test statistic} = 15.56 &amp;gt; \text{critical value} = 3.84146\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Like for any statistical test, when the test statistic is larger than the critical value, we can reject the null hypothesis at the specified significance level.&lt;/p&gt;
&lt;p&gt;In our case, we can therefore reject the null hypothesis of independence between the two categorical variables at the 5% significance level.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Rightarrow\)&lt;/span&gt; This means that there is a significant relationship between the smoking habit and being an athlete or not. Knowing the value of one variable helps to predict the value of the other variable.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope the article helped you to perform the Chi-square test of independence by hand and interpret its results. If you would like to learn how to do this test in R, read the article “&lt;a href=&#34;/blog/chi-square-test-of-independence-in-r/&#34;&gt;Chi-square test of independence in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For readers that prefer to check the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value in order to reject or not the null hypothesis, I also created a &lt;a href=&#34;/blog/a-guide-on-how-to-read-statistical-tables/&#34;&gt;Shiny app&lt;/a&gt; to help you compute the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value given a test statistic.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Descriptive statistics by hand</title>
      <link>/blog/descriptive-statistics-by-hand/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/descriptive-statistics-by-hand/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location-versus-dispersion-measures&#34;&gt;Location versus dispersion measures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location&#34;&gt;Location&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-and-maximum&#34;&gt;Minimum and maximum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mean&#34;&gt;Mean&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#median&#34;&gt;Median&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#odd-number-of-observations&#34;&gt;Odd number of observations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#even-number-of-observations&#34;&gt;Even number of observations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mean-vs.median&#34;&gt;Mean vs. median&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#st-and-3rd-quartiles&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(1^{st}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(3^{rd}\)&lt;/span&gt; quartiles&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#q_0.25-q_0.75-and-q_0.5&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q_{0.5}\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-note-on-deciles-and-percentiles&#34;&gt;A note on deciles and percentiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mode&#34;&gt;Mode&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mode-for-qualitative-variables&#34;&gt;Mode for qualitative variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dispersion&#34;&gt;Dispersion&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#range&#34;&gt;Range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation&#34;&gt;Standard deviation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation-for-a-population&#34;&gt;Standard deviation for a population&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation-for-a-sample&#34;&gt;Standard deviation for a sample&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance&#34;&gt;Variance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#variance-for-a-population&#34;&gt;Variance for a population&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance-for-a-sample&#34;&gt;Variance for a sample&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-deviation-vs.variance&#34;&gt;Standard deviation vs. variance&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#notations&#34;&gt;Notations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interquartile-range&#34;&gt;Interquartile range&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-variation&#34;&gt;Coefficient of variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-variation-vs.standard-deviation&#34;&gt;Coefficient of variation vs. standard deviation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/descriptive-statistics-by-hand.jpeg&#34; alt=&#34;Photo by Pop &amp;amp; Zebra&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Pop &amp;amp; Zebra&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This article explains how to compute the main descriptive statistics by hand and how to interpret them. To learn how to compute these measures in R, read the article “&lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;Descriptive statistics in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;Descriptive statistics (in the broad sense of the term) is a branch of statistics aiming at summarizing, describing and presenting a series of values or a dataset. Long series of values without any preparation or without any summary measures are often not informative due to the difficulty of recognizing any pattern in the data. Below an example with the height (in cm) of a population of 100 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt;, &lt;em&gt;173.9&lt;/em&gt;, &lt;em&gt;190.1&lt;/em&gt;, &lt;em&gt;174.1&lt;/em&gt;, &lt;em&gt;195.2&lt;/em&gt;, &lt;em&gt;174.4&lt;/em&gt;, &lt;em&gt;188&lt;/em&gt;, &lt;em&gt;197.9&lt;/em&gt;, &lt;em&gt;161.1&lt;/em&gt;, &lt;em&gt;172.2&lt;/em&gt;, &lt;em&gt;173.7&lt;/em&gt;, &lt;em&gt;181.4&lt;/em&gt;, &lt;em&gt;172.2&lt;/em&gt;, &lt;em&gt;148.4&lt;/em&gt;, &lt;em&gt;150.6&lt;/em&gt;, &lt;em&gt;188.2&lt;/em&gt;, &lt;em&gt;171.9&lt;/em&gt;, &lt;em&gt;157.2&lt;/em&gt;, &lt;em&gt;173.3&lt;/em&gt;, &lt;em&gt;187.1&lt;/em&gt;, &lt;em&gt;194&lt;/em&gt;, &lt;em&gt;170.7&lt;/em&gt;, &lt;em&gt;172.4&lt;/em&gt;, &lt;em&gt;157.4&lt;/em&gt;, &lt;em&gt;179.6&lt;/em&gt;, &lt;em&gt;168.6&lt;/em&gt;, &lt;em&gt;179.6&lt;/em&gt;, &lt;em&gt;182&lt;/em&gt;, &lt;em&gt;185.4&lt;/em&gt;, &lt;em&gt;168.9&lt;/em&gt;, &lt;em&gt;180&lt;/em&gt;, &lt;em&gt;157.8&lt;/em&gt;, &lt;em&gt;167.2&lt;/em&gt;, &lt;em&gt;166.5&lt;/em&gt;, &lt;em&gt;150.9&lt;/em&gt;, &lt;em&gt;175.4&lt;/em&gt;, &lt;em&gt;177.1&lt;/em&gt;, &lt;em&gt;171.4&lt;/em&gt;, &lt;em&gt;182.6&lt;/em&gt;, &lt;em&gt;167.7&lt;/em&gt;, &lt;em&gt;161.3&lt;/em&gt;, &lt;em&gt;179.3&lt;/em&gt;, &lt;em&gt;166.9&lt;/em&gt;, &lt;em&gt;189.4&lt;/em&gt;, &lt;em&gt;170.7&lt;/em&gt;, &lt;em&gt;181.6&lt;/em&gt;, &lt;em&gt;178.2&lt;/em&gt;, &lt;em&gt;167.2&lt;/em&gt;, &lt;em&gt;190.8&lt;/em&gt;, &lt;em&gt;181.4&lt;/em&gt;, &lt;em&gt;175.9&lt;/em&gt;, &lt;em&gt;177.8&lt;/em&gt;, &lt;em&gt;181.8&lt;/em&gt;, &lt;em&gt;175.9&lt;/em&gt;, &lt;em&gt;145.1&lt;/em&gt;, &lt;em&gt;177.8&lt;/em&gt;, &lt;em&gt;171.3&lt;/em&gt;, &lt;em&gt;176.9&lt;/em&gt;, &lt;em&gt;180.8&lt;/em&gt;, &lt;em&gt;189&lt;/em&gt;, &lt;em&gt;167.7&lt;/em&gt;, &lt;em&gt;188&lt;/em&gt;, &lt;em&gt;178.4&lt;/em&gt;, &lt;em&gt;185.4&lt;/em&gt;, &lt;em&gt;184.2&lt;/em&gt;, &lt;em&gt;182.2&lt;/em&gt;, &lt;em&gt;164.6&lt;/em&gt;, &lt;em&gt;174.1&lt;/em&gt;, &lt;em&gt;181.2&lt;/em&gt;, &lt;em&gt;165.5&lt;/em&gt;, &lt;em&gt;169.6&lt;/em&gt;, &lt;em&gt;180.8&lt;/em&gt;, &lt;em&gt;182.7&lt;/em&gt;, &lt;em&gt;179.6&lt;/em&gt;, &lt;em&gt;166.1&lt;/em&gt;, &lt;em&gt;164&lt;/em&gt;, &lt;em&gt;190.1&lt;/em&gt;, &lt;em&gt;177.6&lt;/em&gt;, &lt;em&gt;175.9&lt;/em&gt;, &lt;em&gt;173.8&lt;/em&gt;, &lt;em&gt;163.1&lt;/em&gt;, &lt;em&gt;181.1&lt;/em&gt;, &lt;em&gt;172.8&lt;/em&gt;, &lt;em&gt;173.2&lt;/em&gt;, &lt;em&gt;184.3&lt;/em&gt;, &lt;em&gt;183.2&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Facing this series, it is hard (not to say impossible) for anyone to understand the data and have a clear view of the size of these adults in a reasonable amount of time. Descriptive statistics allow to summarize, and thus have a better overview of the data. Of course, by summarizing data through one or several measures, some information will inevitably be lost. However, in many cases it is generally better to lose some information but in return gain an overview.&lt;/p&gt;
&lt;p&gt;Descriptive statistics is often the first step and an important part in any statistical analysis. It allows to check the quality of the data by detecting potential outliers (i.e., data points that appear to be separated from the rest of the data), collection or encoding errors. It also helps to “understand” the data and if well presented, descriptive statistics is already a good starting point for further analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;location-versus-dispersion-measures&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Location versus dispersion measures&lt;/h1&gt;
&lt;p&gt;Several different measures (called statistics if we are analyzing a sample) are used to summarize the data. Some of them give an understanding about the location of the data, others give and understanding about the dispersion of the data. In practice, both types of measures are often used together in order to summarize the data in the most concise but complete way. We illustrate this point with the graph below, representing the height (in cm) of 100 persons divided into two groups (50 persons in each group):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The black line corresponds to the mean. The mean height (in cm) is similar in both groups. However, it is clear that the dispersion of heights are very different in the two groups. For this reason, location or dispersion measures are often not enough if presented individually and it is a good practice to present several statistics from both types of measures.&lt;/p&gt;
&lt;p&gt;In the following sections, we detail the most common location and dispersion measures and illustrate them with examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;location&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Location&lt;/h1&gt;
&lt;p&gt;Location measures allow to see “where” the data are located, around which values. In other words, location measures give an understanding on what is the central tendency, the “position” of the data as a whole. It includes the following statistics (others exist but we focus on the most common ones):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;minimum&lt;/li&gt;
&lt;li&gt;maximum&lt;/li&gt;
&lt;li&gt;mean&lt;/li&gt;
&lt;li&gt;median&lt;/li&gt;
&lt;li&gt;first quartile&lt;/li&gt;
&lt;li&gt;third quartile&lt;/li&gt;
&lt;li&gt;mode&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We detail and compute by hand each of them in the following sections.&lt;/p&gt;
&lt;div id=&#34;minimum-and-maximum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Minimum and maximum&lt;/h2&gt;
&lt;p&gt;Minimum (&lt;span class=&#34;math inline&#34;&gt;\(min\)&lt;/span&gt;) and maximum (&lt;span class=&#34;math inline&#34;&gt;\(max\)&lt;/span&gt;) are simply the lowest and largest values, respectively. Given the height (in cm) of a sample of 6 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt; and &lt;em&gt;173.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The minimum is 169.4 cm and the maximum is 188.7 cm. These two basic statistics give a clear idea about the size of the smallest and tallest of these 6 adults.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean&lt;/h2&gt;
&lt;p&gt;The mean, also known as average, is probably the most common statistics. It gives an idea on what is the average value, that is, the central value of the data or in other words the center of gravity. The mean is found by summing all values and dividing this sum by the number of observations (denoted &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[mean = \bar{x} = \frac{\text{sum of all values}}{\text{number of values}} = \frac{1}{n}\sum^{n}_{i = 1} x_i\]&lt;/span&gt;
Below a visual representation of the mean:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/mean.png&#34; alt=&#34;Mean. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Mean. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Given our sample of 6 adults presented above, the mean is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{x} = \frac{188.7 + 169.4 + 178.6 + 181.3 + 179 + 173.9}{6} = 178.4833\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the mean size, that is, the average size of our sample of 6 adults is 178.48 cm (rounded to 2 decimals).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;median&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Median&lt;/h2&gt;
&lt;p&gt;The median is another measure of location so it also gives an idea about the central tendency of the data. The interpretation of the median is that there are as many observations below as above the median. In other words, 50% of the observations lie below the median, and 50% of the observations lie above the median. Below a visual representation of the median:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/median.png&#34; alt=&#34;Median. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Median. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The easiest way to compute the median is by first sorting the data from lowest to highest (i.e., in ascending order) then take the middle point as the median. From the sorted values, for an odd number of observations, the middle point is easy to find: it is the value with as many observations below as above. Still from the sorted values, for an even number of observations, the middle point is exactly between the two middle values. Formally, after sorting, the median is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; (number of observations) is odd: &lt;span class=&#34;math display&#34;&gt;\[med(x) = x_{\frac{n+1}{2}}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is even: &lt;span class=&#34;math display&#34;&gt;\[med(x) = \frac{1}{2}\big(x_{\frac{n}{2}} + x_{\frac{n}{2} + 1}\big)\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where the subscript of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; denotes the numbering of the sorted data. The formulas look harder than they really are, so let’s see with two concrete examples.&lt;/p&gt;
&lt;div id=&#34;odd-number-of-observations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Odd number of observations&lt;/h3&gt;
&lt;p&gt;Given the height of a sample of 7 adults taken from the 100 adults presented in the introduction:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We first sort the order from lowest to highest:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt; and &lt;em&gt;188.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given that the number of observations &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is odd (since &lt;span class=&#34;math inline&#34;&gt;\(n = 7\)&lt;/span&gt;), the median is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[med(x) = x_\frac{7 + 1}{2} = x_4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we take the fourth value from the sorted values, which corresponds to 166.4. In conclusion, the median size of these 7 adults is 166.4 cm. As you can see, there are 3 observations below 166.4 and 3 observations above 166.4 cm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;even-number-of-observations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Even number of observations&lt;/h3&gt;
&lt;p&gt;Now let’s see when the number of observations is even, which is slightly more complicated than when the number of observations is odd. Given the height of a sample of 6 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt; and &lt;em&gt;173.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We sort the values in ascending order:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;173.9&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt; and &lt;em&gt;188.7&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given that the number of observations &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is even (since &lt;span class=&#34;math inline&#34;&gt;\(n = 6\)&lt;/span&gt;), the median is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[med(x) = \frac{1}{2}\big(x_{\frac{6}{2}} + x_{\frac{6}{2} + 1}\big) = \frac{1}{2}\big(x_{3} + x_{4}\big)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we sum the third and fourth values from the sorted values and divide this sum by 2 (which is equivalent than taking the mean of these two middle values):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{2}(178.6 + 179) = 178.8\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the median size of these 6 adults is 178.8 cm. Again, remark that there are as many observations below as above 178.8 cm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mean-vs.median&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean vs. median&lt;/h2&gt;
&lt;p&gt;Although the mean and median are often relatively close to each other they should not be confused since they both have advantages and disadvantages in different contexts. Besides the fact that almost everyone knows (or at least have heard about) the mean, it has the advantage that it gives a unique picture for each different series of data. However, it has the disadvantage that the mean is sensible to outliers (i.e., extreme values). On the other hand, the advantage of the median is that it is resistant to outliers and the inconvenient is that it may be the exact same value for very different series of data (so not unique to the data).&lt;/p&gt;
&lt;p&gt;To illustrate the “sensible to outlier” argument, consider 3 friends in a bar comparing their salaries. Their salaries are &lt;em&gt;1800&lt;/em&gt;, &lt;em&gt;2000&lt;/em&gt; and &lt;em&gt;2100&lt;/em&gt;€, for an average (mean) salary of &lt;em&gt;1967&lt;/em&gt;€. A friend of them (who happens to be friend with Bill Gates as well) joins them in the bar. Their salaries are now &lt;em&gt;1800&lt;/em&gt;, &lt;em&gt;2000&lt;/em&gt;, &lt;em&gt;2100&lt;/em&gt; and &lt;em&gt;1000000&lt;/em&gt;€. The average salary of the 4 friends is now &lt;em&gt;251475&lt;/em&gt;€, compared to &lt;em&gt;1967&lt;/em&gt;€ without the rich friend. Although it is statistically correct to say that the average salary of the 4 friends is &lt;em&gt;251475&lt;/em&gt;€, you will concede that this measure does not represent a fair image of the salaries of the 4 friends, as 3 of them earn much less than the average salary. As we have just seen, the mean is sensible to outliers. On the other hand, if we report the medians, we see that the median salary of the 3 first friends is &lt;em&gt;2000&lt;/em&gt;€, and the median salary of the 4 friends is &lt;em&gt;2050&lt;/em&gt;€. As you can see with this example, the median is not sensible to outliers and for series with such extreme value(s), the median is more appropriate compared to the mean as it often gives a better representation of the data. (&lt;em&gt;Note:&lt;/em&gt; this example also shows how a large majority of people earn less than the average salary reported in the news. This is however beyond the scope of the article.)&lt;/p&gt;
&lt;p&gt;Given the previous example, one may then choose to always use the median instead of the mean. However, the median has it own inconvenient which the mean does not have: the median is less unique and less specific to its underlying data than the mean. Consider the following data, representing the grades of 5 students taking a statistics and economics exam:&lt;/p&gt;
&lt;table style=&#34;width:51%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;studentID&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;economics&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;statistics&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The median of the grades is the same in economics and statistics (median = &lt;em&gt;10&lt;/em&gt;). Therefore, had we computed only the medians, we could have concluded that the students performed as well in economics as in statistics. However, although the medians are exactly the same for both classes, it is clear that students performed better in economics than in statistics. In fact, the mean of the grades in economics is &lt;em&gt;13.6&lt;/em&gt; and the mean of the grades in statistics is &lt;em&gt;8.6&lt;/em&gt;. What we have just shown here is that the median is based only on one single value, the middle value, or on the two middle values if there are an even number of observations, while the mean is based on all values (and thus includes more information). The median is therefore not sensible to outliers, but it is also not unique (i.e., not specific) to different series of data, whereas the mean is much more likely to be different and unique for different series of data. This difference in terms of specificity and uniqueness between the two measures may make the mean more useful for data with no outlier.&lt;/p&gt;
&lt;p&gt;In conclusion, depending on the context and the data, it is often more interesting to report the mean or the median, or both. As a last remark regarding the comparison between the two most important location measures, note that when the mean and median are equal, the distribution of your data can often be considered to follow a normal distribution (also referred as Gaussian distribution).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;st-and-3rd-quartiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(1^{st}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(3^{rd}\)&lt;/span&gt; quartiles&lt;/h2&gt;
&lt;p&gt;The first and third quartiles are similar to the median in the sense that they also divide the observations into two parts, except that these parts are not equal. Remind that the median divides the data into two equal parts (with 50% of the observations below and 50% above the median). The first quartile cuts the observations such that there are 25% of the observations &lt;strong&gt;below&lt;/strong&gt; and thus 75% &lt;strong&gt;above&lt;/strong&gt; the first quartile. The third quartile, as you have guessed by now, represents the value with 75% of the observations below it and thus 25% of the observations above it. There exists several methods to compute the first and third quartile (which sometimes give slight differences, R for instance uses a different method), but here is I believe the easiest one when computing these statistics by hand:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;sort the data in ascending order&lt;/li&gt;
&lt;li&gt;compute &lt;span class=&#34;math inline&#34;&gt;\(0.25 \cdot n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(0.75 \cdot n\)&lt;/span&gt; (i.e., 0.25 and 0.75 times the number of observations)&lt;/li&gt;
&lt;li&gt;round up these two numbers to the next whole number&lt;/li&gt;
&lt;li&gt;these two numbers represent the rank of the first and third quartile (in the sorted series), respectively&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The steps are the same for both an odd and even number of observations. Here is an example with the following series, representing the height in cm of 9 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We first order from lowest to highest:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt; and &lt;em&gt;188.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are 9 observations so &lt;span class=&#34;math display&#34;&gt;\[0.25 \cdot 9 = 2.25\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[0.75 \cdot 9 = 6.75\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Rounding up to the whole number gives 3 and 7, which represent the rank of the first and third quartiles, respectively. Therefore, the first quartile is 163.9 cm and the third quartile is 181.5 cm.&lt;/p&gt;
&lt;p&gt;In conclusion, 25% of adults are less than 163.9 cm tall (and thus 75% of them are more than 163.9 cm tall), while 75% of adults are less than 181.5 cm tall (and thus 25% of them are more than 181.5 cm tall).&lt;/p&gt;
&lt;div id=&#34;q_0.25-q_0.75-and-q_0.5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q_{0.5}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Note that the first quartile is denoted &lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt; and the third quartile is denoted &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; stands for quartile). As you can see, the median is actually the second quartile and for this reason it is also sometimes denoted &lt;span class=&#34;math inline&#34;&gt;\(q_{0.5}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-note-on-deciles-and-percentiles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A note on deciles and percentiles&lt;/h3&gt;
&lt;p&gt;Deciles and percentiles are similar to quartiles except that they cuts the data in 10 and 100 equal parts. For instance, the &lt;span class=&#34;math inline&#34;&gt;\(4^{th}\)&lt;/span&gt; decile (&lt;span class=&#34;math inline&#34;&gt;\(q_{0.4}\)&lt;/span&gt;) is the value such that there are 40% of the observations below it and thus 60% of the observations above it. Furthermore, the &lt;span class=&#34;math inline&#34;&gt;\(98^{th}\)&lt;/span&gt; percentile (&lt;span class=&#34;math inline&#34;&gt;\(q_{0.98}\)&lt;/span&gt;, also sometimes denoted &lt;span class=&#34;math inline&#34;&gt;\(P98\)&lt;/span&gt;) is the value such that there are 98% of the observations below it and thus 2% of the observations above it. Percentiles are often used for the weight and height of babies, giving precise information to the parents about where their child stands compared to other children of the same age.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mode&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mode&lt;/h2&gt;
&lt;p&gt;The mode of a series is the value that appears most often. In other words, it is the value that has the highest number of occurrences. Given the height of 9 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;170&lt;/em&gt;, &lt;em&gt;168&lt;/em&gt;, &lt;em&gt;171&lt;/em&gt;, &lt;em&gt;170&lt;/em&gt;, &lt;em&gt;182&lt;/em&gt;, &lt;em&gt;165&lt;/em&gt;, &lt;em&gt;170&lt;/em&gt;, &lt;em&gt;189&lt;/em&gt; and &lt;em&gt;167&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mode is 170 because it is the most common value with 3 occurrences. All other values appear only once. In conclusion, most adults of this sample are 170 cm tall. Note that it is possible that a series has no mode (e.g., &lt;em&gt;4&lt;/em&gt;, &lt;em&gt;7&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt; and &lt;em&gt;10&lt;/em&gt;) or more than one mode (e.g., &lt;em&gt;4&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, &lt;em&gt;8&lt;/em&gt;, &lt;em&gt;11&lt;/em&gt; and &lt;em&gt;11&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Data with two modes are often called bimodal and data with more than two modes are often called multimodal, as opposed to series with one mode which are referred as unimodal.&lt;/p&gt;
&lt;div id=&#34;mode-for-qualitative-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mode for qualitative variables&lt;/h3&gt;
&lt;p&gt;Unlike the previous descriptive statistics (i.e., min, max, mean, median, first and third quartile) that can only be computed for quantitative variables, the mode can be computed for quantitative &lt;strong&gt;and&lt;/strong&gt; qualitative variables (see a recap of the different &lt;a href=&#34;/blog/variable-types-and-examples/&#34;&gt;types of variables&lt;/a&gt; if you do not remember the difference). Given the eye color of the 9 adults presented above:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt;, &lt;em&gt;blue&lt;/em&gt;, &lt;em&gt;blue&lt;/em&gt;, &lt;em&gt;blue&lt;/em&gt;, &lt;em&gt;brown&lt;/em&gt; and &lt;em&gt;green&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mode is brown, so most adults of this sample have brown eyes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dispersion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Dispersion&lt;/h1&gt;
&lt;p&gt;All previous descriptive statistics helps to get a sense of the location and position of the data. We now present the most common dispersion measures, which help to get a sense of the dispersion and the variability of the data (to which extent a distribution is squeezed or stretched):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;range&lt;/li&gt;
&lt;li&gt;standard deviation&lt;/li&gt;
&lt;li&gt;variance&lt;/li&gt;
&lt;li&gt;interquartile range&lt;/li&gt;
&lt;li&gt;coefficient of variation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As for location measures, we detail and compute by hand each of these statistics one by one.&lt;/p&gt;
&lt;div id=&#34;range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Range&lt;/h2&gt;
&lt;p&gt;The range is the difference between the maximum and the minimum value:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[range = max - min\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given the height (in cm) of our sample of 6 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;188.7&lt;/em&gt;, &lt;em&gt;169.4&lt;/em&gt;, &lt;em&gt;178.6&lt;/em&gt;, &lt;em&gt;181.3&lt;/em&gt;, &lt;em&gt;179&lt;/em&gt; and &lt;em&gt;173.9&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The range is 188.7 &lt;span class=&#34;math inline&#34;&gt;\(-\)&lt;/span&gt; 169.4 &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; 19.3 cm. The advantage of the range is that it is extremely easy to compute it and it gives a precise idea on what are the possible values in the data. The disadvantage is that it relies on the two most extreme values only.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-deviation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard deviation&lt;/h2&gt;
&lt;p&gt;The standard deviation is the most common dispersion measure in statistics. Like the mean for the location measures, if we have to present one statistics which summarizes the spread of the data, it is usually the standard deviation. As its name suggests, the standard deviation tells what is the “normal” deviation of the data. It actually computes the average deviation from the &lt;strong&gt;mean&lt;/strong&gt;. The larger the standard deviation, the more scattered the data are. On the contrary, the smaller the standard deviation, the more the data are centred around the mean. Below a visual representation of the standard deviation:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/standard-deviation.png&#34; alt=&#34;Standard deviation. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Standard deviation. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The standard deviation is a bit more complex than the previous statistics in the sense that there are two formulas depending on whether we face a sample or a population. A population includes all members from a specified group, all possible outcomes or measurements that are of interest. A sample consists of some observations drawn from the population, so a part or a subset of the population. For instance, the population may be “&lt;strong&gt;all&lt;/strong&gt; people living in Belgium” and the sample may be “&lt;strong&gt;some&lt;/strong&gt; people living in Belgium”. Read this article on &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;the difference between population and sample&lt;/a&gt; if you want to learn more.&lt;/p&gt;
&lt;div id=&#34;standard-deviation-for-a-population&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standard deviation for a population&lt;/h3&gt;
&lt;p&gt;The standard deviation for a population, denoted &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma = \sqrt{\frac{1}{n}\sum^n_{i = 1}(x_i - \mu)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As you can see from the formula, the standard deviation is actually the average deviation of the data from their mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Note the square for the difference between the observations and the mean to avoid that negative differences are compensated by positive differences.&lt;/p&gt;
&lt;p&gt;For the sake of easiness, imagine a population of only 3 adults (the steps are the same with a large population, the computation is just longer). Below their heights (in cm):&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mean is 172.6 (rounded to 1 decimal). The standard deviation is thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma = \sqrt{\frac{1}{3} \big[(160.4 - 172.6)^2 + (175.8 - 172.6)^2 + (181.5 - 172.6)^2 \big]}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sigma = 8.91\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the standard deviation for the heights of these 3 adults is 8.91 cm. This means that, on average, the height of the adults in this population deviates from the mean by 8.91 cm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-deviation-for-a-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standard deviation for a sample&lt;/h3&gt;
&lt;p&gt;The standard deviation for a sample is similar to the standard deviation for a population except that we divide by &lt;span class=&#34;math inline&#34;&gt;\(n -1\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and it is denoted &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{1}{n-1}\sum^n_{i = 1}(x_i - \bar{x})^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Imagine now that the 3 adults presented in the previous section is a sample instead of a population:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mean is still 172.6 (rounded to 1 decimal) since the mean is the same whether it is a population or a sample. The standard deviation is now:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{1}{3 - 1} \big[(160.4 - 172.6)^2 + (175.8 - 172.6)^2 + (181.5 - 172.6)^2 \big]}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[s = 10.92\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the standard deviation for the heights of these 3 adults is 10.92 cm. The interpretation is the same than for a population.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance&lt;/h2&gt;
&lt;p&gt;The variance is simply the square of the standard deviation. Put it another way, the standard deviation is the square root of the variance. We also distinguish between the variance for a population and for a sample.&lt;/p&gt;
&lt;div id=&#34;variance-for-a-population&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance for a population&lt;/h3&gt;
&lt;p&gt;The variance for a population, denoted &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;, is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2 = \frac{1}{n}\sum^n_{i = 1}(x_i - \mu)^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the formula for variance is the same than for standard deviation, except that the square root is removed for the variance. Remember the heights of our population of 3 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The standard deviation was 8.91 cm, so the variance of the height of these adults is &lt;span class=&#34;math inline&#34;&gt;\(8.91^2 = 79.39\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(cm^2\)&lt;/span&gt; (see below why the unit of a variance is &lt;span class=&#34;math inline&#34;&gt;\(unit^2\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-for-a-sample&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance for a sample&lt;/h3&gt;
&lt;p&gt;Again, the variance for a sample is similar to the variance for a population except that we divide by &lt;span class=&#34;math inline&#34;&gt;\(n -1\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and it is denoted &lt;span class=&#34;math inline&#34;&gt;\(s^2\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s^2 = \frac{1}{n-1}\sum^n_{i = 1}(x_i - \bar{x})^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Imagine again that the 3 adults in the previous section is a sample instead of a population:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The standard deviation for this sample was 10.92 cm, so the variance of the height of these adults is 119.14 &lt;span class=&#34;math inline&#34;&gt;\(cm^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-deviation-vs.variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard deviation vs. variance&lt;/h2&gt;
&lt;p&gt;Standard deviation and variance are often used interchangeably and both quantify the spread of a given dataset by measuring how far the observations are from their mean. However, the standard deviation can be more easily interpreted because the unit for the standard deviation is the same than the unit of measurement of the data (while it is the &lt;span class=&#34;math inline&#34;&gt;\(unit^2\)&lt;/span&gt; for the variance). Following our example of adult heights in cm, the standard deviation is measured in cm while the variance is measured in &lt;span class=&#34;math inline&#34;&gt;\(cm^2\)&lt;/span&gt;. The fact that the standard deviation keeps the same unit than the initial unit of measurement makes it more interpretable and thus often more used in practice.&lt;/p&gt;
&lt;div id=&#34;notations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notations&lt;/h3&gt;
&lt;p&gt;For completeness, below a table showing the different notations for variance and standard deviation in case of population and sample:&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Population&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Sample&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Standard deviation&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(s^2\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interquartile-range&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interquartile range&lt;/h2&gt;
&lt;p&gt;Remember the first &lt;span class=&#34;math inline&#34;&gt;\(q_{0.25}\)&lt;/span&gt; and third quartile &lt;span class=&#34;math inline&#34;&gt;\(q_{0.75}\)&lt;/span&gt; presented earlier. The interquartile range is another measure of dispersion of the data, using the quartiles. It is the difference between the third and first quartile:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IQR = q_{0.75} - q_{0.25}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Considering the height of the 9 adults presented in the section about the first and third quartile:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;170.2&lt;/em&gt;, &lt;em&gt;181.5&lt;/em&gt;, &lt;em&gt;188.9&lt;/em&gt;, &lt;em&gt;163.9&lt;/em&gt;, &lt;em&gt;166.4&lt;/em&gt;, &lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The first quartile was 163.9 cm and the third quartile was 181.5 cm. The IQR is thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[IQR = 181.5 - 163.9 = 17.6\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the interquartile range is 17.6 cm. The interquartile range is actually the range (since it is the difference between a higher and a lower value) of the middle data. The graph below may help to understand better the IQR and the quartiles:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/descriptive-statistics-by-hand_files/IQR-quartiles.png&#34; alt=&#34;IQR, first and third quartile. Source: LFSAB1105 at UCLouvain&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;IQR, first and third quartile. Source: LFSAB1105 at UCLouvain&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-variation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficient of variation&lt;/h2&gt;
&lt;p&gt;The last dispersion measure is the coefficient of variation. The coefficient of variation, denoted &lt;span class=&#34;math inline&#34;&gt;\(CV\)&lt;/span&gt;, is the standard deviation divided by the mean. Formally:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[CV = \frac{s}{\bar{x}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consider the height of a sample of 4 adults:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;163.7&lt;/em&gt;, &lt;em&gt;160.4&lt;/em&gt;, &lt;em&gt;175.8&lt;/em&gt; and &lt;em&gt;181.5&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mean is &lt;span class=&#34;math inline&#34;&gt;\(\bar{x} =\)&lt;/span&gt; 170.35 cm and the standard deviation is &lt;span class=&#34;math inline&#34;&gt;\(s =\)&lt;/span&gt; 9.95 cm. (Find the same values as an exercise!) The coefficient of variation is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[CV = \frac{9.95 \text{ cm}}{170.35 \text{ cm}} = 0.058 = 5.8\%\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In conclusion, the coefficient of variation is 5.8%. Note that, as a rule of thumb, a coefficient of variation greater than 15% usually means that the data are heterogeneous while a coefficient of variation equal to or less than 15% means that the data are homogeneous. Given that the coefficient of variation equals 5.8% in this case, we can conclude that these 4 adults are homogeneous in terms of height.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-variation-vs.standard-deviation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficient of variation vs. standard deviation&lt;/h2&gt;
&lt;p&gt;Although the coefficient of variation is rather unknown to the public, it is, in fact, worth presenting when making descriptive statistics.&lt;/p&gt;
&lt;p&gt;The standard deviation should always be understood in the context of the mean of the data and is dependent on its unit. The standard deviation has the advantage that it tells by how far on average the data is from the mean in terms of unit in which the data has been measured. Standard deviation is useful when considering variables with same units and approximately same means. However, standard deviation becomes less useful when comparing variables with different units or widely different means. For instance, a variable with a standard deviation of 10 cm cannot be compared to a variable with a standard deviation of 10€ to conclude which one is the most dispersed.&lt;/p&gt;
&lt;p&gt;The coefficient of variation is a ratio of two statistics with the same units. It has thus no unit and is independent of the unit in which the data has been measured. Being unit-free, coefficients of variation computed on datasets or variables with different units or widely different means can be compared to conclude, in fine, which data or variables is more (or less) dispersed. For instance, consider a sample of 10 women with their heights in cm and their salaries in €. The coefficients of variation are 0.032 and 0.061 respectively for the height and the salary. We can conclude that, relative to their respective average, their salaries vary more than their heights for these women (since the coefficient of variation is larger for the salary compared to the coefficient variation for the height).&lt;/p&gt;
&lt;p&gt;This concludes a relatively long article, thanks for reading! I hope the article helped you to understand and compute the different descriptive statistics by hand. If you would like to learn how to compute these measures in R, read the article “&lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;Descriptive statistics in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What is the difference between population and sample?</title>
      <link>/blog/what-is-the-difference-between-population-and-sample/</link>
      <pubDate>Sat, 18 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/what-is-the-difference-between-population-and-sample/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/what-is-the-difference-between-population-and-sample_files/difference-between-population-and-sample.jpeg&#34; alt=&#34;Photo by Roland Denes&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Photo by Roland Denes&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;People often fail to properly distinguish between population and sample. It is however essential in any statistical analysis, starting from &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;descriptive statistics&lt;/a&gt; with different formulas for variance and standard deviation depending on whether we face a sample or a population. Moreover, the branch of statistics called &lt;a href=&#34;/blog/a-shiny-app-for-inferential-statistics-by-hand/&#34;&gt;inferential statistics&lt;/a&gt; is often defined as the science of drawing conclusions about a population from observations made on a representative sample of that population. It is therefore crucial to properly distinguish between the two concepts. So, what exactly is the difference between population and sample?&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/what-is-the-difference-between-population-and-sample_files/population-sample.png&#34; alt=&#34;Population versus sample. Source: towardsdatascience.com&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Population versus sample. Source: towardsdatascience.com&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A population includes &lt;strong&gt;all members&lt;/strong&gt; from a specified group, all possible outcomes or measurements that are of interest. The exact population will depend on the scope of the study. For example, say you would like to know whether there is an association between job performance and the amount of home working hours per week in the specific case of Belgian data scientists. In this case, the population might be Belgian data scientists. However, if the scope of the study is more narrow (e.g., the study focuses on french-speaking Belgian data scientists who live at least 30km away from their workplace), then the population will be more specific and include only workers who meet the criteria. The point is that the population should only include people to whom the results will apply.&lt;/p&gt;
&lt;p&gt;A sample consists of some observations drawn from the population, so a part or a &lt;strong&gt;subset of the population&lt;/strong&gt;. The sample is the group of elements who actually participated in the study.&lt;/p&gt;
&lt;p&gt;Members and elements are defined in the broad sense of the term. It may be human. For instance, the population may be “&lt;strong&gt;all&lt;/strong&gt; people living in Belgium” and the sample may be “&lt;strong&gt;some&lt;/strong&gt; people living in Belgium”. It can be anything else too. Say you are testing the effect of a new fertilizer on crop yield. All the crop fields represent your population, whereas the 10 crop fields you tested correspond to your sample. Since a sample is a subset of a population, a sample is always smaller than the population.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Note that, a population must not necessarily be large. It might be the case that you study such a narrow population (e.g., first-year male bachelor students from your university who passed the statistics exam in June and for whom their parents have been divorced for more than 5 years), that the size of the population is actually rather small.&lt;/p&gt;
&lt;p&gt;As mentioned at the beginning of this article, one of the main concern in statistics is being able to draw conclusions about a population from a representative sample. Why using a sample of the population and not directly the population? In general it is almost always impossible to carry out measurements for the entire study population because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the population is too large. Example: the population of pregnant women. If we want to take measurements on all pregnant women in the world, it will most likely either take too long or cost too much&lt;/li&gt;
&lt;li&gt;the population is virtual. In this case “virtual” population is understood as a “hypothetical” population: it is unlimited in size. Example: for an experimental study, we focus on men with prostate cancer treated with a new treatment. We do not know how many people will be treated, so the population varies, is infinite and uncountable at the present time, and therefore virtual&lt;/li&gt;
&lt;li&gt;the population is not easily reachable. Example: the population of homeless persons in Belgium&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For these reasons, measurements are made on a subgroup of observations from the population, i.e., on a sample of our population. These measures are then used to draw conclusions about the population of interest. With an appropriate methodology, the results obtained on a sample are often almost as accurate as those that would be obtained on the entire population.&lt;/p&gt;
&lt;p&gt;Of course, the sample must be selected to be representative of the population under study. If participants are included in a study on a voluntary basis, there is a serious concern that the resulting sample may not be representative of the population. It may be the case that volunteers are different in terms of the parameter&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; of interest, leading to a selection bias. Another selection bias can occur when, for instance, a researcher collects citizens’ wage, by the means of internet. It might be the case that people having access to internet have different wages than people who do not have access.&lt;/p&gt;
&lt;p&gt;The gold standard to select a sample representative of the population under study is by selecting a &lt;strong&gt;random&lt;/strong&gt; sample. A random sample is a sample selected at random from the population so that each member of the population has an equal chance of being selected. A random sample is usually an unbiased sample, that is, a sample whose randomness is not in doubt.&lt;/p&gt;
&lt;p&gt;In some situations (e.g., in medicine) it is complicated or even impossible to obtain a random sample of the population. In such cases, it will be important to consider how representative the resulting sample will be. Last but not least, a paired sample is a sample in which groups (often pairs) of experimental units are linked together by the same experimental conditions. For example, one may measure the hours of sleep for 20 individuals before taking a sleeping pill, and then repeat the measurements on the same individuals after they have taken a sleeping pill. The two measurements for each individual (hours of sleep before and after the sleeping pill) are of course related. Statistical tools accounting for a relation between the samples exist and should be preferred in that case.&lt;/p&gt;
&lt;p&gt;To summarize, the sample is the group of individuals who participated in the study and the population is the broader group to whom the results will apply. Measurements on the entire population is too complex or impossible, so representative samples are used to draw conclusions about the population. Samples based on a random selection are often the most representative samples.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope the article helped you to understand the difference between population and sample.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;There can be, however, many different samples from the same population. All samples combined together can therefore be larger than the population. This is beyond the scope of this article, and at the moment we assume there is only one sample from a specified population.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The tools used to describe a population are called parameters, whereas the tools used to describe a sample are referred as statistics. See the &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;most common statistics for a sample&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Shiny app for inferential statistics by hand</title>
      <link>/blog/a-shiny-app-for-inferential-statistics-by-hand/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/a-shiny-app-for-inferential-statistics-by-hand/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/a-shiny-app-for-inferential-statistics_files/Screenshot%202020-02-04%20at%2011.36.38.png&#34; alt=&#34;A Shiny app for inferential statistics: hypothesis tests and confidence intervals&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;A Shiny app for inferential statistics: hypothesis tests and confidence intervals&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Statistics is divided into four main branches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Descriptive statistics&lt;/li&gt;
&lt;li&gt;Inferential statistics&lt;/li&gt;
&lt;li&gt;Predictive analysis&lt;/li&gt;
&lt;li&gt;Exploratory analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Descriptive statistics provide a summary of the data; it helps explaining the data in a concise way without losing too much information. Data can be summarized numerically or graphically. See &lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;descriptive statistics by hand&lt;/a&gt; or &lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;in R&lt;/a&gt; to learn more about this branch of statistics.&lt;/p&gt;
&lt;p&gt;The branch of predictive analysis aims at predicting a dependent variable based on one or several independent variables. Depending on the type of data to be predicted, it often encompasses methods such as regression or classification.&lt;/p&gt;
&lt;p&gt;Exploratory analyses focus on using graphical approaches to delve into the data and identify the relationships that exist between the different variables in the dataset. They are therefore more akin to data visualization.&lt;/p&gt;
&lt;p&gt;Inferential statistics uses a random sample of data taken from a population to make inferences, i.e., to draw conclusions about the population (see the &lt;a href=&#34;/blog/what-is-the-difference-between-population-and-sample/&#34;&gt;difference between population and sample&lt;/a&gt;). In other words, information from the sample is used to make generalizations about the parameter of interest in the population. The two major tools in inferential statistics are confidence intervals and hypothesis tests. Here is a Shiny app which helps you to use these two tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-201/&#34;&gt;Statistics-201&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This Shiny app focuses on confidence intervals and hypothesis tests for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 and 2 means (with unpaired and paired samples)&lt;/li&gt;
&lt;li&gt;1 and 2 proportions&lt;/li&gt;
&lt;li&gt;1 and 2 variances&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Note that the link may not work if the app has hit the monthly usage limit. Try again later if that is the case.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;how-to-use-this-app&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to use this app?&lt;/h1&gt;
&lt;p&gt;Follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the app via this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-201/&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Choose the parameter(s) you want to do inference for (i.e., mean(s), proportion(s) or variance(s))&lt;/li&gt;
&lt;li&gt;Write your data in Sample. Observations are separated by a comma and the decimal is a point&lt;/li&gt;
&lt;li&gt;Set the null and alternative hypothesis&lt;/li&gt;
&lt;li&gt;Select the significance level (most of the time &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the results panel (on the right side or below depending on the size of your screen), you will see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a recap of your sample together with some appropriate descriptive statistics&lt;/li&gt;
&lt;li&gt;the confidence interval&lt;/li&gt;
&lt;li&gt;the hypothesis test&lt;/li&gt;
&lt;li&gt;the interpretation&lt;/li&gt;
&lt;li&gt;and an illustration of the hypothesis test&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All formulas, steps and computations to arrive at the final results are also provided.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope you will find this app useful to do inferential statistics and in particular confidence interval and hypothesis testing by hand.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fshiny%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Shiny app for simple linear regression by hand and in R</title>
      <link>/blog/a-shiny-app-for-simple-linear-regression-by-hand-and-in-r/</link>
      <pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/a-shiny-app-for-simple-linear-regression-by-hand-and-in-r/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/a-shiny-app-for-simple-linear-regression_files/Screenshot%202020-02-04%20at%2011.45.09.png&#34; alt=&#34;A Shiny app to perform simple linear regression (by hand and in R)&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;A Shiny app to perform simple linear regression (by hand and in R)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Simple linear regression is a statistical method to summarize and study relationships between two variables. When more than two variables are of interest, it is referred as multiple linear regression.&lt;/p&gt;
&lt;p&gt;In this article, we focus only on a Shiny app which allows to perform simple linear regression by hand and in R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-202/&#34;&gt;Statistics-202&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For further details about what is linear regression and when it is used, please see the numerous resources on the topic available in textbooks and online.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that the link may not work if the app has hit the monthly usage limit. Try again later if that is the case.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;how-to-use-this-app&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to use this app?&lt;/h1&gt;
&lt;p&gt;Follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the app via this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-202/&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Enter your data in the x and y fields. The x field corresponds to the independent variable, while the y field corresponds to the dependent variable&lt;/li&gt;
&lt;li&gt;If you do not want to display the confidence interval around the regression line, uncheck the checkbox under Plot&lt;/li&gt;
&lt;li&gt;Change the x and y-axis labels for the regression plot if needed&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the results panel (on the right side or below depending on the size of your screen), you will see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a recap of your dataset together with some appropriate descriptive statistics&lt;/li&gt;
&lt;li&gt;the estimates &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and the regression model computed by hand&lt;/li&gt;
&lt;li&gt;the results of the model computed in R&lt;/li&gt;
&lt;li&gt;the regression plot with some key measures&lt;/li&gt;
&lt;li&gt;and the interpretations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All formulas, steps and computations to arrive at the final results are also provided. Note that it is your responsibility to check the validity of your linear model. This app only serves you to compute the results of the linear model given the data but it does not check whether the assumptions are met. Last but not least, you can download a report of the results by clicking on the Download button. You can choose the format of the report (i.e., HTML, PDF or Word) and whether you want to include the R code or not.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope you will find this app useful to do simple linear regression by hand and in R.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fshiny%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A guide on how to read statistical tables</title>
      <link>/blog/a-guide-on-how-to-read-statistical-tables/</link>
      <pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/a-guide-on-how-to-read-statistical-tables/</guid>
      <description>


&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/a-guide-on-how-to-read-statistics-table_files/Probability-distributions-statsandr.com.png&#34; alt=&#34;Shiny app to compute probabilities for the main probability distributions&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Shiny app to compute probabilities for the main probability distributions&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Below a Shiny app to help you read the main statistical tables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-101/&#34;&gt;Statistics-101&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This Shiny app helps you to compute probabilities for the main probability distributions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note that the link may not work if the app has hit the monthly usage limit. Try again later if that is the case.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;how-to-use-this-app&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to use this app?&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the app via this &lt;a href=&#34;https://antoinesoetewey.shinyapps.io/statistics-101/&#34;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Choose the distribution&lt;/li&gt;
&lt;li&gt;Set the parameter(s) of the distribution (the parameters depend of course on the chosen distribution)&lt;/li&gt;
&lt;li&gt;Select whether you want to find the lower tail, upper tail or an interval&lt;/li&gt;
&lt;li&gt;Choose the value of x&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the right panel (or below depending on the size of your screen) you will see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a recap of the data you just entered&lt;/li&gt;
&lt;li&gt;the numerical solution (i.e., the probability)&lt;/li&gt;
&lt;li&gt;a visualization of the solution&lt;/li&gt;
&lt;li&gt;the probability density function together with the mean, the standard deviation and the variance&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;Here is an example with the most common distribution: the &lt;strong&gt;normal distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Imagine the following problem: The cost of weekly maintenance and repair of a business has been observed over a long period of time and turns out to be distributed according to a normal distribution with an average of 402€ and a standard deviation of 22€. Having set a budget of 439€ for next week, what is the probability that the cost exceeds this budget?&lt;/p&gt;
&lt;p&gt;To solve this problem, follow these steps in the app:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose the normal distribution, as it is said that the costs follow a normal distribution&lt;/li&gt;
&lt;li&gt;Set the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; equal to 402, as it is said that the average cost is 402€&lt;/li&gt;
&lt;li&gt;In the statement, the standard deviation is given (and not the variance) so select “Standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;” and set it equal to 22&lt;/li&gt;
&lt;li&gt;We are asked what is the probability the the cost &lt;strong&gt;exceeds&lt;/strong&gt; the budget. Therefore, we look for the probability &lt;strong&gt;above&lt;/strong&gt; a certain x, so select upper tail &lt;span class=&#34;math inline&#34;&gt;\(P(X &amp;gt; x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;We are now asked to find the probability that the cost exceeds 439€, so set x equal to 439&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The solution panel gives a recap of the data:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X ∼ \mathcal{N}(\mu = 402, \sigma^2 = 484)\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(484 = 22^2\)&lt;/span&gt;, and the solution: &lt;span class=&#34;math display&#34;&gt;\[P(X &amp;gt; 439) = P(Z &amp;gt; 1.68) = 0.0463\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{X - \mu}{\sigma} = \frac{439 - 402}{22} = 1.68\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z ∼ \mathcal{N}(\mu = 0, \sigma^2 = 1)\)&lt;/span&gt; (known as the standard normal distribution). Thus, the probability that the cost next week exceeds the budget of 439€ is 0.0463, or 4.63%.&lt;/p&gt;
&lt;p&gt;It also shows the normal distribution (with &lt;span class=&#34;math inline&#34;&gt;\(\mu = 402\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = 484\)&lt;/span&gt;) with the shaded area corresponding to the probability we are looking for. It then gives some details about the density function, the mean, the standard deviation and the variance.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope you will find this app useful to compute probabilities for the main distributions.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fshiny%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Variable types and examples</title>
      <link>/blog/variable-types-and-examples/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/variable-types-and-examples/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#big-picture&#34;&gt;Big picture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantitative&#34;&gt;Quantitative&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#discrete&#34;&gt;Discrete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous&#34;&gt;Continuous&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qualitative&#34;&gt;Qualitative&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nominal&#34;&gt;Nominal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ordinal&#34;&gt;Ordinal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variable-transformations&#34;&gt;Variable transformations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#from-continuous-to-discrete&#34;&gt;From continuous to discrete&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#from-quantitative-to-qualitative&#34;&gt;From quantitative to qualitative&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-notes&#34;&gt;Additional notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This article presents the different variable types from a statistical point of view. To learn about the different data types in R, read “&lt;a href=&#34;/blog/data-types-in-r/&#34;&gt;Data types in R&lt;/a&gt;”.&lt;/p&gt;
&lt;div id=&#34;big-picture&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Big picture&lt;/h1&gt;
&lt;p&gt;In statistics, variables are classified into 4 different types:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph {\n\n\n\n\n  \&#34;1\&#34; [label = \&#34;Variable\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Variable\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;2\&#34; [label = \&#34;Qualitative\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Qualitative\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;3\&#34; [label = \&#34;Nominal\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Nominal\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;4\&#34; [label = \&#34;Ordinal\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Ordinal\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;5\&#34; [label = \&#34;Quantitative\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Quantitative\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;6\&#34; [label = \&#34;Discrete\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Discrete\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;7\&#34; [label = \&#34;Continuous\&#34;, style = \&#34;filled,rounded\&#34;, shape = \&#34;box\&#34;, fontname = \&#34;helvetica\&#34;, tooltip = \&#34;- name: Continuous\&#34;, fillcolor = \&#34;LightGray\&#34;, fontcolor = \&#34;#000000\&#34;] \n  \&#34;1\&#34;-&gt;\&#34;2\&#34; \n  \&#34;1\&#34;-&gt;\&#34;5\&#34; \n  \&#34;2\&#34;-&gt;\&#34;3\&#34; \n  \&#34;2\&#34;-&gt;\&#34;4\&#34; \n  \&#34;5\&#34;-&gt;\&#34;6\&#34; \n  \&#34;5\&#34;-&gt;\&#34;7\&#34; \n}&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;quantitative&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantitative&lt;/h1&gt;
&lt;p&gt;A quantitative variable is a variable that reflects a notion of magnitude, that is, if the values it can take are numbers. A quantitative variable represents thus a measure and is numerical.&lt;/p&gt;
&lt;p&gt;Quantitative variables are divided into two types: discrete and continuous.&lt;/p&gt;
&lt;div id=&#34;discrete&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Discrete&lt;/h2&gt;
&lt;p&gt;Quantitative discrete variables are variables for which the values it can take are &lt;strong&gt;countable&lt;/strong&gt; and have a &lt;strong&gt;finite number of possibilities&lt;/strong&gt;. The values are often (but not always) integers. Here are some examples of discrete variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of children per family&lt;/li&gt;
&lt;li&gt;Number of students in a class&lt;/li&gt;
&lt;li&gt;Number of citizens of a country&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even if it would take a long time to count the citizens of a large country, it is still doable. Moreover, for all examples, the number of possibilities is finite. Whatever the number of children in a family, it will never be 3.58 or 7.912 so the number of possibilities is a finite number and thus countable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous&lt;/h2&gt;
&lt;p&gt;On the other hand, continuous variables are variables for which the values are &lt;strong&gt;not countable&lt;/strong&gt; and have an &lt;strong&gt;infinite number of possibilities&lt;/strong&gt;. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Weight&lt;/li&gt;
&lt;li&gt;Height&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For simplicity, we usually referred to years, kilograms (or pounds) and centimeters (or feet and inches) for age, weight and height respectively. However, a 28-year-old man could actually be 28 years, 7 months, 16 days, 3 hours, 4 minutes, 5 seconds, 31 milliseconds, 9 nanoseconds old. For all measurements, we usually stop at a standard level of granularity, but nothing (except our measurement tools) prevents us from going deeper, leading to an infinite number of potential values. The fact that the values can take an infinite number of possibilities makes it uncountable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;qualitative&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Qualitative&lt;/h1&gt;
&lt;p&gt;In opposition to quantitative variables, qualitative variables (also referred as categorical variables or factors) are variables that are not numerical and which values fits into categories. In other words, a qualitative variable is a variable which takes as its values modalities, categories or even levels, in contrast to quantitative variables which measure a quantity on each individual.&lt;/p&gt;
&lt;p&gt;Qualitative variables are divided into two types: nominal and ordinal.&lt;/p&gt;
&lt;div id=&#34;nominal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nominal&lt;/h2&gt;
&lt;p&gt;A nominal variable is a qualitative variable where no ordering is possible or implied in the levels. For example, the variable gender is nominal because there is no order in the levels female/male. Eye color is another example of a nominal variable because there is no order among blue, brown or green eyes. A nominal variable can have between two levels (e.g., do you smoke? Yes/No or what is your gender? Female/Male) and a large number of levels (what is your college major? Each major is a level in that case).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ordinal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ordinal&lt;/h2&gt;
&lt;p&gt;On the other hand, an ordinal variable is a qualitative variable with an order implied in the levels. For instance, the variable “severity of road accidents” is ordinal because there is a clear order in the levels light/moderate/fatal.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variable transformations&lt;/h1&gt;
&lt;p&gt;There exists two main variable transformations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;From a continuous to a discrete variable&lt;/li&gt;
&lt;li&gt;From a quantitative to a qualitative variable&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;from-continuous-to-discrete&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From continuous to discrete&lt;/h2&gt;
&lt;p&gt;Let’s say we are interested in babies’ ages. The data collected is the age of the babies, so a continuous variable. However, we may work with only the number of weeks since birth and thus transforming the age into a discrete variable. The variable age remains a continuous variable but the variable we are working on (i.e., the number of weeks since birth) is a discrete variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;from-quantitative-to-qualitative&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;From quantitative to qualitative&lt;/h2&gt;
&lt;p&gt;Let’s say we are interested in the Body Mass Index (BMI). For this, a researcher collects data on height and weight of individuals and computes the BMI. The BMI is a continuous variable but the researcher may want to turn it into a qualitative variable by categorizing individuals below a certain threshold as underweighted, above a certain threshold as overweighted and the rest as normal weight. The BMI is a continuous variable but it has been transformed to another variable, which is now a qualitative (ordinal) variable.&lt;/p&gt;
&lt;p&gt;Same goes for age when age is transformed to an ordinal variable with levels such as minors, adults and seniors. It is also often the case (especially in surveys) that the variable salary (continuous) is transformed into an ordinal variable with different range of salaries (e.g., &amp;lt; 1000€, 1000 - 2000€, &amp;gt; 2000€).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-notes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Additional notes&lt;/h1&gt;
&lt;p&gt;The reason why we often class variables into different types is because not all statistical analyses can be performed on all variable types. For instance, it is impossible to compute the mean of the variable “hair color” as you cannot add brown and blond hair. On the other hand, finding the mode of a continuous variable does not really make any sense because most of the time there will not be two exact same values, so there will be no mode. And even in the case there is a mode, there will be very few observations with this value. As an example, try finding the mode of the height of the students in your class. If you are lucky, a couple of students will have the same size. However, most of the time, every student will have a different size (especially if the measurements include several decimals) and thus there will be no mode. To see what kind of analysis is possible on each type of variable, see “&lt;a href=&#34;/blog/descriptive-statistics-by-hand/&#34;&gt;Descriptive statistics by hand&lt;/a&gt;” or “&lt;a href=&#34;/blog/descriptive-statistics-in-r/&#34;&gt;Descriptive statistics in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;Last but not least, in datasets it is very often the case that numbers are assigned to qualitative variables. For instance, a study may assign the number “1” to women and the number “2” to men (or “0” to the answer “No” and “1” to the answer “Yes”). Despite the numerical classification, the variable gender is still a qualitative variable and not a discrete variable as it may look. The numerical classification is used only for data analysis. It is indeed easier to write “1” or “2” instead of “women” or “men”, and thus less prone to encoding errors. If you face this kind of setup, do not forget to transform your variable into the right type before performing any statistical analyses.&lt;/p&gt;
&lt;p&gt;Thanks for reading. I hope this article helped you to understand the different types of variable. If you would like to learn more about the different data types in R, read the article “&lt;a href=&#34;/blog/data-types-in-r/&#34;&gt;Data types in R&lt;/a&gt;”.&lt;/p&gt;
&lt;p&gt;As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by &lt;a href=&#34;https://github.com/AntoineSoetewey/statsandr/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raising an issue on GitHub&lt;/a&gt;. For all other requests, you can contact me &lt;a href=&#34;/contact/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Get updates every time a new article is published by &lt;a href=&#34;/subscribe/&#34;&gt;subscribing to this blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related articles:&lt;/strong&gt;&lt;/p&gt;
&lt;script src=&#34;//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fstatistics%2Findex.xml&amp;detail=-1&amp;limit=5&amp;showtitle=false&amp;type=js&#34;&gt;&lt;/script&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>