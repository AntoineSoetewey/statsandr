<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability on Stats and R</title>
    <link>/tags/probability/</link>
    <description>Recent content in Probability on Stats and R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Tue, 03 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/probability/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The 7 concepts and formulas in probability that every data scientist should know</title>
      <link>/blog/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/</link>
      <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-probability&#34;&gt;What is probability?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-probability-is-always-between-0-and-1&#34;&gt;1. A probability is always between 0 and 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compute-a-probability&#34;&gt;2. Compute a probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complement-of-an-event&#34;&gt;3. Complement of an event&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#union-of-two-events&#34;&gt;4. Union of two events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#intersection-of-two-events&#34;&gt;5. Intersection of two events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#independence-of-two-events&#34;&gt;6. Independence of two events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-probability&#34;&gt;7. Conditional probability&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bayes-theorem&#34;&gt;Bayes’ theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accuracy-measures&#34;&gt;8. Accuracy measures&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#false-negatives&#34;&gt;False negatives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#false-positives&#34;&gt;False positives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sensitivity&#34;&gt;Sensitivity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#specificity&#34;&gt;Specificity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#positive-predictive-value&#34;&gt;Positive predictive value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#negative-predictive-value&#34;&gt;Negative predictive value&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#counting-techniques&#34;&gt;9. Counting techniques&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;xxx add image&lt;/p&gt;
&lt;div id=&#34;what-is-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is probability?&lt;/h1&gt;
&lt;p&gt;Probability is the likelihood of an event occurring; it is a mathematical model to describe random phenomena. In other words, probability is a branch of mathematics that provides models to describe random processes. These mathematical tools allow to establish theoretical models for random phenomena and to use them to make predictions. Like every model, the probabilistic model is a simplification of the world. However, the model is useful as soon as it captures the essential features.&lt;/p&gt;
&lt;p&gt;In this article, we present xxx fundamental formulas and conceps in probability to get you started in data science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-probability-is-always-between-0-and-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. A probability is always between 0 and 1&lt;/h1&gt;
&lt;p&gt;The probability of an event is always between 0 and 1,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[0 \le P(A) \le 1\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If an event is impossible: &lt;em&gt;P(A) = 0&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;If an event is certain: &lt;em&gt;P(A) = 1&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, throwing a 7 with a standard dice (with faces from 1 to 6) is impossible so its probability is equal to 0. Throwing any number from 1 to 6 with a dice is certain, so its probability is equal to 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-a-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Compute a probability&lt;/h1&gt;
&lt;p&gt;If the elements of a sample space (the set of all possible results of a randomized experiment) are equiprobable (= all elements have the same probability), then the probability of an event occurring is equal to the number of favourable cases (number of ways it can happen) divided by the number of possible cases (total number of outcomes):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A) = \frac{\text{number of favourable cases}}{\text{number of possible cases}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For example, all numbers of a six-sided dice are equiprobable since they all have the same probability of occuring. The probability of rolling a 3 with a dice is thus&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(3) = \frac{\text{number of favourable cases}}{\text{number of possible cases}} = \frac{1}{6}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;because there is only one favourable case (there is only one face with a 3 on it), and there are 6 possible cases (because there are 6 faces altogether).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;complement-of-an-event&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Complement of an event&lt;/h1&gt;
&lt;p&gt;The probability of the complement (or opposite) of an event is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{not A}) = P(\bar{A}) = 1 - P(A)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;union-of-two-events&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Union of two events&lt;/h1&gt;
&lt;p&gt;The probability of the union of two events is the probability of either occurring:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{A or B)} = P(A \cup B) = P(A) + P(B) - P(A \cap B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Suppose that the probability of a fire breaking out in two houses in a given year is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in house A: 60%&lt;/li&gt;
&lt;li&gt;in house B: 45%&lt;/li&gt;
&lt;li&gt;in at least one of the two houses: 80%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Graphically we have&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The probability of a fire breaking out in house A &lt;strong&gt;or&lt;/strong&gt; house B is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cup B) = P(A) + P(B) - P(A \cap B) = 0.6 + 0.45 - 0.25 = 0.8\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By summing P(A) and P(B), the intersection of A and B is counted twice. This is the reason we subtract it to count it only once.&lt;/p&gt;
&lt;p&gt;If two events are mutually exclusive (i.e., two events that cannot occur simultaneously), the probability of both events occuring is equal to 0, so the above formula becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cup B) = P(A) + P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For example, rolling a 3 or a 6 on a six-sided dice are two mutually exclusive events since they cannot both occur at the same time. Since their joint probability is equal to 0, the probability of rolling a 3 or 6 on a six-sided dice is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(3 \cup 6) = P(3) + P(6) = \frac{1}{6} + \frac{1}{6} = \frac{1}{3}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intersection-of-two-events&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Intersection of two events&lt;/h1&gt;
&lt;p&gt;If two events are independent, the probability of the intersection of two events (i.e., the joint probability) is the probability of the two events occuring:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{A and B)} = P(A \cap B) = P(A) \cdot P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For instance, if two coins are flipped, the probability of both being tails is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]&lt;/span&gt;
Note that &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B) = P(B \cap A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If two events are mutually exclusive, their joint probability is equal to 0:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) = 0\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;independence-of-two-events&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;6. Independence of two events&lt;/h1&gt;
&lt;p&gt;The independence of two events can be verified thanks to the above formula. If the equality holds, the two events are said to be independent, otherwise the two events are said to be dependent. Formally, the events A and B are independent if and only if&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) = P(A) \cdot P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the example of the two coins:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1 \cap T_2) = \frac{1}{4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so the following equality holds&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{4}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two events are thus independent, denoted &lt;span class=&#34;math inline&#34;&gt;\(T_1{\perp\!\!\!\perp}T_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the example of the fire breaking out in two houses (see &lt;a href=&#34;/blog/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/#union-of-two-events&#34;&gt;section 4&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) = 0.25\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A) \cdot P(B) = 0.6 \cdot 0.45 = 0.27\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so the following equality does not hold&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A \cap B) \ne P(A) \cdot P(B)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two events are thus dependent (or not independent), denoted &lt;span class=&#34;math inline&#34;&gt;\(A \not\!\perp\!\!\!\perp B\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;7. Conditional probability&lt;/h1&gt;
&lt;p&gt;Suppose two events A and B and P(B) &amp;gt; 0. The conditional probability of A given (knowing) B is the likelihood of event A occurring given that event B has occured:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B \cap A)}{P(B)} \text{ (since } P(A \cap B) = P(B \cap A))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that, in general, the probability of A given B is not equal to the probability of B given A, that is, &lt;span class=&#34;math inline&#34;&gt;\(P(A | B) \ne P(B | A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From the formula of the conditional probability, we can derive the multiplicative law:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)} \text{ (Eq. 1)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) \cdot P(B) = \frac{P(A \cap B)}{P(B)} \cdot P(B)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) \cdot P(B) = P(A \cap B) \text{ (multiplicative law)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If two events are independent, &lt;span class=&#34;math inline&#34;&gt;\(P(A \cap B) = P(A) \cdot P(B)\)&lt;/span&gt;, and:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(B) &amp;gt; 0, the conditional probability becomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A) \cdot P(B)}{P(B)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = P(A) \text{ (Eq. 2)}\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(A) &amp;gt; 0, the conditional probability becomes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(B \cap A)}{P(A)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(B) \cdot P(A)}{P(A)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = P(B) \text{ (Eq. 3)}\]&lt;/span&gt;
Equations 2 and 3 mean that knowing that one event occured does not influence the probability of the outcome of the other event. This is in fact the definition of the independence: if knowing that one event occured does not help to predict (does not influence) the outcome of the other event, the two events are by essence independent.&lt;/p&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes’ theorem&lt;/h2&gt;
&lt;p&gt;From the formulas of the conditional probability and the multiplicative law, we can derive the Bayes’ theorem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(B \cap A)}{P(A)} \text{ (from the conditional probability)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(A \cap B)}{P(A)} \text{ (since } P(A \cap B) = P(B \cap A))\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \text{ (from the multiplicative law)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is equivalent to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)} \text{ (Bayes&amp;#39; theorem)}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;In order to illustrate the conditional probability and the Bayes’ theorem, suppose the following problem:&lt;/p&gt;
&lt;p&gt;In order to determine the presence of a disease in a person, a blood test is performed. When a person has the disease, the test can reveal the disease in 80% of cases. When the disease is not present, the test is negative in 90% of cases. Experience has shown that the probability of the disease being present is 10%. A researcher would like to know the probability that an individual has the disease given that the result of the test is positive.&lt;/p&gt;
&lt;p&gt;To answer this question, the following events are defined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P: the test result is positive&lt;/li&gt;
&lt;li&gt;D: the person has the disease&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, we use a tree diagram to illustrate the statement:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/Screenshot%202020-03-03%20at%2013.54.24.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(The sum of all 4 scenarios must be equal to 1 since these 4 scenarios include all possible cases.)&lt;/p&gt;
&lt;p&gt;We are looking for the probability that an individual has the disease given that the result of the test is positive, &lt;span class=&#34;math inline&#34;&gt;\(P(D | P)\)&lt;/span&gt;. Following the formula of the conditional probability (Eq. 1) we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In terms of our problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(D | P) = \frac{P(D \cap P)}{P(P)}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[P(D | P) = \frac{0.08}{P(P)} \text{ (Eq. 4)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the tree diagram, we can see that a positive test result is possible under two scenarios: (i) when a person has the disease, or (ii) when the person does not actually have the disease. In order to find the probability of a positive test result, &lt;span class=&#34;math inline&#34;&gt;\(P(P)\)&lt;/span&gt;, we need to sum up those two scenarios:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(P) = P(D \cap P) + P(\bar{D} \cap P) = 0.08+0.09=0.17\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Eq. 4 then becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(D | P) = \frac{0.08}{0.17} = 0.4706\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The probability of having the disease given that the result of the test is positive is only 47.06%. This means that in this specific case (with the same percentages), an individual has less than 1 chance out of 2 of having the disease knowing that his test is positive!&lt;/p&gt;
&lt;p&gt;This relatively small percentage is due to the facts that the disease is quite rare (only 10% of the population is affected) and that the test is not always correct (sometimes it detects the disease although it is not present, and sometimes it does not detect it although it is present). As a consequence, a higher percentage of healthy people have a positive result (9%) compared to the percentage of people who have a positive result and who actually have the disease (8%). This explains why several diagnostic tests are often performed before announcing the result of the test, especially for rare diseases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;accuracy-measures&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;8. Accuracy measures&lt;/h1&gt;
&lt;p&gt;Based on the example of the disease and the diagnostic test presented above, we explain the most common accuracy measures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;False negatives&lt;/li&gt;
&lt;li&gt;False positives&lt;/li&gt;
&lt;li&gt;Sensitivity&lt;/li&gt;
&lt;li&gt;Specificity&lt;/li&gt;
&lt;li&gt;Positive predictive value&lt;/li&gt;
&lt;li&gt;Negative predictive value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before diving into the details of these accurary measures, here is an overview ofhthe measures and the tree diagram with the labels for each of the 4 scenarios:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know.png&#34; alt=&#34;Adapted from Wikipedia&#34; style=&#34;width:100.0%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Adapted from Wikipedia&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/Screenshot%202020-03-03%20at%2015.53.19.png&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;false-negatives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;False negatives&lt;/h2&gt;
&lt;p&gt;The false negatives (FN) are the number of people incorrectly labeled as &lt;strong&gt;not&lt;/strong&gt; having the disease or the condition, when in reality it is present. It is like telling a women who is 7 months pregnant that she is not pregnant.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[FN = P(D \cap \bar{P}) = 0.02\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;false-positives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;False positives&lt;/h2&gt;
&lt;p&gt;The false positives (FP) are the number of people incorrectly labeled as having the disease or the condition, when in reality it is &lt;strong&gt;not&lt;/strong&gt; present. It is like telling a man he is pregnant.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[FP = P(\bar{D} \cap P) = 0.09\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sensitivity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sensitivity&lt;/h2&gt;
&lt;p&gt;The sensitivity of a test, also referred as the recall, measures the ability of a test to detect the condition when the condition is present (the percentage of sick people who are correctly identified as having the disease):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Sensitivity = \frac{TP}{TP + FN}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;TP&lt;/em&gt; is the true positives and &lt;em&gt;FN&lt;/em&gt; is the false negatives.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Sensitivity = \frac{TP}{TP + FN} = P(P|D) = 0.8\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specificity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specificity&lt;/h2&gt;
&lt;p&gt;The specificity of a test measures the ability of a test to correctly exclude the condition when the condition is absent (the percentage of healthy people who are correctly identified as not having the disease):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Specificity = \frac{TN}{TN + FP}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;em&gt;TN&lt;/em&gt; is the true negative and &lt;em&gt;FP&lt;/em&gt; is the false positives.&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Specificity = \frac{TN}{TN + FP} = P(\bar{P} | \bar{D}) = 0.9\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;positive-predictive-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Positive predictive value&lt;/h2&gt;
&lt;p&gt;The positive predictive value, also referred as the precision, is the proportion of positives that correspond to the presence of the condition, so the proportions of positive results that are true positive results:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[PPV = \frac{TP}{TP+FP}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[PPV = \frac{TP}{TP+FP} = P(D | P) = \frac{P(D \cap P)}{P(P)} = \frac{0.08}{0.08+0.09} = 0.4706\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;negative-predictive-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Negative predictive value&lt;/h2&gt;
&lt;p&gt;The negative predictive value is the proportion of negatives that correspond to the absence of the condition, so the proportions of negative results that are true negative results:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[NPV = \frac{TN}{TN + FN}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the tree diagram, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[NPV = \frac{TN}{TN + FN} = P(\bar{D} | \bar{P}) = \frac{P(\bar{D} \cap \bar{P})}{P(\bar{P})} = \frac{0.81}{0.81+0.02} = 0.9759\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;counting-techniques&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;9. Counting techniques&lt;/h1&gt;
&lt;p&gt;In order to use the formula in &lt;a href=&#34;/blog/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/#computing-a-probability&#34;&gt;section 2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>