---
title: "The complete guide to clustering analysis: k-means and hierarchical clustering by hand and in R"
author: Antoine Soetewey
date: "2020-02-13"
slug: clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r
categories: []
tags:
  - R
  - Statistics
meta_img: blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r-statsandr.com.jpeg
# description: Description for the page.
output:
  blogdown::html_page:
    toc: true
    toc_depth: 6
# draft: true
bibliography: bibliography.bib
---


<div id="TOC">
<ul>
<li><a href="#what-is-clustering-analysis">What is clustering analysis?</a><ul>
<li><a href="#application-1-computing-distances">Application 1: Computing distances</a><ul>
<li><a href="#solution">Solution</a></li>
</ul></li>
</ul></li>
<li><a href="#k-means-clustering"><em>k</em>-means clustering</a><ul>
<li><a href="#application-2-k-means-clustering">Application 2: <em>k</em>-means clustering</a><ul>
<li><a href="#data">Data</a></li>
<li><a href="#kmeans-with-2-groups"><code>kmeans()</code> with 2 groups</a></li>
<li><a href="#quality-of-a-k-means-partition">Quality of a <em>k</em>-means partition</a></li>
<li><a href="#nstart-for-several-initial-centers"><code>nstart</code> for several initial centers</a></li>
<li><a href="#kmeans-with-3-groups"><code>kmeans()</code> with 3 groups</a></li>
<li><a href="#manual-application-and-verification-in-r">Manual application and verification in R</a><ul>
<li><a href="#solution-by-hand">Solution by hand</a></li>
<li><a href="#solution-in-r">Solution in R</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#hierarchical-clustering">Hierarchical clustering</a><ul>
<li><a href="#application-3-hierarchical-clustering">Application 3: hierarchical clustering</a><ul>
<li><a href="#data-1">Data</a></li>
<li><a href="#solution-by-hand-1">Solution by hand</a><ul>
<li><a href="#single-linkage">Single linkage</a></li>
<li><a href="#complete-linkage">Complete linkage</a></li>
<li><a href="#average-linkage">Average linkage</a></li>
</ul></li>
<li><a href="#solution-in-r-1">Solution in R</a><ul>
<li><a href="#single-linkage-1">Single linkage</a></li>
<li><a href="#complete-linkage-1">Complete linkage</a></li>
<li><a href="#average-linkage-1">Average linkage</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#k-means-versus-hierarchical-clustering"><em>k</em>-means versus hierarchical clustering</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div class="figure">
<img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r-statsandr.com.jpeg" alt="Photo by Nikola Johnny Mirkovic" style="width:100.0%" />
<p class="caption">Photo by Nikola Johnny Mirkovic</p>
</div>
<div id="what-is-clustering-analysis" class="section level1">
<h1>What is clustering analysis?</h1>
<p>Clustering analysis is a form of exploratory data analysis in which observations are divided into different groups that share common characteristics.</p>
<p>The purpose of cluster analysis (also known as classification) is to construct groups (or classes or <em>clusters</em>) while ensuring the following property: <strong>within a group</strong> the observations must be as <strong>similar</strong> as possible, while observations belonging to <strong>different groups</strong> must be as <strong>different</strong> as possible.</p>
<p>There are two main types of classification:</p>
<ol style="list-style-type: decimal">
<li><em>k</em>-means clustering</li>
<li>Hierarchical clustering</li>
</ol>
<p>The first is generally used when the <strong>number of classes is fixed</strong> in advance, while the second is generally used for an <strong>unknown number of classes</strong> and helps to determine this optimal number. Both methods are illustrated below through applications by hand and in R. Note that for hierarchical clustering, only the <em>ascending</em> classification is presented in this article.</p>
<p>Clustering algorithms use the <strong>distance</strong> in order to separate observations into different groups. Therefore, before diving into the presentation of the two classification methods, a reminder exercise on how to compute distances between points is presented.</p>
<div id="application-1-computing-distances" class="section level2">
<h2>Application 1: Computing distances</h2>
<p>Let a data set containing the points <span class="math inline">\(\boldsymbol{a} = (0, 0)&#39;\)</span>, <span class="math inline">\(\boldsymbol{b} = (1, 0)&#39;\)</span> and <span class="math inline">\(\boldsymbol{c} = (5, 5)&#39;\)</span>. Compute the matrix of Euclidean distances between the points by hand and in R.</p>
<div id="solution" class="section level3">
<h3>Solution</h3>
<p>The points are as follows:</p>
<pre class="r"><code># We create the points in R
a &lt;- c(0, 0)
b &lt;- c(1, 0)
c &lt;- c(5, 5)

X &lt;- rbind(a, b, c) # a, b and c are combined per row
colnames(X) &lt;- c(&quot;x&quot;, &quot;y&quot;) # rename columns

X # display the points</code></pre>
<pre><code>##   x y
## a 0 0
## b 1 0
## c 5 5</code></pre>
<p>By the Pythagorean theorem, we will remember that the distance between 2 points <span class="math inline">\((x_a, y_a)\)</span> and <span class="math inline">\((x_b, y_b)\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span> is given by <span class="math inline">\(\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\)</span>. So for instance, for the distance between the points <span class="math inline">\(\boldsymbol{b} = (1, 0)&#39;\)</span> and <span class="math inline">\(\boldsymbol{c} = (5, 5)&#39;\)</span> presented in the statement above, we have:</p>
<p><span class="math display">\[\begin{equation}
\sqrt{(x_b - x_c)^2 + (y_b - y_c)^2} = \sqrt{(1-5)^2 + (0-5)^2} = 6.403124
\end{equation}\]</span></p>
<p>We can proceed similarly for all pairs of points to find the distance matrix by hand. In R, the <code>dist()</code> function allows you to find the distance of points in a matrix or dataframe in a very simple way:</p>
<pre class="r"><code># The distance is found using the dist() function:
distance &lt;- dist(X, method = &quot;euclidean&quot;)
distance # display the distance matrix</code></pre>
<pre><code>##          a        b
## b 1.000000         
## c 7.071068 6.403124</code></pre>
<p>Note that the argument <code>method = &quot;euclidean&quot;</code> is not mandatory because the Euclidean method is the default one.</p>
<p>The distance matrix resulting from the <code>dist()</code> function gives the distance between the different points. The Euclidean distance between the points <span class="math inline">\(\boldsymbol{b}\)</span> and <span class="math inline">\(\boldsymbol{c}\)</span> is 6.403124, which corresponds to what we found above via the Pythagorean formula.</p>
<p>Now that the distance has been presented, let’s see how to perform clustering analysis with the k-means algorithm.</p>
</div>
</div>
</div>
<div id="k-means-clustering" class="section level1">
<h1><em>k</em>-means clustering</h1>
<p>The first form of classification is the method called <em><em>k</em>-means clustering</em> or the mobile center algorithm. As a reminder, this method aims at partitioning <span class="math inline">\(n\)</span> observations into <span class="math inline">\(k\)</span> clusters in which each observation belongs to the cluster with the closest average, serving as a prototype of the cluster. It is presented below.</p>
<div id="application-2-k-means-clustering" class="section level2">
<h2>Application 2: <em>k</em>-means clustering</h2>
<div id="data" class="section level3">
<h3>Data</h3>
<p>For this exercise, the <code>Eurojobs.csv</code> database available <a href="/blog/data/Eurojobs.csv">here</a> is used.</p>
<p>This database contains the percentage of the population employed in different industries in 26 European countries in 1979. It contains 10 variables:</p>
<ul>
<li><code>Country</code> - the name of the country (identifier)</li>
<li><code>Agr</code> - % of workforce employed in agriculture</li>
<li><code>Min</code> - % in mining</li>
<li><code>Man</code> - % in manufacturing</li>
<li><code>PS</code> - % in power supplies industries</li>
<li><code>Con</code> - % in construction</li>
<li><code>SI</code> - % in service industries</li>
<li><code>Fin</code> - % in finance</li>
<li><code>SPS</code> - % in social and personal services</li>
<li><code>TC</code> - % in transportation and communications</li>
</ul>
<p>We first import the dataset. See <a href="/blog/how-to-import-an-excel-file-in-rstudio">how to import data into R</a> if you need a reminder.</p>
<pre class="r"><code># Import data
Eurojobs &lt;- read.csv(
  file = &quot;data/Eurojobs.csv&quot;,
  sep = &quot;,&quot;, dec = &quot;.&quot;, header = TRUE
)
head(Eurojobs) # head() is used to display only the first 6 observations</code></pre>
<pre><code>##      Country  Agr Min  Man  PS  Con   SI Fin  SPS  TC
## 1    Belgium  3.3 0.9 27.6 0.9  8.2 19.1 6.2 26.6 7.2
## 2    Denmark  9.2 0.1 21.8 0.6  8.3 14.6 6.5 32.2 7.1
## 3     France 10.8 0.8 27.5 0.9  8.9 16.8 6.0 22.6 5.7
## 4 W. Germany  6.7 1.3 35.8 0.9  7.3 14.4 5.0 22.3 6.1
## 5    Ireland 23.2 1.0 20.7 1.3  7.5 16.8 2.8 20.8 6.1
## 6      Italy 15.9 0.6 27.6 0.5 10.0 18.1 1.6 20.1 5.7</code></pre>
<p>Note that there is a numbering before the first variable <code>Country</code>. For more clarity, we will replace this numbering by the country. To do this, we add the argument <code>row.names = 1</code> in the import function <code>read.csv()</code> to specify that the first column corresponds to the row names:</p>
<pre class="r"><code>Eurojobs &lt;- read.csv(
  file = &quot;data/Eurojobs.csv&quot;,
  sep = &quot;,&quot;, dec = &quot;.&quot;, header = TRUE, row.names = 1
)
Eurojobs # displays dataset</code></pre>
<pre><code>##                 Agr Min  Man  PS  Con   SI  Fin  SPS  TC
## Belgium         3.3 0.9 27.6 0.9  8.2 19.1  6.2 26.6 7.2
## Denmark         9.2 0.1 21.8 0.6  8.3 14.6  6.5 32.2 7.1
## France         10.8 0.8 27.5 0.9  8.9 16.8  6.0 22.6 5.7
## W. Germany      6.7 1.3 35.8 0.9  7.3 14.4  5.0 22.3 6.1
## Ireland        23.2 1.0 20.7 1.3  7.5 16.8  2.8 20.8 6.1
## Italy          15.9 0.6 27.6 0.5 10.0 18.1  1.6 20.1 5.7
## Luxembourg      7.7 3.1 30.8 0.8  9.2 18.5  4.6 19.2 6.2
## Netherlands     6.3 0.1 22.5 1.0  9.9 18.0  6.8 28.5 6.8
## United Kingdom  2.7 1.4 30.2 1.4  6.9 16.9  5.7 28.3 6.4
## Austria        12.7 1.1 30.2 1.4  9.0 16.8  4.9 16.8 7.0
## Finland        13.0 0.4 25.9 1.3  7.4 14.7  5.5 24.3 7.6
## Greece         41.4 0.6 17.6 0.6  8.1 11.5  2.4 11.0 6.7
## Norway          9.0 0.5 22.4 0.8  8.6 16.9  4.7 27.6 9.4
## Portugal       27.8 0.3 24.5 0.6  8.4 13.3  2.7 16.7 5.7
## Spain          22.9 0.8 28.5 0.7 11.5  9.7  8.5 11.8 5.5
## Sweden          6.1 0.4 25.9 0.8  7.2 14.4  6.0 32.4 6.8
## Switzerland     7.7 0.2 37.8 0.8  9.5 17.5  5.3 15.4 5.7
## Turkey         66.8 0.7  7.9 0.1  2.8  5.2  1.1 11.9 3.2
## Bulgaria       23.6 1.9 32.3 0.6  7.9  8.0  0.7 18.2 6.7
## Czechoslovakia 16.5 2.9 35.5 1.2  8.7  9.2  0.9 17.9 7.0
## E. Germany      4.2 2.9 41.2 1.3  7.6 11.2  1.2 22.1 8.4
## Hungary        21.7 3.1 29.6 1.9  8.2  9.4  0.9 17.2 8.0
## Poland         31.1 2.5 25.7 0.9  8.4  7.5  0.9 16.1 6.9
## Rumania        34.7 2.1 30.1 0.6  8.7  5.9  1.3 11.7 5.0
## USSR           23.7 1.4 25.8 0.6  9.2  6.1  0.5 23.6 9.3
## Yugoslavia     48.7 1.5 16.8 1.1  4.9  6.4 11.3  5.3 4.0</code></pre>
<pre class="r"><code>dim(Eurojobs) # displays the number of rows and columns</code></pre>
<pre><code>## [1] 26  9</code></pre>
<p>We now have a “clean” dataset of 26 observations and 9 variables on which we can base the classification. Note that in this case it is not necessary to standardize the data because they are all expressed in the same unit (in percentage). If this was not the case, we would have had to standardize the data via the <code>scale()</code> function (do not forget it otherwise your results may be completely different!). The so-called <em>k</em>-means clustering is done via the <code>kmeans()</code> function. We apply the classification with 2 classes and then 3 classes as examples.</p>
</div>
<div id="kmeans-with-2-groups" class="section level3">
<h3><code>kmeans()</code> with 2 groups</h3>
<pre class="r"><code>model &lt;- kmeans(Eurojobs, centers = 2)

# displays the class determined by
# the model for all observations:
print(model$cluster)</code></pre>
<pre><code>##        Belgium        Denmark         France     W. Germany        Ireland 
##              1              1              1              1              2 
##          Italy     Luxembourg    Netherlands United Kingdom        Austria 
##              1              1              1              1              1 
##        Finland         Greece         Norway       Portugal          Spain 
##              1              2              1              2              2 
##         Sweden    Switzerland         Turkey       Bulgaria Czechoslovakia 
##              1              1              2              2              1 
##     E. Germany        Hungary         Poland        Rumania           USSR 
##              1              2              2              2              2 
##     Yugoslavia 
##              2</code></pre>
<p>Note that the argument <code>centers = 2</code> is used to set the number of clusters, determined in advance. In this exercise the number of clusters has been determined arbitrarily. This number of clusters should be determined according to the context and goal of your analysis. Calling <code>print(model$cluster)</code> or <code>model$cluster</code> is the same. This output specifies the group (i.e., 1 or 2) to which each country belongs to.</p>
</div>
<div id="quality-of-a-k-means-partition" class="section level3">
<h3>Quality of a <em>k</em>-means partition</h3>
<p>The quality of a <em>k</em>-means partition is found by calculating the percentage of the <em>TSS</em> “explained” by the partition using the following formula:</p>
<p><span class="math display">\[\begin{equation}
\dfrac{\operatorname{BSS}}{\operatorname{TSS}} \times 100\%
\end{equation}\]</span></p>
<p>where <em>BSS</em> and <em>TSS</em> stands for <em>Between Sum of Squares</em> and <em>Total Sum of Squares</em>, respectively. The higher the percentage, the better the score (and thus the quality) because it means that <em>BSS</em> is large and/or <em>WSS</em> is small.</p>
<p>Here is how you can check the quality of the partition in R:</p>
<pre class="r"><code># BSS and TSS are extracted from the model and stored
(BSS &lt;- model$betweenss)</code></pre>
<pre><code>## [1] 4823.535</code></pre>
<pre class="r"><code>(TSS &lt;- model$totss)</code></pre>
<pre><code>## [1] 9299.59</code></pre>
<pre class="r"><code># We calculate the quality of the partition
BSS / TSS * 100</code></pre>
<pre><code>## [1] 51.86826</code></pre>
<p>The quality of the partition is 51.87%. This value has no real interpretation in absolute terms except that a higher quality means a higher explained percentage. However, it is more insightful when it is compared to the quality of other partitions (with the same number of clusters!) in order to determine the best partition among the ones considered.</p>
</div>
<div id="nstart-for-several-initial-centers" class="section level3">
<h3><code>nstart</code> for several initial centers</h3>
<p>The <em>k</em>-means algorithm uses a random set of initial points to arrive at the final classification. Due to the fact that the initial centers are randomly chosen, the same command <code>kmeans(Eurojobs, centers = 2)</code> may give slightly different results every time it is run, and thus slight differences in the quality of the partitions. The <code>nstart</code> argument in the <code>kmeans()</code> function allows to run the algorithm several times with different initial centers, in order to obtain a potentially better partition:</p>
<pre class="r"><code>model2 &lt;- kmeans(Eurojobs, centers = 2, nstart = 10)
100 * model2$betweenss / model2$totss</code></pre>
<pre><code>## [1] 54.2503</code></pre>
<p>Depending on the initial random choices, this new partition will be better or not compared to the first one. In our example, the partition is better as the quality increased to 54.25%.</p>
</div>
<div id="kmeans-with-3-groups" class="section level3">
<h3><code>kmeans()</code> with 3 groups</h3>
<p>We now perform the <em>k</em>-means classification with 3 clusters and compute its quality:</p>
<pre class="r"><code>model3 &lt;- kmeans(Eurojobs, centers = 3)
BSS3 &lt;- model3$betweenss
TSS3 &lt;- model3$totss
BSS3 / TSS3 * 100</code></pre>
<pre><code>## [1] 74.59455</code></pre>
<p>It can be seen that the classification into three groups allows for a higher explained percentage and a higher quality. This will always be the case: with more classes, the partition will be finer, and the <em>BSS</em> contribution will be higher. On the other hand, the “model” will be more complex, requiring more classes. In the extreme case where <em>k = n</em> (each observation is a singleton class), we have <em>BSS = TSS</em>, but the partition has lost all interest.</p>
<p>Now that the <em>k</em>-means clustering has been detailed in R, see how to do the algorithm by hand in the following sections.</p>
</div>
<div id="manual-application-and-verification-in-r" class="section level3">
<h3>Manual application and verification in R</h3>
<p>Perform <strong>by hand</strong> the <em>k</em>-means algorithm for the points shown in the graph below, with <em>k</em> = 2 and with the points <em>i</em> = 5 and <em>i</em> = 6 as initial centers. Compute the quality of the partition you just found and then <strong>check</strong> your answers <strong>in R</strong>.</p>
<p><em>Assume that the variables have the same units so there is no need to scale the data.</em></p>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div id="solution-by-hand" class="section level4">
<h4>Solution by hand</h4>
<p>Step 1. Here are the coordinates of the 6 points:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
point
</th>
<th class="tg-cly1">
x
</th>
<th class="tg-cly1">
y
</th>
</tr>
<tr>
<td class="tg-cly1">
1
</td>
<td class="tg-cly1">
7
</td>
<td class="tg-cly1">
3
</td>
</tr>
<tr>
<td class="tg-cly1">
2
</td>
<td class="tg-cly1">
4
</td>
<td class="tg-cly1">
5
</td>
</tr>
<tr>
<td class="tg-0lax">
3
</td>
<td class="tg-0lax">
2
</td>
<td class="tg-0lax">
4
</td>
</tr>
<tr>
<td class="tg-0lax">
4
</td>
<td class="tg-0lax">
0
</td>
<td class="tg-0lax">
1
</td>
</tr>
<tr>
<td class="tg-0lax">
5
</td>
<td class="tg-0lax">
9
</td>
<td class="tg-0lax">
7
</td>
</tr>
<tr>
<td class="tg-0lax">
6
</td>
<td class="tg-0lax">
6
</td>
<td class="tg-0lax">
8
</td>
</tr>
</table>
</center>
<p><br></p>
<p>And the initial centers:</p>
<ul>
<li>Group 1: point 5 with center <em>(9, 7)</em></li>
<li>Group 2: point 6 with center <em>(6, 8)</em></li>
</ul>
<p>Step 2. Compute the distance matrix point by point with the Pythagorean theorem. Remind that the distance between point <em>a</em> and point <em>b</em> is found with:</p>
<p><span class="math display">\[\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\]</span></p>
<p>We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to two decimals):</p>
<pre class="r"><code>round(dist(X), 2)</code></pre>
<pre><code>##       1     2     3     4     5
## 2  3.61                        
## 3  5.10  2.24                  
## 4  7.28  5.66  3.61            
## 5  4.47  5.39  7.62 10.82      
## 6  5.10  3.61  5.66  9.22  3.16</code></pre>
<p>Step 3. Based on the distance matrix computed in step 2, we can put each point to its closest group and compute the coordinates of the center.</p>
<p>We first put each point in its closest group:</p>
<ul>
<li>point 1 is closer to point 5 than to point 6 because the distance between points 1 and 5 is 4.47 while the distance between points 1 and 6 is 5.10</li>
<li>point 2 is closer to point 6 than to point 5 because the distance between points 2 and 5 is 5.39 while the distance between points 2 and 6 is 3.61</li>
<li>point 3 is closer to point 6 than to point 5 because the distance between points 3 and 5 is 7.62 while the distance between points 3 and 6 is 5.66</li>
<li>point 4 is closer to point 6 than to point 5 because the distance between points 4 and 5 is 10.82 while the distance between points 4 and 6 is 9.22</li>
</ul>
<p>Note that computing the distances between each point and the points 5 and 6 is sufficient. There is no need to compute the distance between the points 1 and 2 for example, as we compare each point to the initial centers (which are points 5 and 6).</p>
<p>We then compute the coordinates of the centers of the two groups by taking the mean of the coordinates <em>x</em> and <em>y</em>:</p>
<ul>
<li>Group 1 includes the points 5 and 1 with <em>(8, 5)</em> as center (<span class="math inline">\(8 = \frac{9+7}{2}\)</span> and <span class="math inline">\(5 = \frac{7+3}{2}\)</span>)</li>
<li>Group 2 includes the points 6, 2, 3 and 4 with <em>(3, 4.5)</em> as center (<span class="math inline">\(3 = \frac{6+4+2+0}{4}\)</span> and <span class="math inline">\(4.5 = \frac{8+5+4+1}{4}\)</span>)</li>
</ul>
<p>We thus have:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
points
</th>
<th class="tg-cly1">
center
</th>
</tr>
<tr>
<td class="tg-cly1">
cluster 1
</td>
<td class="tg-cly1">
5 &amp; 1
</td>
<td class="tg-cly1">
(8, 5)
</td>
</tr>
<tr>
<td class="tg-0lax">
cluster 2
</td>
<td class="tg-0lax">
6, 2, 3 &amp; 4
</td>
<td class="tg-0lax">
(3, 4.5)
</td>
</tr>
</table>
</center>
<p><br></p>
<p>Step 4. We make sure that the allocation is optimal by checking that each point is in the nearest cluster. The distance between a point and the center of a cluster is again computed thanks to the Pythagorean theorem. Thus, we have:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-scde{color:#009901;text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-yi9q{color:#009901;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
points
</th>
<th class="tg-cly1">
Distance to cluster 1
</th>
<th class="tg-cly1">
Distance to cluster 2
</th>
</tr>
<tr>
<td class="tg-cly1">
1
</td>
<td class="tg-scde">
2.24
</td>
<td class="tg-cly1">
4.27
</td>
</tr>
<tr>
<td class="tg-cly1">
2
</td>
<td class="tg-cly1">
4
</td>
<td class="tg-scde">
1.12
</td>
</tr>
<tr>
<td class="tg-0lax">
3
</td>
<td class="tg-0lax">
6.08
</td>
<td class="tg-yi9q">
1.12
</td>
</tr>
<tr>
<td class="tg-0lax">
4
</td>
<td class="tg-0lax">
8.94
</td>
<td class="tg-yi9q">
4.61
</td>
</tr>
<tr>
<td class="tg-0lax">
5
</td>
<td class="tg-yi9q">
2.24
</td>
<td class="tg-0lax">
6.5
</td>
</tr>
<tr>
<td class="tg-0lax">
6
</td>
<td class="tg-yi9q">
3.61
</td>
<td class="tg-0lax">
4.61
</td>
</tr>
</table>
</center>
<p><br></p>
<p>The minimum distance between the points and the two clusters is colored in green.</p>
<p>We check that each point is in the correct group (i.e., the closest cluster). According to the distance in the table above, point 6 seems to be closer to the cluster 1 than to the cluster 2. Therefore, the allocation is not optimal and point 6 should be reallocated to cluster 1.</p>
<p>Step 5. We compute again the centers of the clusters after this reallocation. The centers are found by taking the mean of the coordinates <em>x</em> and <em>y</em> of the points belonging to the cluster. We thus have:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
points
</th>
<th class="tg-cly1">
center
</th>
</tr>
<tr>
<td class="tg-cly1">
cluster 1
</td>
<td class="tg-cly1">
1, 5 &amp; 6
</td>
<td class="tg-cly1">
(7.33, 6)
</td>
</tr>
<tr>
<td class="tg-0lax">
cluster 2
</td>
<td class="tg-0lax">
2, 3 &amp; 4
</td>
<td class="tg-0lax">
(2, 3.33)
</td>
</tr>
</table>
</center>
<p><br></p>
<p>where, for instance, 3.33 is simply <span class="math inline">\(\frac{5+4+1}{3}\)</span>.</p>
<p>Step 6. Repeat step 4 until the allocation is optimal. If the allocation is optimal, the algorithm stops. In our example we have:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-scde{color:#009901;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
points
</th>
<th class="tg-cly1">
Distance to cluster 1
</th>
<th class="tg-cly1">
Distance to cluster 2
</th>
</tr>
<tr>
<td class="tg-cly1">
1
</td>
<td class="tg-scde">
3.02
</td>
<td class="tg-cly1">
5.01
</td>
</tr>
<tr>
<td class="tg-cly1">
2
</td>
<td class="tg-cly1">
3.48
</td>
<td class="tg-scde">
2.61
</td>
</tr>
<tr>
<td class="tg-cly1">
3
</td>
<td class="tg-cly1">
5.69
</td>
<td class="tg-scde">
0.67
</td>
</tr>
<tr>
<td class="tg-cly1">
4
</td>
<td class="tg-cly1">
8.87
</td>
<td class="tg-scde">
3.07
</td>
</tr>
<tr>
<td class="tg-cly1">
5
</td>
<td class="tg-scde">
1.95
</td>
<td class="tg-cly1">
7.9
</td>
</tr>
<tr>
<td class="tg-cly1">
6
</td>
<td class="tg-scde">
2.4
</td>
<td class="tg-cly1">
5.08
</td>
</tr>
</table>
</center>
<p><br></p>
<p>All points are correctly allocated to its nearest cluster, so the allocation is optimal and the algorithm stops.</p>
<p>Step 7. State the final partition and the centers. In our example:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
points
</th>
<th class="tg-cly1">
center
</th>
</tr>
<tr>
<td class="tg-cly1">
cluster 1
</td>
<td class="tg-cly1">
1, 5 &amp; 6
</td>
<td class="tg-cly1">
(7.33, 6)
</td>
</tr>
<tr>
<td class="tg-cly1">
cluster 2
</td>
<td class="tg-cly1">
2, 3 &amp; 4
</td>
<td class="tg-cly1">
(2, 3.33)
</td>
</tr>
</table>
</center>
<p><br></p>
<p>Now that we have the clusters and the final centers, we compute the quality of the partition we just found. Remember that we need to compute the BSS and TSS to find the quality. Below the steps to compute the quality of this partition by <em>k</em>-means, based on this summary table:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-0a7q{border-color:#000000;text-align:left;vertical-align:middle}
.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}
</style>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-0pky" colspan="3">
cluster 1
</th>
<th class="tg-0pky" colspan="3">
cluster 2
</th>
</tr>
<tr>
<td class="tg-0pky">
point
</td>
<td class="tg-0pky">
x
</td>
<td class="tg-0pky">
y
</td>
<td class="tg-0pky">
point
</td>
<td class="tg-0pky">
x
</td>
<td class="tg-0pky">
y
</td>
</tr>
<tr>
<td class="tg-0pky">
1
</td>
<td class="tg-0pky">
7
</td>
<td class="tg-0pky">
3
</td>
<td class="tg-0pky">
2
</td>
<td class="tg-0pky">
4
</td>
<td class="tg-0pky">
5
</td>
</tr>
<tr>
<td class="tg-0pky">
5
</td>
<td class="tg-0pky">
9
</td>
<td class="tg-0pky">
7
</td>
<td class="tg-0pky">
3
</td>
<td class="tg-0pky">
2
</td>
<td class="tg-0pky">
4
</td>
</tr>
<tr>
<td class="tg-0pky">
6
</td>
<td class="tg-0pky">
6
</td>
<td class="tg-0pky">
8
</td>
<td class="tg-0pky">
4
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
1
</td>
</tr>
<tr>
<td class="tg-0pky">
mean
</td>
<td class="tg-0pky">
7.33
</td>
<td class="tg-0pky">
6
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
2
</td>
<td class="tg-0pky">
3.33
</td>
</tr>
</table>
</center>
<p><br></p>
<p>Step 1. Compute the overall mean of the <em>x</em> and <em>y</em> coordinates:</p>
<p><span class="math display">\[\overline{\overline{x}} = \frac{7+4+2+0+9+6+3+5+4+1+7+8}{12} = 4.67\]</span></p>
<p>Step 2. Compute TSS and WSS:</p>
<p><span class="math display">\[TSS = (7-4.67)^2 + (4-4.67)^2 + (2-4.67)^2 + (0-4.67)^2 \\+ (9-4.67)^2 + (6-4.67)^2 + (3-4.67)^2 + (5-4.67)^2 \\ + (4-4.67)^2 + (1-4.67)^2 + (7-4.67)^2 + (8-4.67)^2 = 88.67\]</span></p>
<p>Regarding WSS, it is splitted between cluster 1 and cluster 2. For cluster 1:</p>
<p><span class="math display">\[WSS[1] = (7-7.33)^2 + (9 - 7.33)^2 + (6 - 7.33)^2 \\ + (3-6)^2 + (7-6)^2 + (8-6)^2 = 18.67\]</span></p>
<p>For cluster 2:</p>
<p><span class="math display">\[WSS[2] = (4-2)^2 + (2-2)^2 + (0-2)^2 \\ + (5-3.33)^2 + (4-3.33)^2 + (1-3.33)^2 = 16.67\]</span></p>
<p>And the total WSS is</p>
<p><span class="math display">\[WSS = WSS[1] + WSS[2] = 18.67 + 16.67 = 35.34\]</span></p>
<p>To find the BSS:</p>
<p><span class="math display">\[BSS = TSS - WSS = 88.67-35.34 = 53.33\]</span></p>
<p>Finally, the quality of the partition is:</p>
<p><span class="math display">\[Quality = \frac{BSS}{TSS} = \frac{53.33}{88.67} = 0.6014\]</span></p>
<p>So the quality of the partition is 60.14%.</p>
<p>We are now going to verify all these solutions (the partition, the final centers and the quality) in R.</p>
</div>
<div id="solution-in-r" class="section level4">
<h4>Solution in R</h4>
<p>As you can imagine, the solution in R us much shorter and requires much less computation on the user side. We first need to enter the data as a matrix or dataframe:</p>
<pre class="r"><code>X &lt;- matrix(c(7, 3, 4, 5, 2, 4, 0, 1, 9, 7, 6, 8),
  nrow = 6, byrow = TRUE
)
X # display the coordinates of the points</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    7    3
## [2,]    4    5
## [3,]    2    4
## [4,]    0    1
## [5,]    9    7
## [6,]    6    8</code></pre>
<p>We now perform the <em>k</em>-means via the <code>kmeans()</code> function with the point 5 and 6 as initial centers:</p>
<pre class="r"><code># take rows 5 and 6 of the X matrix as initial centers
res.k &lt;- kmeans(X, centers = X[c(5, 6), ],
                algorithm = &quot;Lloyd&quot;)</code></pre>
<p>Unlike in the previous application with the dataset <code>Eurojobs.csv</code> where the initial centers are randomly chosen by R, in this second application we want to specify which points are going to be the two initial centers. For this, we need to set <code>centers = X[c(5,6), ]</code> to indicate that that there are 2 centers, and that they are going to be the points 5 and 6 (see a reminder on <a href="/blog/data-manipulation-in-r">how to subset a dataframe</a> if needed).</p>
<p>The reason for adding the argument <code>algorithm = &quot;Lloyd&quot;</code> can be found in the usage of the R function <code>kmeans()</code>. In fact, there are several variants of the <em>k</em>-means algorithm. The default choice is the <span class="citation">Hartigan and Wong (1979)</span> version, which is more sophisticated than the basic version detailed in the solution by hand. By using the original version of <span class="citation">Lloyd (1982)</span>, we find the same solution in R and by hand. For more information, you can consult the documentation of the <code>kmeans()</code> function (via <code>?kmeans</code> or <code>help(kmeans)</code>) and read the articles mentioned.</p>
<p>The solution in R is then found by extracting</p>
<ul>
<li>the partition with <code>$cluster</code>:</li>
</ul>
<pre class="r"><code>res.k$cluster</code></pre>
<pre><code>## [1] 1 2 2 2 1 1</code></pre>
<p>Points 1, 5 and 6 belong to cluster 1, points 2, 3 and 4 belong to cluster 2.</p>
<ul>
<li>the coordinates of the final centers with <code>$centers</code>:</li>
</ul>
<pre class="r"><code># We extract the coordinates of the 2 final centers, rounded to 2 decimals
round(res.k$centers, digits = 2)</code></pre>
<pre><code>##   [,1] [,2]
## 1 7.33 6.00
## 2 2.00 3.33</code></pre>
<ul>
<li>and then the quality of the partition by dividing the BSS to the TSS:</li>
</ul>
<pre class="r"><code>res.k$betweenss / res.k$totss</code></pre>
<pre><code>## [1] 0.6015038</code></pre>
<p>The 3 results are equal to what we found by hand (except the quality which is slightly different due to rounding).</p>
</div>
</div>
</div>
</div>
<div id="hierarchical-clustering" class="section level1">
<h1>Hierarchical clustering</h1>
<p>Remind that the difference with the partition by <em>k</em>-means is that for hierarchical clustering, the number of classes is <strong>not</strong> specified in advance. Hierarchical clustering will help to determine the optimal number of clusters.</p>
<p>In this article, the 3 main types of hierarchical clustering are discussed:</p>
<ul>
<li>Single linkage (minimum distance)</li>
<li>Complete linkage (maximum distance)</li>
<li>Average linkage (average distance)</li>
</ul>
<p>In the following sections, we first perform these 3 algorithms by hand and then verify the results in R.</p>
<div id="application-3-hierarchical-clustering" class="section level2">
<h2>Application 3: hierarchical clustering</h2>
<div id="data-1" class="section level3">
<h3>Data</h3>
<p>Using the data from the graph and the table below, perform <strong>by hand</strong> the 3 algorithms (single, complete and average linkage) and draw the dendrograms. Then <strong>check</strong> your answers <strong>in R</strong>.</p>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre><code>##      V1    V2
## 1  2.03  0.06
## 2 -0.64 -0.10
## 3 -0.42 -0.53
## 4 -0.36  0.07
## 5  1.14  0.37</code></pre>
</div>
<div id="solution-by-hand-1" class="section level3">
<h3>Solution by hand</h3>
<p>Step 1. For all 3 algorithms, we first need to compute the distance matrix between the 5 points thanks to the Pythagorean theorem. Remind that the distance between point <em>a</em> and point <em>b</em> is found with:</p>
<p><span class="math display">\[\sqrt{(x_a - x_b)^2 + (y_a - y_b)^2}\]</span></p>
<p>We apply this theorem to each pair of points, to finally have the following distance matrix (rounded to three decimals):</p>
<pre><code>##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530</code></pre>
<div id="single-linkage" class="section level4">
<h4>Single linkage</h4>
<p>Step 2. From the distance matrix computed in step 1, we see that the <strong>smallest distance</strong> = 0.328 between points 2 and 4. 0.328 corresponds to the first height (more on this later when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp; 4, 3 and 5. The new distances between the group 2 &amp; 4 and all other points are now:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-0pky">
</th>
<th class="tg-0pky">
1
</th>
<th class="tg-0pky">
2 &amp; 4
</th>
<th class="tg-0pky">
3
</th>
<th class="tg-0pky">
5
</th>
</tr>
<tr>
<td class="tg-0pky">
1
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
2 &amp; 4
</td>
<td class="tg-0pky">
2.390
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
3
</td>
<td class="tg-0pky">
2.520
</td>
<td class="tg-0pky">
0.483
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
5
</td>
<td class="tg-0pky">
0.942
</td>
<td class="tg-0pky">
1.530
</td>
<td class="tg-0pky">
1.801
</td>
<td class="tg-0pky">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<p>To construct this new distance matrix, proceed point by point:</p>
<ul>
<li>the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the initial distance matrix (found in step 1), which was 2.520</li>
<li>same goes for the distance between points 1 and 5 and points 3 and 5; the distances are the same than in the initial distance matrix since the points have not changed</li>
<li>the distance between points 1 and 2 &amp; 4 has changed since points 2 &amp; 4 are now together</li>
<li>since we are applying the <strong>single linkage</strong> criterion, the new distance between points 1 and 2 &amp; 4 corresponds to the <strong>minimum distance</strong> between the distance between points 1 and 2 and the distance between points 1 and 4</li>
<li>the initial distance between points 1 and 2 is 2.675 and the initial distance between points 1 and 4 is 2.390</li>
<li>therefore, the minimum distance between these two distances is 2.390</li>
<li>2.390 is thus the new distance between points 1 and 2 &amp; 4</li>
<li>we apply the same process for points 3 and 2 &amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The minimum distance between these 2 distances is 0.483 so the new distance between points 3 and 2 &amp; 4 is 0.483</li>
<li>follow the same process for all other points</li>
</ul>
<p>Step 3. Based on the distance matrix in step 2, the smallest distance is 0.483 between points 3 and 2 &amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp; 3 &amp; 4. The groups are thus: 1, 2 &amp; 3 &amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
1
</th>
<th class="tg-cly1">
2 &amp; 3 &amp; 4
</th>
<th class="tg-cly1">
5
</th>
</tr>
<tr>
<td class="tg-cly1">
1
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
2 &amp; 3 &amp; 4
</td>
<td class="tg-cly1">
2.390
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
5
</td>
<td class="tg-cly1">
0.942
</td>
<td class="tg-cly1">
1.530
</td>
<td class="tg-cly1">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<ul>
<li>points 1 and 5 have not change, so the distance between these two points are the same than in previous step</li>
<li>from step 2 we see that the distance between points 1 and 2 &amp; 4 is 2.390 and the distance between points 1 and 3 is 2.520</li>
<li>since we apply the single linkage criterion, we take the minimum distance, which is 2.390</li>
<li>the distance between points 1 and 2 &amp; 3 &amp; 4 is thus 2.390</li>
<li>same process for points 5 and 2 &amp; 3 &amp; 4</li>
</ul>
<p>Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp; 5. The groups are thus: 1 &amp; 5 and 2 &amp; 3 &amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
1 &amp; 5
</th>
<th class="tg-cly1">
2 &amp; 3 &amp; 4
</th>
</tr>
<tr>
<td class="tg-cly1">
1 &amp; 5
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
2 &amp; 3 &amp; 4
</td>
<td class="tg-cly1">
1.530
</td>
<td class="tg-cly1">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<ul>
<li>the only distance left to compute is the distance between points 1 &amp; 5 and 2 &amp; 3 &amp; 4</li>
<li>from the previous step we see that the distance between points 1 and 2 &amp; 3 &amp; 4 is 2.390 and the distance between points 5 and 2 &amp; 3 &amp; 4 is 1.530</li>
<li>since we apply the single linkage criterion, we take the minimum distance, which is 1.530</li>
<li>the distance between points 1 &amp; 5 and 2 &amp; 3 &amp; 4 is thus 1.530</li>
</ul>
<p>Step 5. The final combination of points is the combination of points 1 &amp; 5 and 2 &amp; 3 &amp; 4, with a final height of 1.530. Heights are used to draw the dendrogram in the sixth and final step.</p>
<p>Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:</p>
<ul>
<li>the first combination of points was between points 2 and 4, with a height of 0.328</li>
<li>the second combination was between points 3 and 2 &amp; 4 with a height of 0.483</li>
<li>the third combination was between points 1 and 5 with a height of 0.942</li>
<li>the final combination was between points 1 &amp; 5 and 2 &amp; 3 &amp; 4 with a height of 1.530</li>
<li>this is exactly what is illustrated in the following dendrogram:</li>
</ul>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="complete-linkage" class="section level4">
<h4>Complete linkage</h4>
<p>Complete linkage is quite similar to single linkage, except that instead of taking the smallest distance when computing the new distance between points that have been grouped, the <strong>maximum distance</strong> is taken.</p>
<p>The steps to perform the hierarchical clustering with the complete linkage (maximum) are detailed below.</p>
<p>Step 1. Step 1 is exactly the same than for single linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:</p>
<pre><code>##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530</code></pre>
<p>Step 2. From the distance matrix computed in step 1, we see that the <strong>smallest distance</strong> = 0.328 between points 2 and 4. It is important to note that even if we apply the complete linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp; 4, 3 and 5. The new distances between the group 2 &amp; 4 and all other points are now:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-0pky">
</th>
<th class="tg-0pky">
1
</th>
<th class="tg-0pky">
2 &amp; 4
</th>
<th class="tg-0pky">
3
</th>
<th class="tg-0pky">
5
</th>
</tr>
<tr>
<td class="tg-0pky">
1
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
2 &amp; 4
</td>
<td class="tg-0pky">
2.675
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
3
</td>
<td class="tg-0pky">
2.520
</td>
<td class="tg-0pky">
0.603
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
5
</td>
<td class="tg-0pky">
0.942
</td>
<td class="tg-0pky">
1.841
</td>
<td class="tg-0pky">
1.801
</td>
<td class="tg-0pky">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<p>To construct this new distance matrix, proceed point by point as we did for single linkage:</p>
<ul>
<li>the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the initial distance matrix (found in step 1), which was 2.520</li>
<li>same goes for the distance between points 1 and 5 and points 3 and 5; the distances are the same than in the initial distance matrix since the points have not changed</li>
<li>the distance between points 1 and 2 &amp; 4 has changed since points 2 &amp; 4 are now together</li>
<li>since we are applying the <strong>complete linkage</strong> criterion, the new distance between points 1 and 2 &amp; 4 corresponds to the <strong>maximum distance</strong> between the distance between points 1 and 2 and the distance between points 1 and 4</li>
<li>the initial distance between points 1 and 2 is 2.675 and the initial distance between points 1 and 4 is 2.390</li>
<li>therefore, the maximum distance between these two distances is 2.675</li>
<li>2.675 is thus the new distance between points 1 and 2 &amp; 4</li>
<li>we apply the same process for points 3 and 2 &amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The maximum distance between these 2 distances is 0.603 so the new distance between points 3 and 2 &amp; 4 is 0.603</li>
<li>follow the same process for all other points</li>
</ul>
<p>Step 3. Based on the distance matrix in step 2, the smallest distance is 0.603 between points 3 and 2 &amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp; 3 &amp; 4. The groups are thus: 1, 2 &amp; 3 &amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
1
</th>
<th class="tg-cly1">
2 &amp; 3 &amp; 4
</th>
<th class="tg-cly1">
5
</th>
</tr>
<tr>
<td class="tg-cly1">
1
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
2 &amp; 3 &amp; 4
</td>
<td class="tg-cly1">
2.675
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
5
</td>
<td class="tg-cly1">
0.942
</td>
<td class="tg-cly1">
1.841
</td>
<td class="tg-cly1">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<ul>
<li>points 1 and 5 have not change, so the distance between these two points are the same than in previous step</li>
<li>from step 2 we see that the distance between points 1 and 2 &amp; 4 is 2.675 and the distance between points 1 and 3 is 2.520</li>
<li>since we apply the complete linkage criterion, we take the maximum distance, which is 2.675</li>
<li>the distance between points 1 and 2 &amp; 3 &amp; 4 is thus 2.675</li>
<li>same process for points 5 and 2 &amp; 3 &amp; 4</li>
</ul>
<p>Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp; 5. The groups are thus: 1 &amp; 5 and 2 &amp; 3 &amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
1 &amp; 5
</th>
<th class="tg-cly1">
2 &amp; 3 &amp; 4
</th>
</tr>
<tr>
<td class="tg-cly1">
1 &amp; 5
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
2 &amp; 3 &amp; 4
</td>
<td class="tg-cly1">
2.675
</td>
<td class="tg-cly1">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<ul>
<li>the only distance left to compute is the distance between points 1 &amp; 5 and 2 &amp; 3 &amp; 4</li>
<li>from the previous step we see that the distance between points 1 and 2 &amp; 3 &amp; 4 is 2.675 and the distance between points 5 and 2 &amp; 3 &amp; 4 is 1.841</li>
<li>since we apply the complete linkage criterion, we take the maximum distance, which is 2.675</li>
<li>the distance between points 1 &amp; 5 and 2 &amp; 3 &amp; 4 is thus 2.675</li>
</ul>
<p>Step 5. The final combination of points is the combination of points 1 &amp; 5 and 2 &amp; 3 &amp; 4, with a final height of 2.675. Heights are used to draw the dendrogram in the sixth and final step.</p>
<p>Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:</p>
<ul>
<li>the first combination of points was between points 2 and 4, with a height of 0.328</li>
<li>the second combination was between points 3 and 2 &amp; 4 with a height of 0.603</li>
<li>the third combination was between points 1 and 5 with a height of 0.942</li>
<li>the final combination was between points 1 &amp; 5 and 2 &amp; 3 &amp; 4 with a height of 2.675</li>
<li>this is exactly what is illustrated in the following dendrogram:</li>
</ul>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="average-linkage" class="section level4">
<h4>Average linkage</h4>
<p>With the average linkage criterion, it is not the minimum nor the maximum distance that is taken when computing the new distance between points that have been grouped, but it is, as you guessed by now, the <strong>average distance</strong> between the points.</p>
<p>The steps to perform the hierarchical clustering with the average linkage are detailed below.</p>
<p>Step 1. Step 1 is exactly the same than for single and complete linkage, that is, we compute the distance matrix of the 5 points thanks to the Pythagorean theorem. This gives us the following distance matrix:</p>
<pre><code>##       1     2     3     4
## 2 2.675                  
## 3 2.520 0.483            
## 4 2.390 0.328 0.603      
## 5 0.942 1.841 1.801 1.530</code></pre>
<p>Step 2. From the distance matrix computed in step 1, we see that the <strong>smallest distance</strong> = 0.328 between points 2 and 4. It is important to note that even if we apply the average linkage, in the distance matrix the points are brought together based on the smallest distance. This is the case for all 3 algorithms. The difference between the 3 algorithms lies in how to compute the new distances between the new combination of points (the single linkage takes the minimum between the distances, the complete linkage takes the maximum distance and the average linkage takes the average distance). 0.328 corresponds to the first height (which will be used when drawing the dendrogram). Since points 2 and 4 are the closest to each other, these 2 points are put together to form a single group. The groups are thus: 1, 2 &amp; 4, 3 and 5. The new distances between the group 2 &amp; 4 and all other points are now:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<tr>
<th class="tg-0pky">
</th>
<th class="tg-0pky">
1
</th>
<th class="tg-0pky">
2 &amp; 4
</th>
<th class="tg-0pky">
3
</th>
<th class="tg-0pky">
5
</th>
</tr>
<tr>
<td class="tg-0pky">
1
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
2 &amp; 4
</td>
<td class="tg-0pky">
2.5325
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
3
</td>
<td class="tg-0pky">
2.520
</td>
<td class="tg-0pky">
0.543
</td>
<td class="tg-0pky">
0
</td>
<td class="tg-0pky">
</td>
</tr>
<tr>
<td class="tg-0pky">
5
</td>
<td class="tg-0pky">
0.942
</td>
<td class="tg-0pky">
1.6855
</td>
<td class="tg-0pky">
1.801
</td>
<td class="tg-0pky">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<p>To construct this new distance matrix, proceed point by point as we did for the two previous criteria:</p>
<ul>
<li>the distance between points 1 and 3 has not changed, so the distance is unchanged compared to the initial distance matrix (found in step 1), which was 2.520</li>
<li>same goes for the distance between points 1 and 5 and points 3 and 5; the distances are the same than in the initial distance matrix since the points have not changed</li>
<li>the distance between points 1 and 2 &amp; 4 has changed since points 2 &amp; 4 are now together</li>
<li>since we are applying the <strong>average linkage</strong> criterion, the new distance between points 1 and 2 &amp; 4 corresponds to the <strong>average distance</strong> between the distance between points 1 and 2 and the distance between points 1 and 4</li>
<li>the initial distance between points 1 and 2 is 2.675 and the initial distance between points 1 and 4 is 2.390</li>
<li>therefore, the average distance between these two distances is <span class="math inline">\(\frac{2.675 + 2.390}{2} = 2.5325\)</span></li>
<li>2.5325 is thus the new distance between points 1 and 2 &amp; 4</li>
<li>we apply the same process for points 3 and 2 &amp; 4: the initial distance between points 3 and 2 is 0.483 and the initial distance between points 3 and 4 is 0.603. The average distance between these 2 distances is 0.543 so the new distance between points 3 and 2 &amp; 4 is 0.543</li>
<li>follow the same process for all other points</li>
</ul>
<p>Step 3. Based on the distance matrix in step 2, the smallest distance is 0.543 between points 3 and 2 &amp; 4 (the second height for the dendrogram). Since points 3 and 2 &amp; 4 are the closest to each other, they are combined to form a new group, the group 2 &amp; 3 &amp; 4. The groups are thus: 1, 2 &amp; 3 &amp; 4 and 5. We construct the new distance matrix based on the same process detailed in step 2:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
1
</th>
<th class="tg-cly1">
2 &amp; 3 &amp; 4
</th>
<th class="tg-cly1">
5
</th>
</tr>
<tr>
<td class="tg-cly1">
1
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
2 &amp; 3 &amp; 4
</td>
<td class="tg-cly1">
2.528333
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
5
</td>
<td class="tg-cly1">
0.942
</td>
<td class="tg-cly1">
1.724
</td>
<td class="tg-cly1">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<ul>
<li>points 1 and 5 have not change, so the distance between these two points are the same than in previous step</li>
<li>from step 2 we see that the distance between points 1 and 2 &amp; 4 is 2.5325 and the distance between points 1 and 3 is 2.520</li>
<li>since we apply the average linkage criterion, we take the average distance</li>
<li>however, we have to take into the consideration that there are 2 points in the group 2 &amp; 4, while there is only one point in the group 3</li>
<li>the average distance for the distance between 1 and 2 &amp; 3 &amp; 4 is thus: <span class="math inline">\(\frac{(2 \cdot 2.5325) + (1 \cdot 2.520)}{3} = 2.528333\)</span></li>
<li>same process for points 5 and 2 &amp; 3 &amp; 4: <span class="math inline">\(\frac{(2 \cdot 1.6855) + (1 \cdot 1.801)}{3} = 1.724\)</span></li>
</ul>
<p>Step 4. Based on the distance matrix in step 3, the smallest distance is 0.942 between points 1 and 5 (the third height in the dendrogram). Since points 1 and 5 are the closest to each other, they are combined to form a new group, the group 1 &amp; 5. The groups are thus: 1 &amp; 5 and 2 &amp; 3 &amp; 4. We construct the new distance matrix based on the same process detailed in steps 2 and 3:</p>
<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
</style>
<table class="tg">
<tr>
<th class="tg-cly1">
</th>
<th class="tg-cly1">
1 &amp; 5
</th>
<th class="tg-cly1">
2 &amp; 3 &amp; 4
</th>
</tr>
<tr>
<td class="tg-cly1">
1 &amp; 5
</td>
<td class="tg-cly1">
0
</td>
<td class="tg-cly1">
</td>
</tr>
<tr>
<td class="tg-cly1">
2 &amp; 3 &amp; 4
</td>
<td class="tg-cly1">
2.126167
</td>
<td class="tg-cly1">
0
</td>
</tr>
</table>
</center>
<p><br></p>
<ul>
<li>the only distance left to compute is the distance between points 1 &amp; 5 and 2 &amp; 3 &amp; 4</li>
<li>from the previous step we see that the distance between points 1 and 2 &amp; 3 &amp; 4 is 2.528333 and the distance between points 5 and 2 &amp; 3 &amp; 4 is 1.724</li>
<li>since we apply the average linkage criterion, we take the average distance, which is <span class="math inline">\(\frac{2.528333 + 1.724}{2} = 2.126167\)</span></li>
<li>the distance between points 1 &amp; 5 and 2 &amp; 3 &amp; 4 is thus 2.126167</li>
</ul>
<p>Step 5. The final combination of points is the combination of points 1 &amp; 5 and 2 &amp; 3 &amp; 4, with a final height of 2.126167. Heights are used to draw the dendrogram in the sixth and final step.</p>
<p>Step 6. Draw the dendrogram thanks to the combination of points and heights found above. Remember that:</p>
<ul>
<li>the first combination of points was between points 2 and 4, with a height of 0.328</li>
<li>the second combination was between points 3 and 2 &amp; 4 with a height of 0.543</li>
<li>the third combination was between points 1 and 5 with a height of 0.942</li>
<li>the final combination was between points 1 &amp; 5 and 2 &amp; 3 &amp; 4 with a height of 2.126167</li>
<li>this is exactly what is illustrated in the following dendrogram:</li>
</ul>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
</div>
<div id="solution-in-r-1" class="section level3">
<h3>Solution in R</h3>
<p>To perform the hierarchical clustering with any of the 3 criterion in R, we first need to enter the data (in this case as a matrix format, but it can also be entered as dataframes):</p>
<pre class="r"><code>X &lt;- matrix(c(2.03, 0.06, -0.64, -0.10, -0.42, -0.53, -0.36, 0.07, 1.14, 0.37),
  nrow = 5, byrow = TRUE
)</code></pre>
<div id="single-linkage-1" class="section level4">
<h4>Single linkage</h4>
<p>We can apply the hierarchical clustering with the single linkage criterion thanks to the <code>hclust()</code> function with the argument <code>method = &quot;single&quot;</code>:</p>
<pre class="r"><code># Hierarchical clustering: single linkage
hclust &lt;- hclust(dist(X), method = &quot;single&quot;)</code></pre>
<p>Note that the <code>hclust()</code> function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix <code>X</code> corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the <code>dist()</code> function.</p>
<p>We can now extract the heights and plot the dendrogram to check our results by hand found above:</p>
<pre class="r"><code>round(hclust$height, 3)</code></pre>
<pre><code>## [1] 0.328 0.483 0.942 1.530</code></pre>
<pre class="r"><code>plot(hclust)</code></pre>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.</p>
<p>Remember that hierarchical clustering is used to determine the optimal number of clusters. This optimal number of clusters can be determined thanks to the dendrogram. For this, we usually look at the largest difference of heights:</p>
<div class="figure">
<img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/dendrogram-single-linkage.png" alt="How to determine the number of clusters from a dendrogram? Take the largest difference of heights and count how many vertical lines you see" />
<p class="caption">How to determine the number of clusters from a dendrogram? Take the largest difference of heights and count how many vertical lines you see</p>
</div>
<p>The largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp; 3 &amp; 4 with the group 1 &amp; 5. To determine the optimal number of clusters, simply count how many vertical lines you see within this largest difference. In our case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the <code>rect.hclust()</code> function:</p>
<pre class="r"><code>plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &quot;blue&quot;
)</code></pre>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Finally, we could also determine the optimal number of cluster thanks to a barplot of the heights (stored in <code>$height</code> of the clustering output):</p>
<pre class="r"><code>barplot(hclust$height,
  names.arg = (nrow(X) - 1):1 # show the number of cluster below each bars
)</code></pre>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Again, look for the largest jump of heights. In our case, the largest jump is from 1 to 2 classes. Therefore, the optimal number of classes is 2.</p>
<p>Note that determining the number of clusters using the dendrogram or barplot is not a strict rule. You can also consider the <em>silhouette plot</em>, <em>elbow plot</em> or some numerical measures such as Dunn’s index, Hubert’s gamma, etc., which show the variation of the error with the number of clusters (<em>k</em>), and you choose the value of <em>k</em> where the error is smallest. However, these methods are outside the scope of this course and the method presented with the dendrogram is generally sufficient.</p>
</div>
<div id="complete-linkage-1" class="section level4">
<h4>Complete linkage</h4>
<p>We can apply the hierarchical clustering with the complete linkage criterion thanks to the <code>hclust()</code> function with the argument <code>method = &quot;complete&quot;</code>:</p>
<pre class="r"><code># Hierarchical clustering: complete linkage
hclust &lt;- hclust(dist(X), method = &quot;complete&quot;)</code></pre>
<p>Note that the <code>hclust()</code> function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix <code>X</code> corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the <code>dist()</code> function.</p>
<p>We can now extract the heights and plot the dendrogram to check our results by hand found above:</p>
<pre class="r"><code>round(hclust$height, 3)</code></pre>
<pre><code>## [1] 0.328 0.603 0.942 2.675</code></pre>
<pre class="r"><code>plot(hclust)</code></pre>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.</p>
<p>Similar to the single linkage, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp; 3 &amp; 4 with the group 1 &amp; 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the <code>rect.hclust()</code> function:</p>
<pre class="r"><code>plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &quot;blue&quot;
)</code></pre>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
<div id="average-linkage-1" class="section level4">
<h4>Average linkage</h4>
<p>We can apply the hierarchical clustering with the average linkage criterion thanks to the <code>hclust()</code> function with the argument <code>method = &quot;average&quot;</code>:</p>
<pre class="r"><code># Hierarchical clustering: average linkage
hclust &lt;- hclust(dist(X), method = &quot;average&quot;)</code></pre>
<p>Note that the <code>hclust()</code> function requires a distance matrix. If your data is not already a distance matrix (like in our case, as the matrix <code>X</code> corresponds to the coordinates of the 5 points), you can transform it into a distance matrix with the <code>dist()</code> function.</p>
<p>We can now extract the heights and plot the dendrogram to check our results by hand found above:</p>
<pre class="r"><code>round(hclust$height, 3)</code></pre>
<pre><code>## [1] 0.328 0.543 0.942 2.126</code></pre>
<pre class="r"><code>plot(hclust)</code></pre>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>As we can see from the dendrogram, the combination of points and the heights are the same than the ones obtained by hand.</p>
<p>Like the single and complete linkages, the largest difference of heights in the dendrogram occurs before the final combination, that is, before the combination of the group 2 &amp; 3 &amp; 4 with the group 1 &amp; 5. In this case, the optimal number of clusters is thus 2. In R, we can even highlight these two clusters directly in the dendrogram with the <code>rect.hclust()</code> function:</p>
<pre class="r"><code>plot(hclust)
rect.hclust(hclust,
  k = 2, # k is used to specify the number of clusters
  border = &quot;blue&quot;
)</code></pre>
<p><img src="/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
</div>
</div>
</div>
<div id="k-means-versus-hierarchical-clustering" class="section level1">
<h1><em>k</em>-means versus hierarchical clustering</h1>
<p>Choosing between <em>k</em>-means and hierarchical clustering is not always easy. If you have a good reason to think that there is a specific number of clusters in your dataset (for example if you would like to distinguish diseased and healthy patients depending on some characteristics but you do not know in which group patients belong to), you should probably opt for the <em>k</em>-means clustering as this technique is used when the number of groups is specified in advance. If you do not have any reason to believe there is a certain number of groups in your dataset (for instance in marketing when trying to distinguish clients without any prior belief on the number of different types of customers), then you should probably opt for the hierarchical clustering to determine in how many clusters your data should be divided.</p>
<p>In addition to this, if you are still undecided note that, on the one hand, with a large number of variables, <em>k</em>-means may be computationally faster than hierarchical clustering if the number of clusters is small. On the other hand, the result of a hierarchical clustering is a structure that is more informative and interpretable than the unstructured set of flat clusters returned by <em>k</em>-means. Therefore, it is easier to determine the optimal number of clusters by looking at the dendrogram of a hierarchical clustering than trying to predict this optimal number in advance in case of <em>k</em>-means.</p>
<p>Thanks for reading. I hope this article helped you understand the different clustering methods and how to compute them by hand and in R.</p>
<p>As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion. If you find a mistake or bug, you can inform me by <a href="https://github.com/AntoineSoetewey/statsandr/issues" target="_blank" rel="noopener">raising an issue on GitHub</a>. For all other requests, you can contact me <a href="/contact/">here</a>.</p>
<p>Get updates every time a new article is published by <a href="/subscribe/">subscribing to this blog</a>.</p>
<p><strong>Related articles:</strong></p>
<script src="//rss.bloople.net/?url=https%3A%2F%2Fwww.statsandr.com%2Ftags%2Fr%2Findex.xml&detail=-1&limit=5&showtitle=false&type=js"></script>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-jaaw28m">
<p>Hartigan, J. A., and M. A. Wong. 1979. “A K-Means Clustering Algorithm.” <em>Applied Statistics</em> 28: 100–108.</p>
</div>
<div id="ref-lloyd1982least">
<p>Lloyd, Stuart. 1982. “Least Squares Quantization in Pcm.” <em>IEEE Transactions on Information Theory</em> 28 (2). IEEE: 129–37.</p>
</div>
</div>
</div>
