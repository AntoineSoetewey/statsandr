---
title: The 9 concepts and formulas in probability that every data scientist should know
author: Antoine Soetewey
date: '2020-03-03'
slug: the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know
categories: []
tags:
  - Basics
  - Probability
  - R
  - Statistics
meta_img: blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know.jpeg
description: Learn the 9 most important formulas in probability that every data scientist should understand and master to appropriately handle any project in probability
output:
  blogdown::html_page:
    toc: true
    toc_depth: 6
# draft: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#what-is-probability">What is probability?</a></li>
<li><a href="#a-probability-is-always-between-0-and-1">1. A probability is always between 0 and 1</a></li>
<li><a href="#compute-a-probability">2. Compute a probability</a></li>
<li><a href="#complement-of-an-event">3. Complement of an event</a></li>
<li><a href="#union-of-two-events">4. Union of two events</a></li>
<li><a href="#intersection-of-two-events">5. Intersection of two events</a></li>
<li><a href="#independence-of-two-events">6. Independence of two events</a></li>
<li><a href="#conditional-probability">7. Conditional probability</a>
<ul>
<li><a href="#bayes-theorem">Bayes’ theorem</a></li>
<li><a href="#example">Example</a></li>
</ul></li>
<li><a href="#accuracy-measures">8. Accuracy measures</a>
<ul>
<li><a href="#false-negatives">False negatives</a></li>
<li><a href="#false-positives">False positives</a></li>
<li><a href="#sensitivity">Sensitivity</a></li>
<li><a href="#specificity">Specificity</a></li>
<li><a href="#positive-predictive-value">Positive predictive value</a></li>
<li><a href="#negative-predictive-value">Negative predictive value</a></li>
</ul></li>
<li><a href="#counting-techniques">9. Counting techniques</a>
<ul>
<li><a href="#multiplication">Multiplication</a>
<ul>
<li><a href="#example-1">Example</a></li>
</ul></li>
<li><a href="#permutation">Permutation</a>
<ul>
<li><a href="#example-2">Example</a>
<ul>
<li><a href="#by-hand">By hand</a></li>
<li><a href="#in-r">In R</a></li>
</ul></li>
</ul></li>
<li><a href="#combination">Combination</a>
<ul>
<li><a href="#example-3">Example</a>
<ul>
<li><a href="#by-hand-1">By hand</a></li>
<li><a href="#in-r-1">In R</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>

<p><img src="/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know.jpeg" style="width:100.0%" /></p>
<div id="what-is-probability" class="section level1">
<h1>What is probability?</h1>
<p>A probability is a number that reflects the <strong>chance that a particular event will occur</strong>. In other words, it quantifies (on a scale from 0 to 1, or from 0% to 100%) <strong>how likely an event is to occur</strong>.</p>
<p>Probability is a branch of mathematics that provides models to describe random processes. These mathematical tools allow to establish theoretical models for random phenomena and to use them to make predictions. Like every model, the probabilistic model is a simplification of the world. However, the model is useful as soon as it captures the essential features.</p>
<p>In this article, we present 9 fundamental formulas and concepts in probability that every data scientist should understand and master in order to appropriately handle any project in probability.</p>
</div>
<div id="a-probability-is-always-between-0-and-1" class="section level1">
<h1>1. A probability is always between 0 and 1</h1>
<p>The probability of an event is always between 0 and 1 (or 0% and 100%),</p>
<p><span class="math display">\[0 \le P(A) \le 1\]</span></p>
<ul>
<li>If an event is impossible: <span class="math inline">\(P(A) = 0\)</span></li>
<li>If an event is certain: <span class="math inline">\(P(A) = 1\)</span></li>
</ul>
<p>For example, throwing a 7 with a standard six-sided dice (with faces ranging from 1 to 6) is impossible so its probability is equal to 0. Throwing head or tail with a coin is certain, so its probability is equal to 1.</p>
</div>
<div id="compute-a-probability" class="section level1">
<h1>2. Compute a probability</h1>
<p>If the elements of a sample space (the set of all possible results of a randomized experiment) are equiprobable (= all elements have the same probability), then the probability of an event occurring is equal to the number of favourable cases (number of ways it can happen) divided by the number of possible cases (total number of outcomes):</p>
<p><span class="math display">\[P(A) = \frac{\text{number of favourable cases}}{\text{number of possible cases}}\]</span></p>
<p>For example, all numbers of a six-sided dice are equiprobable since they all have the same probability of occurring. The probability of rolling a 3 with a dice is thus</p>
<p><span class="math display">\[P(3) = \frac{\text{number of favourable cases}}{\text{number of possible cases}} = \frac{1}{6}\]</span></p>
<p>because there is only one favourable case (there is only one face with a 3 on it), and there are 6 possible cases (because there are 6 faces altogether).</p>
</div>
<div id="complement-of-an-event" class="section level1">
<h1>3. Complement of an event</h1>
<p>The probability of the complement (or opposite) of an event is:</p>
<p><span class="math display">\[P(\text{not A}) = P(\bar{A}) = 1 - P(A)\]</span></p>
<p>For instance, the probability of not throwing a 3 with a dice is:</p>
<p><span class="math display">\[P(\bar{A}) = 1 - P(A) = 1 - \frac{1}{6} = \frac{5}{6}\]</span></p>
</div>
<div id="union-of-two-events" class="section level1">
<h1>4. Union of two events</h1>
<p>The probability of the union of two events is the probability of either occurring:</p>
<p><span class="math display">\[P(\text{A or B)} = P(A \cup B) = P(A) + P(B) - P(A \cap B)\]</span></p>
<p>Suppose that the probability of a fire breaking out in two houses in a given year is:</p>
<ul>
<li>in house A: 60%, so <span class="math inline">\(P(A) = 0.6\)</span></li>
<li>in house B: 45%, so <span class="math inline">\(P(B) = 0.45\)</span></li>
<li>in at least one of the two houses: 80%, so <span class="math inline">\(P(A \cup B) = 0.8\)</span></li>
</ul>
<p>Graphically we have</p>
<p><img src="/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/figure-html/unnamed-chunk-1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The probability of a fire breaking out in house A <strong>or</strong> house B is</p>
<p><span class="math display">\[P(A \cup B) = P(A) + P(B) - P(A \cap B)\]</span>
<span class="math display">\[= 0.6 + 0.45 - 0.25 = 0.8\]</span></p>
<p>By summing <span class="math inline">\(P(A)\)</span> and <span class="math inline">\(P(B)\)</span>, the intersection of A and B, i.e. <span class="math inline">\(P(A \cap B)\)</span>, is counted twice. This is the reason we subtract it to count it only once.</p>
<p>If two events are mutually exclusive (i.e., two events that cannot occur simultaneously), the probability of both events occurring is equal to 0, so the above formula becomes</p>
<p><span class="math display">\[P(A \cup B) = P(A) + P(B)\]</span></p>
<p>For example, the event “rolling a 3” and the event “rolling a 6” on a six-sided dice are two mutually exclusive events since they cannot both occur at the same time. Since their joint probability is equal to 0, the probability of rolling a 3 or 6 on a six-sided dice is</p>
<p><span class="math display">\[P(3 \cup 6) = P(3) + P(6) = \frac{1}{6} + \frac{1}{6} = \frac{1}{3}\]</span></p>
</div>
<div id="intersection-of-two-events" class="section level1">
<h1>5. Intersection of two events</h1>
<p>If two events are independent, the probability of the intersection of the two events (i.e., the joint probability) is the probability of the two events occurring:</p>
<p><span class="math display">\[P(\text{A and B)} = P(A \cap B) = P(A) \cdot P(B)\]</span></p>
<p>For instance, if two coins are flipped, the probability of both coins being tails is</p>
<p><span class="math display">\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]</span>
Note that <span class="math inline">\(P(A \cap B) = P(B \cap A)\)</span>.</p>
<p>If two events are mutually exclusive, their joint probability is equal to 0:</p>
<p><span class="math display">\[P(A \cap B) = 0\]</span></p>
</div>
<div id="independence-of-two-events" class="section level1">
<h1>6. Independence of two events</h1>
<p>The independence of two events can be verified thanks to the above formula. If the equality holds, the two events are said to be independent, otherwise the two events are said to be dependent. Formally, the events A and B are independent if and only if</p>
<p><span class="math display">\[P(A \cap B) = P(A) \cdot P(B)\]</span></p>
<ul>
<li>In the example of the two coins:</li>
</ul>
<p><span class="math display">\[P(T_1 \cap T_2) = \frac{1}{4}\]</span></p>
<p>and</p>
<p><span class="math display">\[P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]</span></p>
<p>so the following equality holds</p>
<p><span class="math display">\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{4}\]</span></p>
<p>The two events are thus independent, denoted <span class="math inline">\(T_1{\perp\!\!\!\perp}T_2\)</span>.</p>
<ul>
<li>In the example of the fire breaking out in two houses (see <a href="/blog/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/#union-of-two-events">section 4</a>):</li>
</ul>
<p><span class="math display">\[P(A \cap B) = 0.25\]</span></p>
<p>and</p>
<p><span class="math display">\[P(A) \cdot P(B) = 0.6 \cdot 0.45 = 0.27\]</span></p>
<p>so the following equality does not hold</p>
<p><span class="math display">\[P(A \cap B) \ne P(A) \cdot P(B)\]</span></p>
<p>The two events are thus dependent (or not independent), denoted <span class="math inline">\(A \not\!\perp\!\!\!\perp B\)</span>.</p>
</div>
<div id="conditional-probability" class="section level1">
<h1>7. Conditional probability</h1>
<p>Suppose two events A and B and <span class="math inline">\(P(B) &gt; 0\)</span>. The conditional probability of A given (knowing) B is the likelihood of event A occurring given that event B has occurred:</p>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]</span>
<span class="math display">\[= \frac{P(B \cap A)}{P(B)} \text{ (since } P(A \cap B) = P(B \cap A))\]</span></p>
<p>Note that, in general, the probability of A given B is not equal to the probability of B given A, that is, <span class="math inline">\(P(A | B) \ne P(B | A)\)</span>.</p>
<p>From the formula of the conditional probability, we can derive the multiplicative law:</p>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)} \text{ (Eq. 1)}\]</span>
<span class="math display">\[P(A | B) \cdot P(B) = \frac{P(A \cap B)}{P(B)} \cdot P(B)\]</span>
<span class="math display">\[P(A | B) \cdot P(B) = P(A \cap B) \text{ (multiplicative law)}\]</span></p>
<p>If two events are independent, <span class="math inline">\(P(A \cap B) = P(A) \cdot P(B)\)</span>, and:</p>
<ul>
<li><span class="math inline">\(P(B) &gt; 0\)</span>, the conditional probability becomes</li>
</ul>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]</span>
<span class="math display">\[P(A | B) = \frac{P(A) \cdot P(B)}{P(B)}\]</span>
<span class="math display">\[P(A | B) = P(A) \text{ (Eq. 2)}\]</span></p>
<ul>
<li><span class="math inline">\(P(A) &gt; 0\)</span>, the conditional probability becomes</li>
</ul>
<p><span class="math display">\[P(B | A) = \frac{P(B \cap A)}{P(A)}\]</span>
<span class="math display">\[P(B | A) = \frac{P(B) \cdot P(A)}{P(A)}\]</span>
<span class="math display">\[P(B | A) = P(B) \text{ (Eq. 3)}\]</span>
Equations 2 and 3 mean that knowing that one event occurred does not influence the probability of the outcome of the other event. This is in fact the definition of the independence: if knowing that one event occurred does not help to predict (does not influence) the outcome of the other event, the two events are by essence independent.</p>
<div id="bayes-theorem" class="section level2">
<h2>Bayes’ theorem</h2>
<p>From the formulas of the conditional probability and the multiplicative law, we can derive the Bayes’ theorem:</p>
<p><span class="math display">\[P(B | A) = \frac{P(B \cap A)}{P(A)} \text{ (from conditional probability)}\]</span>
<span class="math display">\[P(B | A) = \frac{P(A \cap B)}{P(A)} \text{ (since } P(A \cap B) = P(B \cap A))\]</span>
<span class="math display">\[P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \text{ (from multiplicative law)}\]</span></p>
<p>which is equivalent to</p>
<p><span class="math display">\[P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)} \text{ (Bayes&#39; theorem)}\]</span></p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>In order to illustrate the conditional probability and the Bayes’ theorem, suppose the following problem:</p>
<p>In order to determine the presence of a disease in a person, a blood test is performed. When a person has the disease, the test can reveal the disease in 80% of cases. When the disease is not present, the test is negative in 90% of cases. Experience has shown that the probability of the disease being present is 10%. A researcher would like to know the probability that an individual has the disease given that the result of the test is positive.</p>
<p>To answer this question, the following events are defined:</p>
<ul>
<li>P: the test result is positive</li>
<li>D: the person has the disease</li>
</ul>
<p>Moreover, we use a tree diagram to illustrate the statement:</p>
<p><img src="/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/Screenshot%202020-03-03%20at%2013.54.24.png" style="width:100.0%" /></p>
<p>(The sum of all 4 scenarios must be equal to 1 since these 4 scenarios include all possible cases.)</p>
<p>We are looking for the probability that an individual has the disease given that the result of the test is positive, <span class="math inline">\(P(D | P)\)</span>. Following the formula of the conditional probability (Eq. 1) we have:</p>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]</span></p>
<p>In terms of our problem:</p>
<p><span class="math display">\[P(D | P) = \frac{P(D \cap P)}{P(P)}\]</span>
<span class="math display">\[P(D | P) = \frac{0.08}{P(P)} \text{ (Eq. 4)}\]</span></p>
<p>From the tree diagram, we can see that a positive test result is possible under two scenarios: (i) when a person has the disease, or (ii) when the person does not actually have the disease (because the test is not always correct). In order to find the probability of a positive test result, <span class="math inline">\(P(P)\)</span>, we need to sum up those two scenarios:</p>
<p><span class="math display">\[P(P) = P(D \cap P) + P(\bar{D} \cap P) = 0.08+0.09=0.17\]</span></p>
<p>Eq. 4 then becomes</p>
<p><span class="math display">\[P(D | P) = \frac{0.08}{0.17} = 0.4706\]</span></p>
<p>The probability of having the disease given that the result of the test is positive is only 47.06%. This means that in this specific case (with the same percentages), an individual has less than 1 chance out of 2 of having the disease knowing that his test is positive!</p>
<p>This relatively small percentage is due to the facts that the disease is quite rare (only 10% of the population is affected) and that the test is not always correct (sometimes it detects the disease although it is not present, and sometimes it does not detect it although it is present). As a consequence, a higher percentage of healthy people have a positive result (9%) compared to the percentage of people who have a positive result and who actually have the disease (8%). This explains why several diagnostic tests are often performed before announcing the result of the test, especially for rare diseases.</p>
</div>
</div>
<div id="accuracy-measures" class="section level1">
<h1>8. Accuracy measures</h1>
<p>Based on the example of the disease and the diagnostic test presented above, we explain the most common accuracy measures:</p>
<ul>
<li>False negatives</li>
<li>False positives</li>
<li>Sensitivity</li>
<li>Specificity</li>
<li>Positive predictive value</li>
<li>Negative predictive value</li>
</ul>
<p>Before diving into the details of these accuracy measures, here is an overview of the measures and the tree diagram with the labels added for each of the 4 scenarios:</p>
<div class="figure">
<img src="/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know.png" style="width:100.0%" alt="" />
<p class="caption">Adapted from Wikipedia</p>
</div>
<p><img src="/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/Screenshot%202020-03-03%20at%2015.53.19.png" style="width:100.0%" /></p>
<div id="false-negatives" class="section level2">
<h2>False negatives</h2>
<p>The false negatives (FN) are the number of people incorrectly labeled as <strong>not</strong> having the disease or the condition, when in reality it is present. It is like telling a women who is 7 months pregnant that she is not pregnant.</p>
<p>From the tree diagram, we have:</p>
<p><span class="math display">\[FN = P(D \cap \bar{P}) = 0.02\]</span></p>
<p>Moreover, the false negative <strong>rate</strong> (<em>FNR</em>) is defined as</p>
<p><span class="math display">\[\begin{align}
FNR &amp;= \frac{FN}{FN + TP} \\
 &amp;= P(\bar{P} | D) \\
 &amp;= \frac{P(\bar{P} \cap D)}{P(D)} \\
 &amp;= \frac{0.02}{0.08 + 0.02} \\
 &amp;= 0.2
\end{align}\]</span></p>
</div>
<div id="false-positives" class="section level2">
<h2>False positives</h2>
<p>The false positives (FP) are the number of people incorrectly labeled as having the disease or the condition, when in reality it is <strong>not</strong> present. It is like telling a man he is pregnant.</p>
<p>From the tree diagram, we have:</p>
<p><span class="math display">\[FP = P(\bar{D} \cap P) = 0.09\]</span></p>
<p>Moreover, the false positive <strong>rate</strong> (<em>FPR</em>) is defined as</p>
<p><span class="math display">\[\begin{align}
FPR &amp;= \frac{FP}{FP + TN} \\
 &amp;= P(P | \bar{D}) \\
 &amp;= \frac{P(P \cap \bar{D})}{P(\bar{D})} \\
 &amp;= \frac{0.09}{0.09 + 0.81} \\
 &amp;= 0.1
\end{align}\]</span></p>
</div>
<div id="sensitivity" class="section level2">
<h2>Sensitivity</h2>
<p>The sensitivity of a test, also referred as the recall, measures the ability of a test to detect the condition when the condition is present (the percentage of sick people who are correctly identified as having the disease):</p>
<p><span class="math display">\[ Sensitivity = \frac{TP}{TP + FN}\]</span></p>
<p>where <em>TP</em> is the true positives.</p>
<p>From the tree diagram, we have:</p>
<p><span class="math display">\[ Sensitivity = \frac{TP}{TP + FN} = P(P|D) = 0.8\]</span></p>
</div>
<div id="specificity" class="section level2">
<h2>Specificity</h2>
<p>The specificity of a test measures the ability of a test to correctly exclude the condition when the condition is absent (the percentage of healthy people who are correctly identified as not having the disease):</p>
<p><span class="math display">\[Specificity = \frac{TN}{TN + FP}\]</span></p>
<p>where <em>TN</em> is the true negatives.</p>
<p>From the tree diagram, we have:</p>
<p><span class="math display">\[Specificity = \frac{TN}{TN + FP} = P(\bar{P} | \bar{D}) = 0.9\]</span></p>
</div>
<div id="positive-predictive-value" class="section level2">
<h2>Positive predictive value</h2>
<p>The positive predictive value, also referred as the precision, is the proportion of positives that correspond to the presence of the condition, so the proportions of positive results that are true positive results:</p>
<p><span class="math display">\[PPV = \frac{TP}{TP+FP}\]</span></p>
<p>From the tree diagram, we have:</p>
<p><span class="math display">\[PPV = \frac{TP}{TP+FP} = P(D | P) = \frac{P(D \cap P)}{P(P)}\]</span>
<span class="math display">\[= \frac{0.08}{0.08+0.09} = 0.4706\]</span></p>
</div>
<div id="negative-predictive-value" class="section level2">
<h2>Negative predictive value</h2>
<p>The negative predictive value is the proportion of negatives that correspond to the absence of the condition, so the proportions of negative results that are true negative results:</p>
<p><span class="math display">\[NPV = \frac{TN}{TN + FN}\]</span></p>
<p>From the tree diagram, we have:</p>
<p><span class="math display">\[NPV = \frac{TN}{TN + FN} = P(\bar{D} | \bar{P}) = \frac{P(\bar{D} \cap \bar{P})}{P(\bar{P})}\]</span>
<span class="math display">\[= \frac{0.81}{0.81+0.02} = 0.9759\]</span></p>
</div>
</div>
<div id="counting-techniques" class="section level1">
<h1>9. Counting techniques</h1>
<p>In order to use the formula in <a href="/blog/the-9-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/#compute-a-probability">section 2</a>, one must know how to count the number of possible elements.</p>
<p>There are 3 main counting techniques in probability:</p>
<ol style="list-style-type: decimal">
<li>Multiplication</li>
<li>Permutation</li>
<li>Combination</li>
</ol>
<p>See below how to count the number of possible elements in case of equiprobable results.</p>
<div id="multiplication" class="section level2">
<h2>Multiplication</h2>
<p>The multiplication rule is as follows:</p>
<p><span class="math display">\[\#(A \times B) = (\#A) \times (\#B)\]</span></p>
<p>where <span class="math inline">\(\#\)</span> is the number of elements.</p>
<div id="example-1" class="section level3">
<h3>Example</h3>
<p>In a restaurant, a customer has to choose a starter, a main course and a dessert. The restaurant offers 2 starters, 3 main courses and 2 desserts. How many different choices are possible?</p>
<p>There are 12 different possible choices (i.e., <span class="math inline">\(2 \cdot 3 \cdot 2\)</span>).</p>
</div>
</div>
<div id="permutation" class="section level2">
<h2>Permutation</h2>
<p>The number of permutations is as follows:</p>
<p><span class="math display">\[P^r_n = n \times (n - 1) \times \cdots \times (n - r + 1) = \frac{n !}{(n - r)!}\]</span></p>
<p>with <span class="math inline">\(r\)</span> the length, <span class="math inline">\(n\)</span> the number of elements and <span class="math inline">\(r \le n\)</span>. Note that <span class="math inline">\(0! = 1\)</span> and <span class="math inline">\(k! = k \times (k - 1) \times (k - 2) \times \cdots \times 2 \times 1\)</span> if <span class="math inline">\(k = 1, 2, \dots\)</span></p>
<p>The order is important in permutations!</p>
<div id="example-2" class="section level3">
<h3>Example</h3>
<p>Count the permutations of length 2 of the set <span class="math inline">\(A = \{a, b, c, d\}\)</span>, without a letter being repeated. How many permutations do you find?</p>
<div id="by-hand" class="section level4">
<h4>By hand</h4>
<p><span class="math display">\[P^4_2 = \frac{4!}{(4-2)!} = \frac{4\cdot3\cdot2\cdot1}{2\cdot1} = 12\]</span></p>
</div>
<div id="in-r" class="section level4">
<h4>In R</h4>
<pre class="r"><code>library(gtools)

x &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)

# See all different permutations
perms &lt;- permutations(
  n = 4, r = 2, v = x,
  repeats.allowed = FALSE
)
perms</code></pre>
<pre><code>##       [,1] [,2]
##  [1,] &quot;a&quot;  &quot;b&quot; 
##  [2,] &quot;a&quot;  &quot;c&quot; 
##  [3,] &quot;a&quot;  &quot;d&quot; 
##  [4,] &quot;b&quot;  &quot;a&quot; 
##  [5,] &quot;b&quot;  &quot;c&quot; 
##  [6,] &quot;b&quot;  &quot;d&quot; 
##  [7,] &quot;c&quot;  &quot;a&quot; 
##  [8,] &quot;c&quot;  &quot;b&quot; 
##  [9,] &quot;c&quot;  &quot;d&quot; 
## [10,] &quot;d&quot;  &quot;a&quot; 
## [11,] &quot;d&quot;  &quot;b&quot; 
## [12,] &quot;d&quot;  &quot;c&quot;</code></pre>
<pre class="r"><code># Count the number of permutations
nrow(perms)</code></pre>
<pre><code>## [1] 12</code></pre>
</div>
</div>
</div>
<div id="combination" class="section level2">
<h2>Combination</h2>
<p>The number of combinations is as follows:</p>
<p><span class="math display">\[C^r_n = \frac{P^r_n}{r!} = \frac{n !}{r!(n - r)!} = {n \choose r}\]</span>
<span class="math display">\[= \frac{n}{r} \times \frac{n - 1}{r - 1} \times \dots \times \frac{n - r + 1}{1}\]</span></p>
<p>with <span class="math inline">\(r\)</span> the length, <span class="math inline">\(n\)</span> the number of elements and <span class="math inline">\(r \le n\)</span>.</p>
<p>The order is <strong>not</strong> important in combinations!</p>
<div id="example-3" class="section level3">
<h3>Example</h3>
<p>In a family of 5 children, what is the probability that there are 3 girls and 2 boys? Assume that the probabilities of giving birth to a girl and a boy are equal.</p>
<div id="by-hand-1" class="section level4">
<h4>By hand</h4>
<ul>
<li>Count of 3 girls and 2 boys (favourable cases): <span class="math inline">\(C^3_5 = {5 \choose 3} = \frac{5!}{3!(5-3)!} = 10\)</span></li>
<li>Count of possible cases: <span class="math inline">\(2^5 = 32\)</span></li>
</ul>
<p><span class="math inline">\(\Rightarrow P(3 \text{ girls and 2 boys}) = \frac{\text{# of favourable cases}}{\text{# of possible cases}}\)</span> <span class="math display">\[= \frac{10}{32} = 0.3125\]</span></p>
</div>
<div id="in-r-1" class="section level4">
<h4>In R</h4>
<ul>
<li>Count of 3 girls and 2 boys:</li>
</ul>
<pre class="r"><code>choose(n = 5, k = 3)</code></pre>
<pre><code>## [1] 10</code></pre>
<ul>
<li>Count of possible cases:</li>
</ul>
<pre class="r"><code>2^5</code></pre>
<pre><code>## [1] 32</code></pre>
<p>Probability of 3 girls and 2 boys:</p>
<pre class="r"><code>choose(n = 5, k = 3) / 2^5</code></pre>
<pre><code>## [1] 0.3125</code></pre>
<p>Thanks for reading. I hope this article helped you to understand the most important formulas and concepts from probability theory.</p>
<p>As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion.</p>
</div>
</div>
</div>
</div>
