---
title: The 7 concepts and formulas in probability that every data scientist should know
author: Antoine Soetewey
date: '2020-03-03'
slug: the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know
categories: []
tags:
  - Basics
  - Probability
  - R
  - Statistics
meta_img: image/image.png
description: xxx
output:
  blogdown::html_page:
    toc: true
    toc_depth: 6
draft: true
---


<div id="TOC">
<ul>
<li><a href="#what-is-probability">What is probability?</a></li>
<li><a href="#a-probability-is-always-between-0-and-1">1. A probability is always between 0 and 1</a></li>
<li><a href="#compute-a-probability">2. Compute a probability</a></li>
<li><a href="#complement-of-an-event">3. Complement of an event</a></li>
<li><a href="#union-of-two-events">4. Union of two events</a></li>
<li><a href="#intersection-of-two-events">5. Intersection of two events</a></li>
<li><a href="#independence-of-two-events">6. Independence of two events</a></li>
<li><a href="#conditional-probability">7. Conditional probability</a><ul>
<li><a href="#bayes-theorem">Bayes’ theorem</a></li>
<li><a href="#example">Example</a></li>
</ul></li>
<li><a href="#accuracy-measures">8. Accuracy measures</a><ul>
<li><a href="#sensibility">Sensibility</a></li>
</ul></li>
</ul>
</div>

<p>xxx add image</p>
<div id="what-is-probability" class="section level1">
<h1>What is probability?</h1>
<p>Probability is the likelihood of an event occurring; it is a mathematical model to describe random phenomena. In other words, probability is a branch of mathematics that provides models to describe random processes. These mathematical tools allow to establish theoretical models for random phenomena and to use them to make predictions. Like every model, the probabilistic model is a simplification of the world. However, the model is useful as soon as it captures the essential features.</p>
<p>In this article, we present xxx fundamental formulas and conceps in probability to get you started in data science.</p>
</div>
<div id="a-probability-is-always-between-0-and-1" class="section level1">
<h1>1. A probability is always between 0 and 1</h1>
<p>The probability of an event is always between 0 and 1,</p>
<p><span class="math display">\[0 \le P(A) \le 1\]</span></p>
<ul>
<li>If an event is impossible: <em>P(A) = 0</em></li>
<li>If an event is certain: <em>P(A) = 1</em></li>
</ul>
<p>For example, throwing a 7 with a standard dice (with faces from 1 to 6) is impossible so its probability is equal to 0. Throwing any number from 1 to 6 with a dice is certain, so its probability is equal to 1.</p>
</div>
<div id="compute-a-probability" class="section level1">
<h1>2. Compute a probability</h1>
<p>If the elements of a sample space (the set of all possible results of a randomized experiment) are equiprobable (= all elements have the same probability), then the probability of an event occurring is equal to the number of favourable cases (number of ways it can happen) divided by the number of possible cases (total number of outcomes):</p>
<p><span class="math display">\[P(A) = \frac{\text{number of favourable cases}}{\text{number of possible cases}}\]</span></p>
<p>For example, all numbers of a six-sided dice are equiprobable since they all have the same probability of occuring. The probability of rolling a 3 with a dice is thus</p>
<p><span class="math display">\[P(3) = \frac{\text{number of favourable cases}}{\text{number of possible cases}} = \frac{1}{6}\]</span></p>
<p>because there is only one favourable case (there is only one face with a 3 on it), and there are 6 possible cases (because there are 6 faces altogether).</p>
</div>
<div id="complement-of-an-event" class="section level1">
<h1>3. Complement of an event</h1>
<p>The probability of the complement (or opposite) of an event is:</p>
<p><span class="math display">\[P(\text{not A}) = P(\bar{A}) = 1 - P(A)\]</span></p>
</div>
<div id="union-of-two-events" class="section level1">
<h1>4. Union of two events</h1>
<p>The probability of the union of two events is the probability of either occurring:</p>
<p><span class="math display">\[P(\text{A or B)} = P(A \cup B) = P(A) + P(B) - P(A \cap B)\]</span></p>
<p>Suppose that the probability of a fire breaking out in two houses in a given year is:</p>
<ul>
<li>in house A: 60%</li>
<li>in house B: 45%</li>
<li>in at least one of the two houses: 80%</li>
</ul>
<p>Graphically we have</p>
<p><img src="/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The probability of a fire breaking out in house A <strong>or</strong> house B is</p>
<p><span class="math display">\[P(A \cup B) = P(A) + P(B) - P(A \cap B) = 0.6 + 0.45 - 0.25 = 0.8\]</span></p>
<p>By summing P(A) and P(B), the intersection of A and B is counted twice. This is the reason we subtract it to count it only once.</p>
<p>If two events are mutually exclusive (i.e., two events that cannot occur simultaneously), the probability of both events occuring is equal to 0, so the above formula becomes</p>
<p><span class="math display">\[P(A \cup B) = P(A) + P(B)\]</span></p>
<p>For example, rolling a 3 or a 6 on a six-sided dice are two mutually exclusive events since they cannot both occur at the same time. Since their joint probability is equal to 0, the probability of rolling a 3 or 6 on a six-sided dice is</p>
<p><span class="math display">\[P(3 \cup 6) = P(3) + P(6) = \frac{1}{6} + \frac{1}{6} = \frac{1}{3}\]</span></p>
</div>
<div id="intersection-of-two-events" class="section level1">
<h1>5. Intersection of two events</h1>
<p>If two events are independent, the probability of the intersection of two events (i.e., the joint probability) is the probability of the two events occuring:</p>
<p><span class="math display">\[P(\text{A and B)} = P(A \cap B) = P(A) \cdot P(B)\]</span></p>
<p>For instance, if two coins are flipped, the probability of both being tails is</p>
<p><span class="math display">\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]</span>
Note that <span class="math inline">\(P(A \cap B) = P(B \cap A)\)</span>.</p>
<p>If two events are mutually exclusive, their joint probability is equal to 0:</p>
<p><span class="math display">\[P(A \cap B) = 0\]</span></p>
</div>
<div id="independence-of-two-events" class="section level1">
<h1>6. Independence of two events</h1>
<p>The independence of two events can be verified thanks to the above formula. If the equality holds, the two events are said to be independent, otherwise the two events are said to be dependent. Formally, the events A and B are independent if and only if</p>
<p><span class="math display">\[P(A \cap B) = P(A) \cdot P(B)\]</span></p>
<ul>
<li>In the example of the two coins:</li>
</ul>
<p><span class="math display">\[P(T_1 \cap T_2) = \frac{1}{4}\]</span></p>
<p>and</p>
<p><span class="math display">\[P(T_1) \cdot P(T_2) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\]</span></p>
<p>so the following equality holds</p>
<p><span class="math display">\[P(T_1 \cap T_2) = P(T_1) \cdot P(T_2) = \frac{1}{4}\]</span></p>
<p>The two events are thus independent, denoted <span class="math inline">\(T_1{\perp\!\!\!\perp}T_2\)</span>.</p>
<ul>
<li>In the example of the fire breaking out in two houses (see <a href="/blog/the-7-concepts-and-formulas-in-probability-that-every-data-scientist-should-know/#union-of-two-events">section 4</a>):</li>
</ul>
<p><span class="math display">\[P(A \cap B) = 0.25\]</span></p>
<p>and</p>
<p><span class="math display">\[P(A) \cdot P(B) = 0.6 \cdot 0.45 = 0.27\]</span></p>
<p>so the following equality does not hold</p>
<p><span class="math display">\[P(A \cap B) \ne P(A) \cdot P(B)\]</span></p>
<p>The two events are thus dependent (or not independent), denoted <span class="math inline">\(A \not\!\perp\!\!\!\perp B\)</span>.</p>
</div>
<div id="conditional-probability" class="section level1">
<h1>7. Conditional probability</h1>
<p>Suppose two events A and B and P(B) &gt; 0. The conditional probability of A given (knowing) B is the likelihood of event A occurring given that event B has occured:</p>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B \cap A)}{P(B)} \text{ (since } P(A \cap B) = P(B \cap A))\]</span></p>
<p>Note that, in general, the probability of A given B is not equal to the probability of B given A, that is, <span class="math inline">\(P(A | B) \ne P(B | A)\)</span>.</p>
<p>From the formula of the conditional probability, we can derive the multiplicative law:</p>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)} \text{ (Eq. 1)}\]</span>
<span class="math display">\[P(A | B) \cdot P(B) = \frac{P(A \cap B)}{P(B)} \cdot P(B)\]</span>
<span class="math display">\[P(A | B) \cdot P(B) = P(A \cap B) \text{ (multiplicative law)}\]</span></p>
<p>If two events are independent, <span class="math inline">\(P(A \cap B) = P(A) \cdot P(B)\)</span>, and:</p>
<ul>
<li>P(B) &gt; 0, the conditional probability becomes</li>
</ul>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]</span>
<span class="math display">\[P(A | B) = \frac{P(A) \cdot P(B)}{P(B)}\]</span>
<span class="math display">\[P(A | B) = P(A) \text{ (Eq. 2)}\]</span></p>
<ul>
<li>P(A) &gt; 0, the conditional probability becomes</li>
</ul>
<p><span class="math display">\[P(B | A) = \frac{P(B \cap A)}{P(A)}\]</span>
<span class="math display">\[P(B | A) = \frac{P(B) \cdot P(A)}{P(A)}\]</span>
<span class="math display">\[P(B | A) = P(B) \text{ (Eq. 3)}\]</span>
Equations 2 and 3 mean that knowing that one event occured does not influence the probability of the outcome of the other event. This is in fact the definition of the independence: if knowing that one event occured does not help to predict (does not influence) the outcome of the other event, the two events are by essence independent.</p>
<div id="bayes-theorem" class="section level2">
<h2>Bayes’ theorem</h2>
<p>From the formulas of the conditional probability and the multiplicative law, we can derive the Bayes’ theorem:</p>
<p><span class="math display">\[P(B | A) = \frac{P(B \cap A)}{P(A)} \text{ (from the conditional probability)}\]</span>
<span class="math display">\[P(B | A) = \frac{P(A \cap B)}{P(A)} \text{ (since } P(A \cap B) = P(B \cap A))\]</span>
<span class="math display">\[P(B | A) = \frac{P(A | B) \cdot P(B)}{P(A)} \text{ (from the multiplicative law)}\]</span></p>
<p>which is equivalent to</p>
<p><span class="math display">\[P(A | B) = \frac{P(B | A) \cdot P(A)}{P(B)} \text{ (Bayes&#39; theorem)}\]</span></p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>In order to illustrate the conditional probability and the Bayes’ theorem, suppose the following problem:</p>
<p>In order to determine the presence of a disease in a person, a blood test is performed. When a person has the disease, the test can reveal the disease in 80% of cases. When the disease is not present, the test is negative in 90% of cases. Experience has shown that the probability of the disease being present is 10%. A researcher would like to know the probability that an individual has the disease given that the result of the test is positive.</p>
<p>To answer this question, the following events are defined:</p>
<ul>
<li>P: the test result is positive</li>
<li>D: the person has the disease</li>
</ul>
<p>Moreover, we use a tree diagram to illustrate the statement:</p>
<p><img src="/blog/the-7-formulas-in-probability-that-every-data-scientist-should-know_files/Screenshot%202020-03-03%20at%2013.54.24.png" style="width:100.0%" /></p>
<p>(The sum of all 4 scenarios must be equal to 1 since these 4 scenarios include all possible cases.)</p>
<p>We are looking for the probability that an individual has the disease given that the result of the test is positive, <span class="math inline">\(P(D | P)\)</span>. Following the formula of the conditional probability (Eq. 1) we have:</p>
<p><span class="math display">\[P(A | B) = \frac{P(A \cap B)}{P(B)}\]</span></p>
<p>In terms of our problem:</p>
<p><span class="math display">\[P(D | P) = \frac{P(D \cap P)}{P(P)}\]</span>
<span class="math display">\[P(D | P) = \frac{0.08}{P(P)} \text{ (Eq. 4)}\]</span></p>
<p>From the tree diagram, we can see that a positive test result is possible under two scenarios: (i) when a person has the disease, or (ii) when the person does not actually have the disease. In order to find the probability of a positive test result, <span class="math inline">\(P(P)\)</span>, we need to sum up those two scenarios:</p>
<p><span class="math display">\[P(P) = P(D \cap P) + P(\bar{D} \cap P) = 0.08+0.09=0.17\]</span></p>
<p>Eq. 4 then becomes</p>
<p><span class="math display">\[P(D | P) = \frac{0.08}{0.17} = 0.4706\]</span></p>
<p>The probability of having the disease given that the result of the test is positive is only 47.06%. This means that in this specific case (with the same percentages), an individual has less than 1 chance out of 2 of having the disease knowing that his test is positive!</p>
<p>This relatively small percentage is due to the facts that the disease is quite rare (only 10% of the population is affected) and that the test is not always correct (sometimes it detects the disease although it is not present, and sometimes it does not detect it although it is present). As a consequence, a higher percentage of healthy people have a positive result (9%) compared to the percentage of people who have a positive result and who actually have the disease (8%). This explains why several diagnostic tests are often performed before announcing the result of the test, especially for rare diseases.</p>
</div>
</div>
<div id="accuracy-measures" class="section level1">
<h1>8. Accuracy measures</h1>
<p>Following the example of the disease and the diagnostic test, we present the most common accuracy measures:</p>
<ul>
<li>Sensibility of a test</li>
<li>Specificity of a test</li>
<li>False negative</li>
<li>False positive</li>
<li>Positive predictive value</li>
<li>Negative predictive value</li>
</ul>
<p>Before diving into the details of these accurary measures, here in an overview of the possible scenarios:</p>
<div id="sensibility" class="section level2">
<h2>Sensibility</h2>
</div>
</div>
