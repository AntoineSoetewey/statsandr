---
title: Multiple linear regression made simple
author: Antoine Soetewey
date: '2021-10-04'
slug: multiple-linear-regression-made-simple
categories: []
tags:
  - R
  - Statistics
  - Regression
meta_img: blog/multiple-linear-regression-made-simple/images/multiple-linear-regression.jpeg
description: Learn how to run multiple and simple linear regression in R, how to interpret the results and how to verify the conditions of application
output:
  blogdown::html_page:
    toc: true
    toc_depth: 6
# draft: true
bibliography: bibliography.bib
---


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#simple-linear-regression-reminder" id="toc-simple-linear-regression-reminder">Simple linear regression: reminder</a>
<ul>
<li><a href="#principle" id="toc-principle">Principle</a></li>
<li><a href="#equation" id="toc-equation">Equation</a></li>
<li><a href="#interpretations-of-coefficients-widehatbeta" id="toc-interpretations-of-coefficients-widehatbeta">Interpretations of coefficients <span class="math inline">\(\widehat\beta\)</span></a>
<ul>
<li><a href="#another-interpretation-of-the-intercept" id="toc-another-interpretation-of-the-intercept">Another interpretation of the intercept</a></li>
</ul></li>
<li><a href="#significance-of-the-relationship" id="toc-significance-of-the-relationship">Significance of the relationship</a>
<ul>
<li><a href="#correlation-does-not-imply-causation" id="toc-correlation-does-not-imply-causation">Correlation does not imply causation</a></li>
</ul></li>
<li><a href="#conditions-of-application" id="toc-conditions-of-application">Conditions of application</a></li>
<li><a href="#visualizations" id="toc-visualizations">Visualizations</a></li>
</ul></li>
<li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression">Multiple linear regression</a>
<ul>
<li><a href="#principle-1" id="toc-principle-1">Principle</a></li>
<li><a href="#equation-1" id="toc-equation-1">Equation</a></li>
<li><a href="#interpretations-of-coefficients-widehatbeta-1" id="toc-interpretations-of-coefficients-widehatbeta-1">Interpretations of coefficients <span class="math inline">\(\widehat\beta\)</span></a></li>
<li><a href="#conditions-of-application-1" id="toc-conditions-of-application-1">Conditions of application</a></li>
<li><a href="#how-to-choose-a-good-linear-model" id="toc-how-to-choose-a-good-linear-model">How to choose a good linear model?</a>
<ul>
<li><a href="#p-value-associated-to-the-model" id="toc-p-value-associated-to-the-model"><span class="math inline">\(P\)</span>-value associated to the model</a></li>
<li><a href="#coefficient-of-determination-r2" id="toc-coefficient-of-determination-r2">Coefficient of determination <span class="math inline">\(R^2\)</span></a></li>
<li><a href="#parsimony" id="toc-parsimony">Parsimony</a></li>
</ul></li>
<li><a href="#visualizations-1" id="toc-visualizations-1">Visualizations</a></li>
<li><a href="#to-go-further" id="toc-to-go-further">To go further</a>
<ul>
<li><a href="#print-models-parameters" id="toc-print-models-parameters">Print model’s parameters</a></li>
<li><a href="#automatic-reporting" id="toc-automatic-reporting">Automatic reporting</a></li>
<li><a href="#predictions" id="toc-predictions">Predictions</a></li>
<li><a href="#linear-hypothesis-tests" id="toc-linear-hypothesis-tests">Linear hypothesis tests</a></li>
<li><a href="#overall-effect-of-categorical-variables" id="toc-overall-effect-of-categorical-variables">Overall effect of categorical variables</a></li>
<li><a href="#interaction" id="toc-interaction">Interaction</a></li>
</ul></li>
</ul></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p><img src="images/multiple-linear-regression.jpeg" style="width:100.0%" /></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Remember that <a href="/blog/descriptive-statistics-in-r/">descriptive statistics</a> is a branch of statistics that allows to describe your data at hand.</p>
<p>Inferential statistics (with the popular <a href="/blog/hypothesis-test-by-hand/">hypothesis tests</a> and confidence intervals) is another branch of statistics that allows to make inferences, that is, to draw conclusions about a population based on a <a href="/blog/what-is-the-difference-between-population-and-sample/">sample</a>.</p>
<p>The last branch of statistics is about <strong>modeling the relationship between two or more variables</strong>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> The most common statistical tool to describe and evaluate the link between variables is linear regression.</p>
<p>There are two types of linear regression:</p>
<ol style="list-style-type: decimal">
<li><strong>Simple linear regression</strong> is a statistical approach that allows to assess the linear relationship between two <a href="/blog/variable-types-and-examples/#quantitative">quantitative variables</a>. More precisely, it enables the relationship to be quantified and its significance to be evaluated.</li>
<li><strong>Multiple linear regression</strong> is a generalization of simple linear regression, in the sense that this approach makes it possible to evaluate the linear relationships between a response variable (quantitative) and several explanatory variables (quantitative or <a href="/blog/variable-types-and-examples/#qualitative">qualitative</a>).</li>
</ol>
<p>In the real world, multiple linear regression is used more frequently than simple linear regression. This is mostly the case because:</p>
<ul>
<li>Multiple linear regression allows to evaluate the relationship between two variables, while <strong>controlling for the effect</strong> (i.e., removing the effect) <strong>of other variables</strong>.</li>
<li>With data collection becoming easier, more variables can be included and taken into account when analyzing data.</li>
</ul>
<p>Multiple linear regression being such a powerful statistical tool, I would like to present it so that everyone understands it, and perhaps even use it when deemed necessary. However, I cannot afford to write about multiple linear regression without first presenting simple linear regression.</p>
<p>So after a reminder about the principle and the interpretations that can be drawn from a simple linear regression, I will illustrate how to perform multiple linear regression in R. I will also show, in the context of multiple linear regression, how to interpret the output and discuss about its conditions of application. I will then conclude the article by presenting more advanced topics directly linked to linear regression.</p>
</div>
<div id="simple-linear-regression-reminder" class="section level1">
<h1>Simple linear regression: reminder</h1>
<p>Simple linear regression is an asymmetric procedure in which:</p>
<ul>
<li>one of the variable is considered the response or the variable to be explained. It is also called <strong>dependent variable</strong>, and is represented on the <span class="math inline">\(y\)</span>-axis</li>
<li>the other variable is the explanatory or also called <strong>independent variable</strong>, and is represented on the <span class="math inline">\(x\)</span>-axis</li>
</ul>
<p>Simple linear regression allows to <strong>evaluate the existence of a <em>linear</em> relationship between two variables</strong> and to quantify this link. Note that linearity is a strong assumption in linear regression in the sense that it tests and quantifies whether the two variables are <em>linearly</em> dependent.</p>
<p>What makes linear regression a powerful statistical tool is that it allows to <strong>quantify by what quantity the response/dependent variable varies when the explanatory/independent variable increases by one unit</strong>.</p>
<p>This concept is key in linear regression and helps to answer the following questions:</p>
<ul>
<li>Is there a link between the amount spent in advertising and the sales during a certain period?</li>
<li>Is the number of years of schooling valued, in financial terms, in the first job?</li>
<li>Will an increase in tobacco taxes reduce its consumption?</li>
<li>What is the most likely price of an apartment, depending on the area?</li>
<li>Does a person’s reaction time to a stimulus depend on gender?</li>
</ul>
<p>Simple linear regression can be seen as an extension to the <a href="/blog/anova-in-r/">analysis of variance (ANOVA)</a> and the <a href="/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/">Student’s t-test</a>. ANOVA and t-test allow to compare groups in terms of a quantitative variable—2 groups for t-test and 3 or more groups for ANOVA.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>For these tests, the independent variable, that is, the grouping variable forming the different groups to compare must be a qualitative variable. Linear regression is an extension because in addition to be used to compare groups, it is also used with quantitative independent variables (which is not possible with t-test and ANOVA).</p>
<p>In this article, we are interested in assessing whether there is a linear relationship between the distance traveled with a gallon of fuel and the weight of cars. For this example, we use the <code>mtcars</code> dataset (preloaded in R).</p>
<p>The dataset includes fuel consumption and 10 aspects of automotive design and performance for 32 automobiles:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<ol style="list-style-type: decimal">
<li><code>mpg</code> Miles/(US) gallon (with a gallon <span class="math inline">\(\approx\)</span> 3.79 liters)</li>
<li><code>cyl</code> Number of cylinders</li>
<li><code>disp</code> Displacement (cu.in.)</li>
<li><code>hp</code> Gross horsepower</li>
<li><code>drat</code> Rear axle ratio</li>
<li><code>wt</code> Weight (1000 lbs, with 1000 lbs <span class="math inline">\(\approx\)</span> 453.59 kg)</li>
<li><code>qsec</code> 1/4 mile time (with 1/4 mile <span class="math inline">\(\approx\)</span> 402.34 meters)</li>
<li><code>vs</code> Engine (0 = V-shaped, 1 = straight)</li>
<li><code>am</code> Transmission (0 = automatic, 1 = manual)</li>
<li><code>gear</code> Number of forward gears</li>
<li><code>carb</code> Number of carburetors</li>
</ol>
<pre class="r"><code>dat &lt;- mtcars

library(ggplot2)
ggplot(dat, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(
    y = &quot;Miles per gallon&quot;,
    x = &quot;Car&#39;s weight (1000 lbs)&quot;
  ) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The <a href="/blog/graphics-in-r-with-ggplot2/#scatter-plot">scatterplot</a> above shows that there seems to be a <strong>negative relationship between the distance traveled with a gallon of fuel and the weight of a car</strong>. This makes sense, as the heavier the car, the more fuel it consumes and thus the fewer miles it can drive with a gallon.</p>
<p>This is already a good overview of the relationship between the two variables, but a simple linear regression with the miles per gallon as dependent variable and the car’s weight as independent variable goes further. It will tell us by <strong>how many miles the distance varies, on average, when the weight varies by one unit</strong> (1000 lbs in this case). This is possible thanks to the regression line.</p>
<div id="principle" class="section level2">
<h2>Principle</h2>
<p>The principle of simple linear regression is to <strong>find the line</strong> (i.e., determine its equation) <strong>which passes as close as possible to the observations</strong>, that is, the set of points formed by the pairs <span class="math inline">\((x_i, y_i)\)</span>.</p>
<p>In the first step, there are many potential lines. Three of them are plotted:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>To find the line which passes as close as possible to all the points, we take the square of the vertical distance between each point and each potential line. Note that we take the square of the distances to make sure that a negative gap (i.e., a point below the line) is not compensated by a positive gap (i.e., a point above the line). The line which passes closest to the set of points is the one which <strong><em>minimizes</em></strong> <strong>the sum of these squared distances</strong>.</p>
<p>The resulting regression line is presented in blue in the following plot, and the dashed gray lines represent the vertical distance between the points and the fitted line. These vertical distances between each observed point and the fitted line determined by the least squares method are called the <strong>residuals</strong> of the linear regression model and denoted <span class="math inline">\(\epsilon\)</span>.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>By definition, there is no other line with a smaller total distance between the points and the line. This method is called the least squares method, or <strong>OLS</strong> for <strong>ordinary least squares</strong>.</p>
</div>
<div id="equation" class="section level2">
<h2>Equation</h2>
<p>The regression model can be written in the form of the equation:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon\]</span></p>
<p>with:</p>
<ul>
<li><span class="math inline">\(Y\)</span> the dependent variable</li>
<li><span class="math inline">\(X\)</span> the independent variable</li>
<li><span class="math inline">\(\beta_0\)</span> the intercept (the mean value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x = 0\)</span>), also sometimes denoted <span class="math inline">\(\alpha\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> the slope (the expected increase in <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> increases by one unit)</li>
<li><span class="math inline">\(\epsilon\)</span> the residuals (the error term of mean 0 which describes the variations of <span class="math inline">\(Y\)</span> not captured by the model, also referred as the noise)</li>
</ul>
<p>When we determine the line which passes closest to all the points (we say that we fit a line to the observed data), we actually <strong>estimate the unknown parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></strong> based on the data at hand. Remember from your geometry classes, to draw a line you only need two parameters—the intercept and the slope.</p>
<p>These estimates (and thus the blue line shown in the previous scatterplot) can be computed by hand with the following formulas:</p>
<p><span class="math display">\[\begin{align}
\widehat\beta_1 &amp;= \frac{\sum^n_{i = 1} (x_i - \bar{x})(y_i - \bar{y})}{\sum^n_{i = 1}(x_i - \bar{x})^2} \\
&amp;= \frac{\left(\sum^n_{i = 1}x_iy_i\right) - n\bar{x}\bar{y}}{\sum^n_{i = 1}(x_i - \bar{x})^2}
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[\widehat\beta_0 = \bar{y} - \widehat\beta_1 \bar{x}\]</span></p>
<p>with <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> denoting the sample mean of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, respectively.</p>
<p>(If you struggle to compute <span class="math inline">\(\widehat\beta_0\)</span> and <span class="math inline">\(\widehat\beta_1\)</span> by hand, see this <a href="/blog/a-shiny-app-for-simple-linear-regression-by-hand-and-in-r/">Shiny app</a> which helps you to easily find these estimates based on your data.)</p>
</div>
<div id="interpretations-of-coefficients-widehatbeta" class="section level2">
<h2>Interpretations of coefficients <span class="math inline">\(\widehat\beta\)</span></h2>
<p>The <strong>intercept <span class="math inline">\(\widehat\beta_0\)</span></strong> is the <strong>mean value of the dependent variable <span class="math inline">\(Y\)</span> when the independent variable <span class="math inline">\(X\)</span> takes the value 0</strong>. Its estimation has no interest in evaluating whether there is a linear relationship between two variables. It has, however, an interest if you want to know what the mean value of <span class="math inline">\(Y\)</span> could be when <span class="math inline">\(x = 0\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>The <strong>slope <span class="math inline">\(\widehat\beta_1\)</span></strong>, on the other hand, corresponds to the expected <strong>variation of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> varies by one unit</strong>. It tells us two important informations:</p>
<ol style="list-style-type: decimal">
<li>The <strong>sign of the slope</strong> indicates the <strong>direction of the line</strong>—a positive slope (<span class="math inline">\(\widehat\beta_1 &gt; 0\)</span>) indicates that there is a positive relationship between the two variables of interest (they vary in the same direction), whereas a negative slope (<span class="math inline">\(\widehat\beta_1 &lt; 0\)</span>) means that there is a negative relationship between the two variables (they vary in opposite directions).</li>
<li>The <strong>value of the slope</strong> provides information on the <strong>speed of evolution</strong> of the variable <span class="math inline">\(Y\)</span> as a function of the variable <span class="math inline">\(X\)</span>. The larger the slope in absolute value, the larger the expected variation of <span class="math inline">\(Y\)</span> for each unit of <span class="math inline">\(X\)</span>. Note, however, that a large value does not necessarily mean that the relationship is statistically significant (more on that in the section about <a href="/blog/multiple-linear-regression-made-simple/#significance-of-the-relationship">significance of the relationship</a>).</li>
</ol>
<p>This is similar to the <a href="/blog/correlation-coefficient-and-correlation-test-in-r/">correlation coefficient</a>, which gives information about the direction and the strength of the relationship between two variables.</p>
<p>To perform a linear regression in R, we use the <code>lm()</code> function (which stands for linear model). The function requires to set the dependent variable first then the independent variable, separated by a tilde (<code>~</code>).</p>
<p>Applied to our example of weight and car’s consumption, we have:</p>
<pre class="r"><code>model &lt;- lm(mpg ~ wt, data = dat)</code></pre>
<p>The <code>summary()</code> function gives the results of the model:</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528,	Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10</code></pre>
<p>In practice, we usually check the conditions of application <em>before</em> interpreting the coefficients (because if they are not respected, results may be biased).</p>
<p>In this article, however, I present the interpretations before testing the conditions because the point is to show how to interpret the results, and less about finding a valid model.</p>
<p>The results can be summarized as follows (see the column <code>Estimate</code> in the table <code>Coefficients</code>):</p>
<ul>
<li>The intercept <span class="math inline">\(\widehat\beta_0 =\)</span> 37.29 indicates that, for a hypothetical car weighting 0 lbs, we can expect, on average, a consumption of 37.29 miles/gallon. This interpretation is shown for illustrative purposes, but as a car weighting 0 lbs is impossible, the interpretation has no meaning. In practice, we would therefore refrain from interpreting the intercept in this case. See another interpretation of the intercept when the independent variable is centered around its mean in this <a href="/blog/multiple-linear-regression-made-simple/#another-interpretation-of-the-intercept">section</a>.</li>
<li>The slope <span class="math inline">\(\widehat\beta_1 =\)</span> -5.34 indicates that:
<ul>
<li>There is a <strong>negative relationship</strong> between the weight and the distance a car can drive with a gallon (this was expected given the negative trend of the points in the scatterplot shown previously).</li>
<li>But more importantly, a slope of -5.34 means that, for an increase of one unit in the weight (that is, an increase of 1000 lbs), the number of miles per gallon decreases, on average, by 5.34 units. In other words, <strong>for an increase of 1000 lbs, the number of miles/gallon decreases, on average, by 5.34</strong>.</li>
</ul></li>
</ul>
<div id="another-interpretation-of-the-intercept" class="section level3">
<h3>Another interpretation of the intercept</h3>
<p>Another useful interpretation of the intercept is when the independent variable is centered around its mean. In this case, the intercept is interpreted as the mean value of <span class="math inline">\(Y\)</span> for individuals who have a value of <span class="math inline">\(X\)</span> equal to the mean of <span class="math inline">\(X\)</span>.</p>
<p>Let’s see it in practice.</p>
<p>We first center the <code>wt</code> variable around the mean then rerun a simple linear model with this new variable:</p>
<pre class="r"><code>dat_centered &lt;- dat

dat_centered$wt_centered &lt;- dat$wt - mean(dat$wt)

mod_centered &lt;- lm(mpg ~ wt_centered,
  data = dat_centered
)

summary(mod_centered)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt_centered, data = dat_centered)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  20.0906     0.5384  37.313  &lt; 2e-16 ***
## wt_centered  -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528,	Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10</code></pre>
<p>Based on the results, we see that:</p>
<ul>
<li>The slope has not changed, the interpretation is the same than without the centering (which makes sense since the regression line has simply been shifted to the right or left).</li>
<li>More importantly, the intercept is now <span class="math inline">\(\widehat\beta_0 =\)</span> 20.09, so we can expect, on average, a consumption of 20.09 miles/gallon for a car with an average weight (the mean of weight is 3.22 so 3220 lbs).</li>
</ul>
<p>This centering is particularly interesting:</p>
<ul>
<li>when the continuous independent variable has <strong>no</strong> meaningful value of 0 (which is the case here as a car with a weight of 0 lbs is not meaningful), or</li>
<li>when interpreting the intercept is important.</li>
</ul>
<p>Note that centering does not have to be done around the mean only. The independent variable can also be centered at some value that is actually in the range of the data. The exact value you center on does not matter as long it’s meaningful and within the range of data (it is not recommended to center it on a value that is not in the range of the data because we are not sure about the type of relationship between the two variables outside that range).</p>
<p>For our example, we may find that choosing the lowest value or the highest value of weight is the best option. So it’s up to us to decide the weight at which it’s most meaningful to interpret the intercept.</p>
</div>
</div>
<div id="significance-of-the-relationship" class="section level2">
<h2>Significance of the relationship</h2>
<p>As mentioned earlier, the <strong>value of the slope does not</strong>, by itself, make it possible to <strong>assess the significance of the linear relationship</strong>.</p>
<p>In other words, a slope different from 0 does not necessarily mean it is <em>significantly</em> different from 0, so it does not mean that there is a <strong>significant</strong> relationship between the two variables in the population. There could be a slope of 10 that is not significant, and a slope of 2 that is significant.</p>
<p>Significance of the relationship also depends on the variability of the slope, which is measured by its standard error and generally noted <span class="math inline">\(se(\widehat\beta_1)\)</span>.</p>
<p>Without going too much into details, to assess the significance of the linear relationship, we divide the slope by its standard error. This ratio is the test statistic and follows a Student distribution with <span class="math inline">\(n - 2\)</span> degrees of freedom:<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p><span class="math display">\[T_{n - 2} = \frac{\widehat\beta_1}{se(\widehat\beta_1)}\]</span></p>
<p>For a bilateral test, the null and alternative hypotheses are:<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<ul>
<li><span class="math inline">\(H_0 : \beta_1 = 0\)</span> (there is no (linear) relationship between the two variables)</li>
<li><span class="math inline">\(H_1 : \beta_1 \ne 0\)</span> (there is a (linear) relationship between the two variables)</li>
</ul>
<p>Roughly speaking, if this ratio is greater than 2 in absolute value then the slope is significantly different from 0, and therefore the relationship between the two variables is significant (and in that case it is positive or negative depending on the sign of the estimate <span class="math inline">\(\widehat\beta_1\)</span>).</p>
<p>The standard error and the test statistic are shown in the column <code>Std. Error</code> and <code>t value</code> in the table <code>Coefficients</code>.</p>
<p>Fortunately, R gives a more precise and easier way to assess to the significance of the relationship. The information is provided in the column <code>Pr(&gt;|t|)</code> of the <code>Coefficients</code> table. This is the <a href="/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/#a-note-on-p-value-and-significance-level-alpha"><em>p</em>-value</a> of the test. As for any <a href="/blog/what-statistical-test-should-i-do/">statistical test</a>, if the <em>p</em>-value is greater than or equal to the significance level (usually <span class="math inline">\(\alpha = 0.05\)</span>), we do not reject the null hypothesis, and if the <em>p</em>-value is lower than the significance level, we reject the null hypothesis.</p>
<p>If we do not reject the null hypothesis, we do not reject the hypothesis of no relationship between the two variables (because we do not reject the hypothesis of a slope of 0). On the contrary, if we reject the null hypothesis of no relationship, we can conclude that there is a significant linear relationship between the two variables.</p>
<p>In our example, the <em>p</em>-value = 1.29e-10 &lt; 0.05 so we reject the null hypothesis at the significance level <span class="math inline">\(\alpha = 5\%\)</span>. We therefore conclude that there is a <strong>significant relationship between a car’s weight and its fuel consumption</strong>.</p>
<p><em>Tip:</em> In order to make sure I interpret only parameters that are significant, I tend to first check the significance of the parameters thanks to the <em>p</em>-values, and then interpret the estimates accordingly. For completeness, note that the test is also performed on the intercept. The <em>p</em>-value being smaller than 0.05, we also conclude that the intercept is significantly different from 0.</p>
<div id="correlation-does-not-imply-causation" class="section level3">
<h3>Correlation does not imply causation</h3>
<p>Be careful that a significant relationship between two variables does not necessarily mean that there is an influence of one variable on the other or that there is a causal effect between these two variables!</p>
<p>A significant relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can appear in several cases:</p>
<ul>
<li><span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(Y\)</span> causes <span class="math inline">\(X\)</span></li>
<li>a third variable cause <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
<li>a combination of these three reasons</li>
</ul>
<p>A statistical model alone cannot establish a causal link between two variables. Demonstrating causality between two variables is more complex and requires, among others, a specific experimental design, the repeatability of the results over time, as well as various samples.</p>
<p>This is the reason you will often read “<a href="/blog/correlation-coefficient-and-correlation-test-in-r/#correlation-does-not-imply-causation">Correlation does not imply causation</a>” and linear regression follows the same principle.</p>
</div>
</div>
<div id="conditions-of-application" class="section level2">
<h2>Conditions of application</h2>
<p>Unfortunately, linear regression cannot be used in all situations.</p>
<p>In addition to the requirement that the dependent variable must be a continuous quantitative variables, simple linear regression requires that the data satisfy the following conditions:</p>
<ol style="list-style-type: decimal">
<li><strong>Linearity:</strong> The relationship between the two variables should be linear (at least roughly). For this reason it is always necessary to represent graphically the data with a scatterplot before performing a simple linear regression.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></li>
</ol>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Independence:</strong> Observations must be independent. It is the sampling plan and the experimental design that usually provide information on this condition. If the data come from different individuals or experimental units, they are usually independent. On the other hand, if the same individuals are measured at different periods, the data are probably not independent.</li>
<li><strong>Normality of the residuals:</strong> For large sample sizes, confidence intervals and tests on the coefficients are (approximately) valid whether the error follows a <a href="/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/">normal distribution</a> or not (a consequence of the central limit theorem, see more in <span class="citation">Ernst and Albers (<a href="#ref-ernst2017regression">2017</a>)</span> and <span class="citation">Lumley et al. (<a href="#ref-lumley2002importance">2002</a>)</span>)! For small sample sizes, residuals should follow a normal distribution. This condition can be tested visually (via a <a href="/blog/descriptive-statistics-in-r/#qq-plot">QQ-plot</a> and/or a <a href="/blog/descriptive-statistics-in-r/#histogram">histogram</a>), or more formally (via the <a href="/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/#normality-test">Shapiro-Wilk test</a> for instance).</li>
<li><strong>Homoscedasticity of the residuals:</strong> The variance of the errors should be constant. There is a lack of homoscedasticity when the dispersion of the residuals increases with the predicted values (fitted values). This condition can be tested visually (by plotting the standardized residuals vs. the fitted values) or more formally (via the Breusch-Pagan test).</li>
<li><strong>No influential points:</strong> If the data contain <a href="/blog/outliers-detection-in-r/">outliers</a>, it is essential to identify them so that they <strong>do not</strong>, on their own, <strong>influence</strong> the results of the regression. Note that an outlier is not an issue <em>per se</em> if the point is in the alignment of the regression line for example because it does not influence the regression line. It becomes a problem in the context of linear regression if it influences in a substantial manner the estimates (and in particular the slope of the regression line). This can be tackled by identifying outliers (via the Cook’s distance<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> or the leverage index<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> for instance), and comparing the results with and without the potential outliers. Do the results remain the same with the two approaches? If yes, outliers are not really an issue in this case. If results are much different, you can use the Theil-Sen estimator, robust regression or quantile regression which are all more robust to outliers.</li>
</ol>
<p><em>Tip:</em> I remember the first 4 conditions thanks to the acronym “LINE”, for Linearity, Independence, Normality and Equality of variance.</p>
<p>If any of the condition is not met, the tests and the conclusions could be erroneous so it is best to avoid using and interpreting the model. If this is the case, sometimes the conditions can be met by transforming the data (e.g., logarithmic transformation, square or square root, Box-Cox transformation, etc.) or by adding a quadratic term to the model.</p>
<p>If it does not help, it could be worth thinking about removing some variables or adding other variables, or even considering other types of models such as non-linear models.</p>
<p>Keep in mind that in practice, <strong>conditions of application should be verified before drawing any conclusion</strong> based on the model. I refrain here from testing the conditions on our data because it will be covered in details in the context of multiple linear regression (see this <a href="/blog/multiple-linear-regression-made-simple/#conditions-of-application-1">section</a>).</p>
</div>
<div id="visualizations" class="section level2">
<h2>Visualizations</h2>
<p>If you are a frequent reader of the blog, you may know that I like to draw (simple but efficient) <a href="/tags/visualization/">visualizations</a> to illustrate my statistical analyses. Linear regression is not an exception.</p>
<p>There are numerous ways to visualize the relationship between the two variables of interest, but the easiest one I found so far is via the <code>visreg()</code> function from the package of the same name:</p>
<pre class="r"><code>library(visreg)
visreg(model)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>I like this approach for its simplicity—only a single line of code.</p>
<p>However, other elements could be displayed on the regression plot (for example the regression equation and the <span class="math inline">\(R^2\)</span>). This can easily be done with the <code>stat_regline_equation()</code> and <code>stat_cor()</code> functions from the <code>{ggpubr}</code> package:</p>
<pre class="r"><code># load necessary libraries
library(ggpubr)

# create plot with regression line, regression equation and R^2
ggplot(dat, aes(x = wt, y = mpg)) +
  geom_smooth(method = &quot;lm&quot;) +
  geom_point() +
  stat_regline_equation(label.x = 3, label.y = 32) + # for regression equation
  stat_cor(aes(label = after_stat(rr.label)), label.x = 3, label.y = 30) + # for R^2
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="multiple-linear-regression" class="section level1">
<h1>Multiple linear regression</h1>
<p>Now that you understand the principle behind simple linear regression and you know how to interpret the results, it is time to discuss about multiple linear regression.</p>
<p>We also start with the underlying principle of multiple linear regression, then show how to interpret the results, how to test the conditions of application and finish with more advanced topics.</p>
<div id="principle-1" class="section level2">
<h2>Principle</h2>
<p>Multiple linear regression is a generalization of simple linear regression, in the sense that this approach makes it possible to relate one variable with <strong>several variables</strong> through a linear function in its parameters.</p>
<p>Multiple linear regression is used to assess the relationship between two variables <strong>while taking into account the effect of other variables</strong>. By taking into account the effect of other variables, we cancel out the effect of these other variables in order to <strong>isolate</strong> and measure the relationship between the two variables of interest. This point is the main difference with simple linear regression.</p>
<p>To illustrate how to perform a multiple linear regression in R, we use the same dataset than the one used for simple linear regression (<code>mtcars</code>). Below a short preview:</p>
<pre class="r"><code>head(dat)</code></pre>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<p>We have seen that there is a significant and negative linear relationship between the distance a car can drive with a gallon and its weight (<span class="math inline">\(\widehat\beta_1 =\)</span> -5.34, <span class="math inline">\(p\)</span>-value &lt; 0.001).</p>
<p>However, one may wonder whether there are not in reality other factors that could explain a car’s fuel consumption.</p>
<p>To explore this, we can visualize the relationship between a car’s fuel consumption (<code>mpg</code>) together with its weight (<code>wt</code>), horsepower (<code>hp</code>) and displacement (<code>disp</code>) (engine displacement is the combined swept (or displaced) volume of air resulting from the up-and-down movement of pistons in the cylinders, usually the higher the more powerful the car):</p>
<pre class="r"><code>ggplot(dat) +
  aes(x = wt, y = mpg, colour = hp, size = disp) +
  geom_point() +
  scale_color_gradient() +
  labs(
    y = &quot;Miles per gallon&quot;,
    x = &quot;Weight (1000 lbs)&quot;,
    color = &quot;Horsepower&quot;,
    size = &quot;Displacement&quot;
  ) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>It seems that, in addition to the negative relationship between miles per gallon and weight, there is also:</p>
<ul>
<li>a negative relationship between miles/gallon and horsepower (lighter points, indicating more horsepower, tend to be more present in low levels of miles per gallon)</li>
<li>a negative relationship between miles/gallon and displacement (bigger points, indicating larger values of displacement, tend to be more present in low levels of miles per gallon).</li>
</ul>
<p>Therefore, we would like to evaluate the relation between the fuel consumption and the weight, but this time by adding information on the horsepower and displacement. By adding this additional information, we are able to <strong>capture only the direct relationship between miles/gallon and weight</strong> (the indirect effect due to horsepower and displacement is canceled out).</p>
<p>This is the whole point of multiple linear regression! In fact, in multiple linear regression, the estimated relationship between the dependent variable and an explanatory variable is an <strong>adjusted</strong> relationship, that is, free of the linear effects of the other explanatory variables.</p>
<p>Let’s illustrate this notion of adjustment by adding both horsepower and displacement in our linear regression model:</p>
<pre class="r"><code>model2 &lt;- lm(mpg ~ wt + hp + disp,
  data = dat
)

summary(model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + hp + disp, data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.891 -1.640 -0.172  1.061  5.861 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
## wt          -3.800891   1.066191  -3.565  0.00133 ** 
## hp          -0.031157   0.011436  -2.724  0.01097 *  
## disp        -0.000937   0.010350  -0.091  0.92851    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.639 on 28 degrees of freedom
## Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8083 
## F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</code></pre>
<p>We can see that now, the relationship between miles/gallon and weight is weaker in terms of slope (<span class="math inline">\(\widehat\beta_1 =\)</span> -3.8 now, against <span class="math inline">\(\widehat\beta_1 =\)</span> -5.34 when only the weight was considered).</p>
<p>The effect of weight on fuel consumption was adjusted according to the effect of horsepower and displacement. This is the remaining effect between miles/gallon and weight after the effects of horsepower and displacement have been taken into account. More detailed interpretations in this <a href="/blog/multiple-linear-regression-made-simple/#interpretations-of-coefficients-widehatbeta-1">section</a>.</p>
</div>
<div id="equation-1" class="section level2">
<h2>Equation</h2>
<p>Multiple linear regression models are defined by the equation</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon\]</span></p>
<p>It is similar than the equation of simple linear regression, except that there is more than one independent variables (<span class="math inline">\(X_1, X_2, \dots, X_p\)</span>).</p>
<p>Estimation of the parameters <span class="math inline">\(\beta_0, \dots, \beta_p\)</span> by the method of least squares is based on the same principle as that of simple linear regression, but applied to <span class="math inline">\(p\)</span> dimensions. It is thus no longer a question of finding the best line (the one which passes closest to the pairs of points (<span class="math inline">\(y_i, x_i\)</span>)), but finding the <span class="math inline">\(p\)</span>-dimensional plane which passes closest to the coordinate points (<span class="math inline">\(y_i, x_{i1}, \dots, x_{ip}\)</span>).</p>
<p>This is done by <strong><em>minimizing</em> the sum of the squares of the deviations of the points on the plane</strong>:</p>
<div class="float">
<img src="images/multiple-linear-regression-plane.png" style="width:100.0%" alt="Source: Thaddeussegura" />
<div class="figcaption">Source: Thaddeussegura</div>
</div>
</div>
<div id="interpretations-of-coefficients-widehatbeta-1" class="section level2">
<h2>Interpretations of coefficients <span class="math inline">\(\widehat\beta\)</span></h2>
<p>The least squares method results in an adjusted estimate of the coefficients. The term adjusted means <strong>after taking into account the linear effects</strong> of the other independent variables on the dependent variable, but also on the predictor variable.</p>
<p>In other words, the coefficient <span class="math inline">\(\beta_1\)</span> corresponds to the slope of the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> when the linear effects of the other explanatory variables (<span class="math inline">\(X_2, \dots, X_p\)</span>) have been removed, both at the level of the dependent variable <span class="math inline">\(Y\)</span> but also at the level of <span class="math inline">\(X_1\)</span>.</p>
<p>Applied to our model with weight, horsepower and displacement as independent variables, we have:</p>
<pre class="r"><code>summary(model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + hp + disp, data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.891 -1.640 -0.172  1.061  5.861 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
## wt          -3.800891   1.066191  -3.565  0.00133 ** 
## hp          -0.031157   0.011436  -2.724  0.01097 *  
## disp        -0.000937   0.010350  -0.091  0.92851    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.639 on 28 degrees of freedom
## Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8083 
## F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</code></pre>
<p>The table <code>Coefficients</code> gives the estimate for each parameter (column <code>Estimate</code>), together with the <span class="math inline">\(p\)</span>-value of the nullity of the parameter (column <code>Pr(&gt;|t|)</code>).</p>
<p>The hypotheses are the same as for simple linear regression, that is:</p>
<ul>
<li><span class="math inline">\(H_0 : \beta_j = 0\)</span></li>
<li><span class="math inline">\(H_1 : \beta_j \ne 0\)</span></li>
</ul>
<p>The test of <span class="math inline">\(\beta_j = 0\)</span> is equivalent to testing the hypothesis: is the dependent variable associated with the independent variable studied, all other things being equal, that is to say, at constant level of the other independent variables.</p>
<p>In other words:</p>
<ul>
<li>the test of <span class="math inline">\(\beta_1 = 0\)</span> corresponds to testing the hypothesis: is fuel consumption associated with a car’s weight, at a constant level of horsepower and displacement</li>
<li>the test of <span class="math inline">\(\beta_2 = 0\)</span> corresponds to testing the hypothesis: is fuel consumption associated with horsepower, at a constant level of weight and displacement</li>
<li>the test of <span class="math inline">\(\beta_3 = 0\)</span> corresponds to testing the hypothesis: is fuel consumption associated with displacement, at a constant level of weight and displacement</li>
<li>(for the sake of completeness: the test of <span class="math inline">\(\beta_0 = 0\)</span> corresponds to testing the hypothesis: is miles/gallon different from 0 when weight, horsepower and displacement are equal to 0)</li>
</ul>
<p>In practice, we usually check the conditions of application <em>before</em> interpreting the coefficients (because if they are not respected, results may be biased). In this article, however, I present the interpretations before testing the conditions because the point is to show how to interpret the results, and less about finding a valid model.</p>
<p>Based on the output of our model, we conclude that:</p>
<ul>
<li>There is a significant and negative relationship between miles/gallon and weight, <strong>all else being equal</strong>. So for an increase of one unit in the weight (that is, an increase of 1000 lbs), the number of miles/gallon decreases, on average, by 3.8, for a constant level of horsepower and displacement (<span class="math inline">\(p\)</span>-value = 0.001).</li>
<li>There is a significant and negative relationship between miles/gallon and horsepower, all else being equal. So for an increase of one unit of horsepower, the distance traveled with a gallon decreases, on average, by 0.03 mile, for a constant level of weight and displacement (<span class="math inline">\(p\)</span>-value = 0.011).</li>
<li>We do not reject the hypothesis of no relationship between miles/gallon and displacement when weight and horsepower stay constant (because <span class="math inline">\(p\)</span>-value = 0.929 &gt; 0.05).</li>
<li>(For completeness but it should be interpreted only when it makes sense: for a weight, horsepower and displacement = 0, we can expect that a car has, on average, a fuel consumption of 37.11 miles/gallon (<span class="math inline">\(p\)</span>-value &lt; 0.001). See a more useful interpretation of the intercept when the independent variables are centered in this <a href="/blog/multiple-linear-regression-made-simple/#another-interpretation-of-the-intercept">section</a>.)</li>
</ul>
<p>This is how to interpret quantitative independent variables. <strong>Interpreting qualitative independent variables</strong> is slightly different in the sense that it quantifies the effect of a level in comparison with the reference level, sill all else being equal.</p>
<p>So it compares the different groups (formed by the different levels of the categorical variable) in terms of the dependent variable (this is why linear regression can be seen as an extension to the t-test and ANOVA).</p>
<p>For the illustration, we model the fuel consumption (<code>mpg</code>) on the weight (<code>wt</code>) and the shape of the engine (<code>vs</code>). The variable <code>vs</code> has two levels: V-shaped (the <a href="/blog/data-manipulation-in-r/#change-reference-level">reference level</a>) and straight engine.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<pre class="r"><code>## Recoding dat$vs
library(forcats)
dat$vs &lt;- as.character(dat$vs)
dat$vs &lt;- fct_recode(dat$vs,
  &quot;V-shaped&quot; = &quot;0&quot;,
  &quot;Straight&quot; = &quot;1&quot;
)

model3 &lt;- lm(mpg ~ wt + vs,
  data = dat
)

summary(model3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + vs, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7071 -2.4415 -0.3129  1.4319  6.0156 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  33.0042     2.3554  14.012 1.92e-14 ***
## wt           -4.4428     0.6134  -7.243 5.63e-08 ***
## vsStraight    3.1544     1.1907   2.649   0.0129 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.78 on 29 degrees of freedom
## Multiple R-squared:  0.801,	Adjusted R-squared:  0.7873 
## F-statistic: 58.36 on 2 and 29 DF,  p-value: 6.818e-11</code></pre>
<p>Based on the output of our model, we conclude that:</p>
<ul>
<li>For a V-shaped engine and for an increase of one unit in the weight (that is, an increase of 1000 lbs), the number of miles/gallon decreases, on average, by 4.44 (<span class="math inline">\(p\)</span>-value &lt; 0.001).</li>
<li>The distance traveled with a gallon of fuel increases by, on average, 3.15 miles <strong>when the engine is straight compared to a V-shaped engine</strong>, for a constant weight (<span class="math inline">\(p\)</span>-value = 0.013).</li>
<li>(For completeness but it should be interpreted only when it makes sense: for a weight = 0 and a V-shaped engine, we can expect that the car has, on average, a fuel consumption of 33 miles/gallon (<span class="math inline">\(p\)</span>-value &lt; 0.001). See a more useful interpretation of the intercept when the independent variables are centered in this <a href="/blog/multiple-linear-regression-made-simple/#another-interpretation-of-the-intercept">section</a>.)</li>
</ul>
</div>
<div id="conditions-of-application-1" class="section level2">
<h2>Conditions of application</h2>
<p>As for simple linear regression, multiple linear regression requires some conditions of application for the model to be usable and the results to be interpretable. Conditions for simple linear regression also apply to multiple linear regression, that is:</p>
<ol style="list-style-type: decimal">
<li><strong>Linearity</strong> of the relationships between the dependent and independent variables<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></li>
<li><strong>Independence</strong> of the observations</li>
<li><strong>Normality</strong> of the residuals</li>
<li><strong>Homoscedasticity</strong> of the residuals</li>
<li><strong>No influential points</strong> (<a href="/blog/outliers-detection-in-r/">outliers</a>)</li>
</ol>
<p>But there is one more condition for multiple linear regression:</p>
<ol start="6" style="list-style-type: decimal">
<li><strong>No multicollinearity:</strong> Multicollinearity arises when there is a strong linear <strong><a href="/blog/correlation-coefficient-and-correlation-test-in-r/">correlation</a> between the independent variables</strong>, conditional on the other variables in the model. It is important to check it because it may lead to an imprecision or an instability of the estimated parameters when a variable changes. It can be assessed by studying the correlation between each pair of independent variables, or even better, by computing the variance inflation factor (VIF). The VIF measures how much the variance of an estimated regression coefficient increases, relative to a situation in which the explanatory variables are strictly independent. A high value of VIF is a sign of multicollinearity (the threshold is generally admitted at 5 or 10 depending on the domain). The easiest way to reduce the VIF is to remove some correlated independent variables, or eventually to <a href="/blog/data-manipulation-in-r/#scale">standardize</a> the data.</li>
</ol>
<p>You will often see that these conditions are verified by running <code>plot(model, which = 1:6)</code> and it is totally correct. However, I recently discovered the <code>check_model()</code> function from the <code>{performance}</code> package which tests these conditions all at the same time (and let’s be honest, in a more elegant way).<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p>Applied on our <code>model2</code> with miles/gallon as dependent variable, and weight, horsepower and displacement as independent variables, we have:</p>
<pre class="r"><code># install.packages(&quot;performance&quot;)
# install.packages(&quot;see&quot;)
library(performance)

check_model(model2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>In addition to testing all conditions at the same time, it also gives insight on how to interpret the different diagnostic plots and what you should expect (see in the subtitles of each plot).</p>
<p>Based on these diagnostic plots, we see that:</p>
<ul>
<li>Homogeneity of variance (middle left plot) is respected</li>
<li>Multicollinearity (bottom left plot) is not an issue (I tend to use the threshold of 10 for VIF, and all of them are below 10)<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></li>
<li>There is no influential points (middle right plot)</li>
<li>Normality of the residuals (bottom right plot) is also not perfect due to 3 points deviating from the reference line but it still seems acceptable to me. In any case, the number of observations is large enough given the number of parameters<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> and given the small deviation from normality so tests on the coefficients are (approximately) valid whether the error follows a normal distribution or not</li>
<li>Linearity (top right plot) is not perfect so let’s check each independent variable separately:</li>
</ul>
<pre class="r"><code># weight
ggplot(dat, aes(x = wt, y = mpg)) +
  geom_point() +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code># horsepower
ggplot(dat, aes(x = hp, y = mpg)) +
  geom_point() +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-2.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code># displacement
ggplot(dat, aes(x = disp, y = mpg)) +
  geom_point() +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-3.png" width="100%" style="display: block; margin: auto;" /></p>
<p>It seems that the relationship between miles/gallon and horsepower is not linear, which could be the main component of the slight linearity defect of the model.</p>
<p>To improve linearity, the variable could be removed, a transformation could be applied (logarithmic and/or squared for instance) or a quadratic term could be added to the model.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> If this does not fix the issue of linearity, other types of models could be considered.</p>
<p>If you want to read more about these conditions of applications and how to deal with them, here is a very complete <a href="http://quantpsych.net/stats_modeling/diagnostics.html">chapter</a> on diagnostics for linear models written by Prof. Dustin Fife.</p>
<p>For the sake of easiness and for illustrative purposes, we assume linearity for the rest of the article.</p>
<p>When the conditions of application are met, we usually say that the model is valid. But not all valid models are <em>good</em> models. The next section deals with model selection.</p>
</div>
<div id="how-to-choose-a-good-linear-model" class="section level2">
<h2>How to choose a good linear model?</h2>
<p>A model which satisfies the conditions of application is the minimum requirement, but you will likely find several models that meet this criteria. So one may wonder <strong>how to choose between different models</strong> that are all valid?</p>
<p>The three most common tools to select a good linear model are according to:</p>
<ol style="list-style-type: decimal">
<li>the <span class="math inline">\(p\)</span>-value associated to the model,</li>
<li>the coefficient of determination <span class="math inline">\(R^2\)</span> and</li>
<li>the Akaike Information Criterion</li>
</ol>
<p>The approaches are detailed in the next sections. Note that the first two are applicable to simple and multiple linear regression, whereas the third is only applicable to multiple linear regression.</p>
<div id="p-value-associated-to-the-model" class="section level3">
<h3><span class="math inline">\(P\)</span>-value associated to the model</h3>
<p>Before interpreting the estimates of a model, it is a good practice to first check the <span class="math inline">\(p\)</span>-value associated to the model. This <span class="math inline">\(p\)</span>-value indicates if the model is <strong>better than a model with only the intercept</strong>.</p>
<p>The hypotheses of the test (called F-test) are:</p>
<ul>
<li><span class="math inline">\(H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0\)</span></li>
<li><span class="math inline">\(H_1:\)</span> at least one coefficient <span class="math inline">\(\beta \ne 0\)</span></li>
</ul>
<p>This <span class="math inline">\(p\)</span>-value can be found at the bottom of the <code>summary()</code> output:</p>
<pre class="r"><code>summary(model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + hp + disp, data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.891 -1.640 -0.172  1.061  5.861 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
## wt          -3.800891   1.066191  -3.565  0.00133 ** 
## hp          -0.031157   0.011436  -2.724  0.01097 *  
## disp        -0.000937   0.010350  -0.091  0.92851    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.639 on 28 degrees of freedom
## Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8083 
## F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</code></pre>
<p>The <span class="math inline">\(p\)</span>-value = 8.65e-11. The null hypothesis is rejected, so we conclude that our model is better than a model with only the intercept because at least one coefficient <span class="math inline">\(\beta\)</span> is significantly different from 0.</p>
<p>If this <span class="math inline">\(p\)</span>-value &gt; 0.05 for one of your model, it means that none of the variables you selected help in explaining the dependent variable. In other words, you should completely forget about this model because it cannot do better than simply taking the mean of the dependent variable.</p>
</div>
<div id="coefficient-of-determination-r2" class="section level3">
<h3>Coefficient of determination <span class="math inline">\(R^2\)</span></h3>
<p>The coefficient of determination, <span class="math inline">\(R^2\)</span>, is a measure of the <strong>goodness of fit of the model</strong>. It measures the proportion of the total variability that is explained by the model, or how well the model fits the data.</p>
<p><span class="math inline">\(R^2\)</span> varies between 0 and 1:</p>
<ul>
<li><span class="math inline">\(R^2 = 0\)</span>: the model explains nothing</li>
<li><span class="math inline">\(R^2 = 1\)</span>: the model explains everything</li>
<li><span class="math inline">\(0 &lt; R^2 &lt; 1\)</span>: the model explains part of the variability</li>
<li>the higher the <span class="math inline">\(R^2\)</span>, the better the model explains the dependent variable. As a rule of thumb, a <span class="math inline">\(R^2 &gt; 0.7\)</span> indicates a good fit of the model<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></li>
</ul>
<p>Note that in a simple linear regression model, the coefficient of determination is equal to the square of the Pearson <a href="/blog/correlation-coefficient-and-correlation-test-in-r/">correlation coefficient</a>:</p>
<p><span class="math display">\[R^2 = corr(X, Y)^2\]</span></p>
<p>Applied on our <code>model2</code> with miles/gallon as dependent variable, and weight, horsepower and displacement as independent variables, we have:</p>
<pre class="r"><code>summary(model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + hp + disp, data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.891 -1.640 -0.172  1.061  5.861 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
## wt          -3.800891   1.066191  -3.565  0.00133 ** 
## hp          -0.031157   0.011436  -2.724  0.01097 *  
## disp        -0.000937   0.010350  -0.091  0.92851    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.639 on 28 degrees of freedom
## Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8083 
## F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</code></pre>
<p><span class="math inline">\(R^2\)</span> is displayed at the bottom of the <code>summary()</code> output or can be extracted with <code>summary(model2)$r.squared</code>.</p>
<p><span class="math inline">\(R^2\)</span> for this model is 0.8268, which means that 82.68% of the variability of the distance traveled with a gallon is explained by the weight, horsepower and displacement of the car. The relatively high <span class="math inline">\(R^2\)</span> means that the weight, horsepower and displacement of a car are good characteristics to explain the distance it can drive with a gallon of fuel.</p>
<p>Note that if you want to compare models with different number of independent variables, it is best to refer to the adjusted <span class="math inline">\(R^2\)</span> (= 0.8083 here).</p>
<p>Indeed, adding variables to the model cannot make the <span class="math inline">\(R^2\)</span> to decrease, even if the variables are not related to the dependent variables (so the <span class="math inline">\(R^2\)</span> will artificially increase when adding variables to the model, or at least stay constant). Therefore, the adjusted <span class="math inline">\(R^2\)</span> takes into account the complexity of the model (the number of variables) by penalizing for additional variables, so it is a compromise between goodness of fit and parsimony.</p>
</div>
<div id="parsimony" class="section level3">
<h3>Parsimony</h3>
<p>A <strong>parsimonious model (few variables) is usually preferred</strong> over a complex model (many variables). There are two ways to obtain a parsimonious model from a model with many independent variables:</p>
<ol style="list-style-type: decimal">
<li>We can <strong>iteratively remove the independent variable least significantly related to the dependent variable</strong> (i.e., the one with the highest <span class="math inline">\(p\)</span>-value in an <a href="/blog/multiple-linear-regression-made-simple/#overall-effect-of-categorical-variables">analysis of variance table</a>) until all of them are significantly associated to the response variable, or</li>
<li>We can select the model based on the <strong>Akaike Information Criterion (AIC)</strong>. AIC expresses a desire to fit the model with the smallest number of coefficients possible and allows to compare models. According to this criterion, the best model is the one with the lowest AIC. This criterion is based on a compromise between the quality of the fit and its complexity. We usually start from a global model with many independent variables, and the procedure (referred as stepwise algorithm)<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> automatically compares models then selects the best one according to the AIC.</li>
</ol>
<p>We show how to do the second option in R. For the illustration, we start with a model with all variables in the dataset as independent variables (do not forget to transform the factor variables first):</p>
<pre class="r"><code>## vs has already been transformed into factor
## so only am is transformed here

## Recoding dat$vs
library(forcats)
dat$am &lt;- as.character(dat$am)
dat$am &lt;- fct_recode(dat$am,
  &quot;Automatic&quot; = &quot;0&quot;,
  &quot;Manual&quot; = &quot;1&quot;
)

model4 &lt;- lm(mpg ~ .,
  data = dat
)

model4 &lt;- step(model4, trace = FALSE)</code></pre>
<p>(<em>Tip:</em> The formula <code>mpg ~ .</code> is a shortcut to consider all variables present in the dataset as independent variables, except the one that has been specified as the dependent variable (<code>mpg</code> here)).</p>
<p>The model that has been selected according to this criterion is the following:</p>
<pre class="r"><code>summary(model4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + qsec + am, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4811 -1.5555 -0.7257  1.4110  4.6610 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   9.6178     6.9596   1.382 0.177915    
## wt           -3.9165     0.7112  -5.507 6.95e-06 ***
## qsec          1.2259     0.2887   4.247 0.000216 ***
## amManual      2.9358     1.4109   2.081 0.046716 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.459 on 28 degrees of freedom
## Multiple R-squared:  0.8497,	Adjusted R-squared:  0.8336 
## F-statistic: 52.75 on 3 and 28 DF,  p-value: 1.21e-11</code></pre>
<p>Be careful when using an automatic procedure because, even though it is the best model that is selected, it is based:</p>
<ul>
<li>on a single criterion (AIC in this case), but more importantly;</li>
<li>it is based on some set of mathematical rules, which means that industry knowledge or human expertise is not taken into consideration.</li>
</ul>
<p>I believe that this kind of automatic procedure for model’s selection is a good starting point, but I also believe that the final model should always be checked and tested against other models to make sure it makes sense in practice (apply common sense).</p>
<p>Last but not least, do not forget to also verify the <a href="/blog/multiple-linear-regression-made-simple/#conditions-of-application-1">conditions of application</a> because the stepwise procedure does not guarantee that they are respected.</p>
</div>
</div>
<div id="visualizations-1" class="section level2">
<h2>Visualizations</h2>
<p>There are many ways to visualize results of a linear regression. The easiest ones I am aware of are:</p>
<ol style="list-style-type: decimal">
<li><code>visreg()</code> illustrates the relationships between the dependent and independent variables in different plots (one for each independent variable unless you specify which relationship you want to illustrate):</li>
</ol>
<pre class="r"><code>library(visreg)

visreg(model4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-1.png" width="100%" style="display: block; margin: auto;" /><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-2.png" width="100%" style="display: block; margin: auto;" /><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-3.png" width="100%" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: decimal">
<li><code>ggcoefstats()</code> illustrates the results in one single plot, with many statistical details:</li>
</ol>
<pre class="r"><code>library(ggstatsplot)

ggcoefstats(model4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-22-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>In this plot:</p>
<ul>
<li>when the solid line does not cross the vertical dashed line, the estimates is significantly different from 0 at the 5% significance level (i.e., <span class="math inline">\(p\)</span>-value &lt; 0.05)</li>
<li>furthermore, a point to the right (left) of the vertical dashed line means that there is a positive (negative) relationship between the two variables</li>
<li>the more extreme the point, the stronger the relationship</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><code>plot_summs()</code> which also illustrates the results but in a more concise way:</li>
</ol>
<pre class="r"><code>library(jtools)
library(ggstance)

plot_summs(model4,
  omit.coefs = NULL
)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-23-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The advantage of this approach is that it is possible to compare coefficients of multiple models simultaneously (particularly interesting when the models are nested):</p>
<pre class="r"><code>model4bis &lt;- lm(mpg ~ wt + qsec + am + hp,
  data = dat
)

plot_summs(model4,
  model4bis,
  omit.coefs = NULL
)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="to-go-further" class="section level2">
<h2>To go further</h2>
<p>Below some more advanced topics related to linear regression. Feel free to comment at the end of the article if you believe I missed an important one.</p>
<div id="print-models-parameters" class="section level3">
<h3>Print model’s parameters</h3>
<p>Thanks to the <code>model_parameters()</code> function from the <code>{parameters}</code> package, you can print a summary of the model in a nicely formatted way to make the output more readable:</p>
<pre class="r"><code>library(parameters)

model_parameters(model4, summary = TRUE)</code></pre>
<pre><code>## Parameter   | Coefficient |   SE |         95% CI | t(28) |      p
## ------------------------------------------------------------------
## (Intercept) |        9.62 | 6.96 | [-4.64, 23.87] |  1.38 | 0.178 
## wt          |       -3.92 | 0.71 | [-5.37, -2.46] | -5.51 | &lt; .001
## qsec        |        1.23 | 0.29 | [ 0.63,  1.82] |  4.25 | &lt; .001
## am [Manual] |        2.94 | 1.41 | [ 0.05,  5.83] |  2.08 | 0.047 
## 
## Model: mpg ~ wt + qsec + am (32 Observations)
## Residual standard deviation: 2.459 (df = 28)
## R2: 0.850; adjusted R2: 0.834</code></pre>
<p>And if you are using <a href="/blog/getting-started-in-r-markdown/">R Markdown</a>, you can use the <code>print_html()</code> function to get a compact and yet comprehensive summary table in your HTML file:</p>
<pre class="r"><code>library(gt)

print_html(model_parameters(model4, summary = TRUE))</code></pre>
<div id="yywrdylzvd" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#yywrdylzvd table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#yywrdylzvd thead, #yywrdylzvd tbody, #yywrdylzvd tfoot, #yywrdylzvd tr, #yywrdylzvd td, #yywrdylzvd th {
  border-style: none;
}

#yywrdylzvd p {
  margin: 0;
  padding: 0;
}

#yywrdylzvd .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 100%;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#yywrdylzvd .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#yywrdylzvd .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#yywrdylzvd .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#yywrdylzvd .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#yywrdylzvd .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#yywrdylzvd .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#yywrdylzvd .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#yywrdylzvd .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#yywrdylzvd .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#yywrdylzvd .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#yywrdylzvd .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#yywrdylzvd .gt_spanner_row {
  border-bottom-style: hidden;
}

#yywrdylzvd .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#yywrdylzvd .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#yywrdylzvd .gt_from_md > :first-child {
  margin-top: 0;
}

#yywrdylzvd .gt_from_md > :last-child {
  margin-bottom: 0;
}

#yywrdylzvd .gt_row {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#yywrdylzvd .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#yywrdylzvd .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#yywrdylzvd .gt_row_group_first td {
  border-top-width: 2px;
}

#yywrdylzvd .gt_row_group_first th {
  border-top-width: 2px;
}

#yywrdylzvd .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#yywrdylzvd .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#yywrdylzvd .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#yywrdylzvd .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#yywrdylzvd .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#yywrdylzvd .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#yywrdylzvd .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#yywrdylzvd .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#yywrdylzvd .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#yywrdylzvd .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#yywrdylzvd .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#yywrdylzvd .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#yywrdylzvd .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#yywrdylzvd .gt_left {
  text-align: left;
}

#yywrdylzvd .gt_center {
  text-align: center;
}

#yywrdylzvd .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#yywrdylzvd .gt_font_normal {
  font-weight: normal;
}

#yywrdylzvd .gt_font_bold {
  font-weight: bold;
}

#yywrdylzvd .gt_font_italic {
  font-style: italic;
}

#yywrdylzvd .gt_super {
  font-size: 65%;
}

#yywrdylzvd .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#yywrdylzvd .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#yywrdylzvd .gt_indent_1 {
  text-indent: 5px;
}

#yywrdylzvd .gt_indent_2 {
  text-indent: 10px;
}

#yywrdylzvd .gt_indent_3 {
  text-indent: 15px;
}

#yywrdylzvd .gt_indent_4 {
  text-indent: 20px;
}

#yywrdylzvd .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
  <thead>
    
    <tr class="gt_col_headings">
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1" scope="col" id="Parameter">Parameter</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="Coefficient">Coefficient</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="SE">SE</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="95% CI">95% CI</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="t(28)">t(28)</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="p">p</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td headers="Parameter" class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #d3d3d3;">(Intercept)</td>
<td headers="Coefficient" class="gt_row gt_center">9.62</td>
<td headers="SE" class="gt_row gt_center">6.96</td>
<td headers="95% CI" class="gt_row gt_center">(-4.64, 23.87)</td>
<td headers="t(28)" class="gt_row gt_center">1.38</td>
<td headers="p" class="gt_row gt_center">0.178 </td></tr>
    <tr><td headers="Parameter" class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #d3d3d3;">wt</td>
<td headers="Coefficient" class="gt_row gt_center">-3.92</td>
<td headers="SE" class="gt_row gt_center">0.71</td>
<td headers="95% CI" class="gt_row gt_center">(-5.37, -2.46)</td>
<td headers="t(28)" class="gt_row gt_center">-5.51</td>
<td headers="p" class="gt_row gt_center">&lt; .001</td></tr>
    <tr><td headers="Parameter" class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #d3d3d3;">qsec</td>
<td headers="Coefficient" class="gt_row gt_center">1.23</td>
<td headers="SE" class="gt_row gt_center">0.29</td>
<td headers="95% CI" class="gt_row gt_center">(0.63, 1.82)</td>
<td headers="t(28)" class="gt_row gt_center">4.25</td>
<td headers="p" class="gt_row gt_center">&lt; .001</td></tr>
    <tr><td headers="Parameter" class="gt_row gt_left" style="border-right-width: 1px; border-right-style: solid; border-right-color: #d3d3d3;">am (Manual)</td>
<td headers="Coefficient" class="gt_row gt_center">2.94</td>
<td headers="SE" class="gt_row gt_center">1.41</td>
<td headers="95% CI" class="gt_row gt_center">(0.05, 5.83)</td>
<td headers="t(28)" class="gt_row gt_center">2.08</td>
<td headers="p" class="gt_row gt_center">0.047 </td></tr>
  </tbody>
  <tfoot class="gt_sourcenotes">
    <tr>
      <td class="gt_sourcenote" colspan="6">Model: mpg ~ wt + qsec + am (32 Observations)<br>Residual standard deviation: 2.459 (df = 28)<br>R2: 0.850; adjusted R2: 0.834</td>
    </tr>
  </tfoot>
  
</table>
</div>
<p><br></p>
</div>
<div id="automatic-reporting" class="section level3">
<h3>Automatic reporting</h3>
<p>The <code>report()</code> function from the package of the same name allows to automatically produces reports of models according to best practices guidelines:</p>
<pre class="r"><code>library(report)

report(model4)[1]</code></pre>
<pre><code>## [1] &quot;We fitted a linear model (estimated using OLS) to predict mpg with wt, qsec and am (formula: mpg ~ wt + qsec + am). The model explains a statistically significant and substantial proportion of variance (R2 = 0.85, F(3, 28) = 52.75, p &lt; .001, adj. R2 = 0.83). The model&#39;s intercept, corresponding to wt = 0, qsec = 0 and am = Automatic, is at 9.62 (95% CI [-4.64, 23.87], t(28) = 1.38, p = 0.178). Within this model:\n\n  - The effect of wt is statistically significant and negative (beta = -3.92, 95% CI [-5.37, -2.46], t(28) = -5.51, p &lt; .001; Std. beta = -0.64, 95% CI [-0.87, -0.40])\n  - The effect of qsec is statistically significant and positive (beta = 1.23, 95% CI [0.63, 1.82], t(28) = 4.25, p &lt; .001; Std. beta = 0.36, 95% CI [0.19, 0.54])\n  - The effect of am [Manual] is statistically significant and positive (beta = 2.94, 95% CI [0.05, 5.83], t(28) = 2.08, p = 0.047; Std. beta = 0.49, 95% CI [7.59e-03, 0.97])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation.&quot;</code></pre>
<p>Note that the function also works for dataframes, <a href="/blog/what-statistical-test-should-i-do/">statistical tests</a> and other models.</p>
</div>
<div id="predictions" class="section level3">
<h3>Predictions</h3>
<p>Linear regression is also very often used for <strong>predictive purposes</strong>. Confidence and prediction intervals for <strong>new data</strong> can be computed with the <code>predict()</code> function.</p>
<p>Suppose we want to predict the miles/gallon for a car with a manual transmission, weighting 3000 lbs and which drives a quarter of a mile (<code>qsec</code>) in 18 seconds:</p>
<pre class="r"><code># confidence interval for new data
predict(model4,
  new = data.frame(wt = 3, qsec = 18, am = &quot;Manual&quot;),
  interval = &quot;confidence&quot;,
  level = .95
)</code></pre>
<pre><code>##        fit      lwr    upr
## 1 22.87005 21.09811 24.642</code></pre>
<pre class="r"><code># prediction interval for new data
predict(model4,
  new = data.frame(wt = 3, qsec = 18, am = &quot;Manual&quot;),
  interval = &quot;prediction&quot;,
  level = .95
)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 22.87005 17.53074 28.20937</code></pre>
<p>Based on our model, it is expected that this car will drive 22.87 miles with a gallon.</p>
<p>The difference between the confidence and prediction interval is that:</p>
<ul>
<li>a <strong>confidence</strong> interval gives the predicted value for the <strong>mean</strong> of <span class="math inline">\(Y\)</span> for a new observation, whereas</li>
<li>a <strong>prediction</strong> interval gives the predicted value for an <strong>individual</strong> <span class="math inline">\(Y\)</span> for a new observation.</li>
</ul>
<p>The prediction interval is wider than the confidence interval to account for the <strong>additional uncertainty due to predicting an individual response</strong>, and not the mean, for a given value of <span class="math inline">\(X\)</span>.</p>
</div>
<div id="linear-hypothesis-tests" class="section level3">
<h3>Linear hypothesis tests</h3>
<p>Linear hypothesis tests make it possible to generalize the F-test mentioned in this <a href="/blog/multiple-linear-regression-made-simple/#p-value-associated-to-the-model">section</a>, while offering the possibility to perform either tests of comparison of coefficients, or tests of equality of linear combinations of coefficients.</p>
<p>For example, to test the linear constraint:</p>
<ul>
<li><span class="math inline">\(H_0: \beta_1 = \beta_2 = 0\)</span></li>
<li><span class="math inline">\(H_1:\)</span> not <span class="math inline">\(H_0\)</span></li>
</ul>
<p>we use the <code>linearHypothesis()</code> function of the <code>{car}</code> package as follows:</p>
<pre class="r"><code>library(car)
linearHypothesis(model4, c(&quot;wt = 0&quot;, &quot;qsec = 0&quot;))</code></pre>
<pre><code>## Linear hypothesis test
## 
## Hypothesis:
## wt = 0
## qsec = 0
## 
## Model 1: restricted model
## Model 2: mpg ~ wt + qsec + am
## 
##   Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)    
## 1     30 720.90                                 
## 2     28 169.29  2    551.61 45.618 1.55e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We reject the null hypothesis and we conclude that at least one of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> is different from 0 (<span class="math inline">\(p\)</span>-value = 1.55e-09).</p>
</div>
<div id="overall-effect-of-categorical-variables" class="section level3">
<h3>Overall effect of categorical variables</h3>
<p>When the independent variables are categorical with <span class="math inline">\(k\)</span> categories, the regression table provides <span class="math inline">\(k-1\)</span> <span class="math inline">\(p\)</span>-values:</p>
<pre class="r"><code>model5 &lt;- lm(mpg ~ vs + am + as.factor(cyl),
  data = dat
)

summary(model5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ vs + am + as.factor(cyl), data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.2821 -1.4402  0.0391  1.8845  6.2179 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       22.809      2.928   7.789 2.24e-08 ***
## vsStraight         1.708      2.235   0.764  0.45135    
## amManual           3.165      1.528   2.071  0.04805 *  
## as.factor(cyl)6   -5.399      1.837  -2.938  0.00668 ** 
## as.factor(cyl)8   -8.161      2.892  -2.822  0.00884 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.097 on 27 degrees of freedom
## Multiple R-squared:  0.7701,	Adjusted R-squared:  0.736 
## F-statistic: 22.61 on 4 and 27 DF,  p-value: 2.741e-08</code></pre>
<p>The variables <code>vs</code> and <code>am</code> have 2 levels so one is displayed in the regression output. The variable <code>cyl</code> has 3 levels (4, 6 and 8) so 2 of them are displayed. The overall effect of <code>vs</code> and <code>am</code> are reported in the <code>Pr(&gt;|t|)</code> column, but not the <strong>overall</strong> effect of <code>cyl</code> because there are more than 2 levels for this variable.</p>
<p>To get the <span class="math inline">\(p\)</span>-value of the overall effect of a categorical variable, we need to get an analysis of variance table via the <code>Anova()</code> function from the <code>{car}</code> package:<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></p>
<pre class="r"><code>library(car)
Anova(model5)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: mpg
##                 Sum Sq Df F value  Pr(&gt;F)  
## vs               5.601  1  0.5841 0.45135  
## am              41.122  1  4.2886 0.04805 *
## as.factor(cyl)  94.591  2  4.9324 0.01493 *
## Residuals      258.895 27                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From this analysis of variance table, we conclude that:</p>
<ul>
<li><code>vs</code> is not significantly associated with <code>mpg</code> (<span class="math inline">\(p\)</span>-value = 0.451)</li>
<li><code>am</code> and <code>cyl</code> are significantly associated with <code>mpg</code> (<span class="math inline">\(p\)</span>-values &lt; 0.05)</li>
</ul>
</div>
<div id="interaction" class="section level3">
<h3>Interaction</h3>
<p>So far we have covered multiple linear regression without any interaction.</p>
<p>There is an <strong>interaction</strong> effect between factors A and B <strong>if the effect of factor A on the response depends on the level taken by factor B</strong>.</p>
<p>In R, interaction can be added as follows:</p>
<pre class="r"><code>model6 &lt;- lm(mpg ~ wt + am + wt:am,
  data = dat
)

# Or in a shorter way:
model6 &lt;- lm(mpg ~ wt * am,
  data = dat
)

summary(model6)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt * am, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6004 -1.5446 -0.5325  0.9012  6.0909 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  31.4161     3.0201  10.402 4.00e-11 ***
## wt           -3.7859     0.7856  -4.819 4.55e-05 ***
## amManual     14.8784     4.2640   3.489  0.00162 ** 
## wt:amManual  -5.2984     1.4447  -3.667  0.00102 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.591 on 28 degrees of freedom
## Multiple R-squared:  0.833,	Adjusted R-squared:  0.8151 
## F-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11</code></pre>
<p>From the output we conclude that there is an interaction between the weight and the transmission (<span class="math inline">\(p\)</span>-value = 0.00102). This means that the effect of the weight on the distance traveled with a gallon <strong>depends on the transmission type</strong>.</p>
<p>The easiest way to handle interaction is to visualize the relationship for each level of the categorical variable:</p>
<pre class="r"><code>visreg(model6, &quot;wt&quot;, by = &quot;am&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-33-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We see that the relationship between weight and miles/gallon is stronger (the slope is steeper) for cars with a manual transmission compared to cars with an automatic transmission.</p>
<p>This is a good example to illustrate the point that when studying a relationship between two variables, say <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, if one also has data for other variables which are potentially associated with both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, it is important to include them in the regression and to analyze the relationship <strong>conditionally on these variables</strong>.</p>
<p>Omitting some variables that should be included in the model may lead to erroneous and misleading conclusions, up to the point that the relationship is completely reversed (a phenomenon referred as <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox" target="_blank">Simpson’s paradox</a>).</p>
</div>
</div>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>In this article, we started with a reminder of <a href="/blog/multiple-linear-regression-made-simple/#simple-linear-regression-reminder">simple linear regression</a> and in particular its <a href="/blog/multiple-linear-regression-made-simple/#principle">principle</a> and how to <a href="/blog/multiple-linear-regression-made-simple/#interpretations-of-coefficients-widehatbeta">interpret the results</a>.</p>
<p>This laid the foundations for a better understanding of <a href="/blog/multiple-linear-regression-made-simple/#multiple-linear-regression">multiple linear regression</a>. After explaining its <a href="/blog/multiple-linear-regression-made-simple/#principle-1">principle</a>, we showed how to <a href="/blog/multiple-linear-regression-made-simple/#interpretations-of-coefficients-widehatbeta-1">interpret the output</a> and how to choose a <a href="/blog/multiple-linear-regression-made-simple/#how-to-choose-a-good-linear-model">good linear model</a>. We then mentioned a couple of <a href="/blog/multiple-linear-regression-made-simple/#visualizations-1">visualizations</a> and finished the article with some more <a href="/blog/multiple-linear-regression-made-simple/#to-go-further">advanced topics</a>.</p>
<p>Thanks for reading.</p>
<p>I hope this article helped you to understand better linear regression and gave you the confidence to do your own linear regressions in R. If you need to model a binary variable instead of a quantitative continuous variable, see how to perform a <a href="/blog/binary-logistic-regression-in-r/">binary logistic regression in R</a>.</p>
<p>As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-austin2015number" class="csl-entry">
Austin, Peter C, and Ewout W Steyerberg. 2015. <span>“The Number of Subjects Per Variable Required in Linear Regression Analyses.”</span> <em>Journal of Clinical Epidemiology</em> 68 (6): 627–36.
</div>
<div id="ref-ernst2017regression" class="csl-entry">
Ernst, Anja F, and Casper J Albers. 2017. <span>“Regression Assumptions in Clinical Psychology Research Practice?a Systematic Review of Common Misconceptions.”</span> <em>PeerJ</em> 5: e3323.
</div>
<div id="ref-james2013introduction" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.
</div>
<div id="ref-lumley2002importance" class="csl-entry">
Lumley, Thomas, Paula Diehr, Scott Emerson, and Lu Chen. 2002. <span>“The Importance of the Normality Assumption in Large Public Health Data Sets.”</span> <em>Annual Review of Public Health</em> 23 (1): 151–69.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Some people see regression analysis as a part of inferential statistics. It is true, as a sample is taken to evaluate the link between two or more variables in a population of interest. I tend to distinguish regression from inferential statistics for the simple reasons that (i) regressions are often used to a broader extent (for predictive analyses, among others), and because (ii) the main goal of linear regression (see this <a href="/blog/multiple-linear-regression-made-simple/#simple-linear-regression-reminder">section</a>) differs from the objectives of confidence intervals and hypothesis testing well known in the field of inferential statistics.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Formally, ANOVA can also be used to compare 2 groups, but in practice we tend to use it for 3 or more groups, leaving the t-test for 2 groups.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>More information about the dataset can be found by executing <code>?mtcars</code>.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Note that it best to avoid interpreting the intercept when <span class="math inline">\(X\)</span> cannot be equal to 0 or when it makes no sense in practice.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p><span class="math inline">\(n\)</span> is the number of observations.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Other values than 0 are accepted as well. In that case, the test statistic becomes <span class="math inline">\(T_{n - 2} = \frac{\widehat\beta - a}{se(\widehat\beta_1)}\)</span> where <span class="math inline">\(a\)</span> is the hypothesized slope.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Note that linearity can be checked with a scatterplot of the two variables, or via a scatterplot of the residuals and the fitted values. See more about this in this <a href="/blog/multiple-linear-regression-made-simple/#conditions-of-application-1">section</a>.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>An observation is considered as an outlier based on the Cook’s distance if its value is &gt; 1.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>An observation has a high leverage value (and thus needs to be investigated) if it is greater than <span class="math inline">\(2p/n\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters in the model (intercept included) and <span class="math inline">\(n\)</span> is the number of observations.<a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>You can always change the reference level with the <code>relevel()</code> function. See more <a href="/blog/data-manipulation-in-r/">data manipulation techniques</a>.<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Note that linearity can also be tested with a scatterplot of the residuals and the fitted values.<a href="#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>After installing the <code>{performance}</code> package, you will also need to install the <code>{see}</code> package manually. See <a href="/blog/an-efficient-way-to-install-and-load-r-packages/">how to install a R package</a> if you need more help.<a href="#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>I use the threshold of 10 because, as shown by <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>, a value between 5 and 10 indicates a moderate correlation, while VIF values greater than 10 indicate a high and <em>non-tolerable</em> correlation.<a href="#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p><span class="citation">Austin and Steyerberg (<a href="#ref-austin2015number">2015</a>)</span> showed that two subjects per variable tends to permit accurate estimation of regression coefficients in a linear regression model estimated using ordinary least squares. The general rule of thumb says that there should be at least 10 observations per variable. Our dataset contains 32 observations, above the minimum of 10 subjects per variable.<a href="#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>If you apply a logarithmic transformation, see two guides on how to interpret the results: in <a href="/blog/multiple-linear-regression-made-simple/images/Interpret-Regression-Coefficient-Estimates-in-linear-regression.png">English</a> and in <a href="https://www.parisschoolofeconomics.eu/docs/yin-remi/interpretation-des-coefficients.pdf" target="_blank">French</a>.<a href="#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>Note that a high <span class="math inline">\(R^2\)</span> does not guarantee that you selected the best variables or that your model is good. It simply tells that the model fits the data quite well. It is advised to apply common sense when comparing models and not only refer to <span class="math inline">\(R^2\)</span> (in particular when <span class="math inline">\(R^2\)</span> are close).<a href="#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>There are two main methods; backward and forward. The backward method consists in starting from the model containing all the explanatory variables likely to be relevant, then recursively removing the variable which reduces the information criterion of the model, until no reduction is possible. The forward method is the reverse of the backward method in the sense that we start from a one-variable model with the lowest information criterion and at each step, an explanatory variable is added. By default, the <code>step()</code> function in R combines the backward and forward methods.<a href="#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>To not be confused with the <code>anova()</code> function because it provides results that depend on the order in which the variables appear in the model.<a href="#fnref18" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
