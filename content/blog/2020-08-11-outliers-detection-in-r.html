---
title: Outliers detection in R
author: Antoine Soetewey
date: '2020-08-11'
slug: outliers-detection-in-r
categories: []
tags:
  - R
  - Statistics
meta_img: blog/2020-08-11-outliers-detection-in-r_files/outliers-detection-in-R.jpeg
description: Learn how to detect outliers in R thanks to descriptive statistics and via the Hampel filter, the Grubbs, the Dixon and the Rosner tests for outliers
output:
  blogdown::html_page:
    toc: true
    toc_depth: 6
# draft: true
bibliography: bibliography.bib
---


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#descriptive-statistics" id="toc-descriptive-statistics">Descriptive statistics</a>
<ul>
<li><a href="#minimum-and-maximum" id="toc-minimum-and-maximum">Minimum and maximum</a></li>
<li><a href="#histogram" id="toc-histogram">Histogram</a></li>
<li><a href="#boxplot" id="toc-boxplot">Boxplot</a></li>
<li><a href="#percentiles" id="toc-percentiles">Percentiles</a></li>
<li><a href="#z-scores" id="toc-z-scores">Z-scores</a></li>
</ul></li>
<li><a href="#hampel-filter" id="toc-hampel-filter">Hampel filter</a></li>
<li><a href="#statistical-tests" id="toc-statistical-tests">Statistical tests</a>
<ul>
<li><a href="#grubbss-test" id="toc-grubbss-test">Grubbs’s test</a></li>
<li><a href="#dixons-test" id="toc-dixons-test">Dixon’s test</a></li>
<li><a href="#rosners-test" id="toc-rosners-test">Rosner’s test</a></li>
</ul></li>
<li><a href="#additional-remarks" id="toc-additional-remarks">Additional remarks</a></li>
<li><a href="#conclusion" id="toc-conclusion">Conclusion</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/outliers-detection-in-R.jpeg" style="width:100.0%" /></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>An <strong>outlier</strong> is a value or an <strong>observation that is distant from other observations</strong>, that is to say, a data point that differs significantly from other data points. <span class="citation">Enderlein (<a href="#ref-enderlein1987hawkins">1987</a>)</span> goes even further as the author considers outliers as values that deviate so much from other observations one might suppose a different underlying sampling mechanism.</p>
<p>An observation must always be compared to other observations made on the same phenomenon before actually calling it an outlier. Indeed, someone who is 200 cm tall (6’7” in US) will most likely be considered as an outlier compared to the general population, but that same person may not be considered as an outlier if we measured the height of basketball players.</p>
<p>An outlier may be due to the variability inherent in the observed phenomenon. For example, it is often the case that there are outliers when collecting data on salaries, as some people make much more money than the rest.</p>
<p>Outliers can also arise due to an experimental, measurement or encoding error. For instance, a human weighting 786 kg (1733 pounds) is clearly an error when encoding the weight of the subject. Her or his weight is most probably 78.6 kg (173 pounds) or 7.86 kg (17 pounds) depending on whether weights of adults or babies have been measured.</p>
<p>For this reason, it sometimes makes sense to formally distinguish two classes of outliers: (i) extreme values and (ii) mistakes. Extreme values are statistically and philosophically more interesting, because they are possible but unlikely responses.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>In this article, I present several approaches to detect outliers in R, from simple techniques such as <a href="/blog/descriptive-statistics-in-r/">descriptive statistics</a> (including minimum, maximum, histogram, boxplot and percentiles) to more formal techniques such as the Hampel filter, the Grubbs, the Dixon and the Rosner tests for outliers.</p>
<p>Although there is no strict or unique rule whether outliers should be removed or not from the dataset before doing statistical analyses, it is quite common to, at least, remove or impute outliers that are due to an experimental or measurement error (like the weight of 786 kg (1733 pounds) for a human). Some <a href="/blog/what-statistical-test-should-i-do/">statistical tests</a> require the absence of outliers in order to draw sound conclusions, but removing outliers is not recommended in all cases and must be done with caution.</p>
<p>This article will not tell you whether you should remove outliers or not (nor if you should impute them with the median, mean, mode or any other value), but it will help you to detect them in order to, as a first step, verify them. After their verification, it is then your choice to exclude or include them for your analyses (and this usually requires a thoughtful reflection on the researcher’s side).</p>
<p>Removing or keeping outliers mostly depend on three factors:</p>
<ol style="list-style-type: decimal">
<li>The domain/context of your analyses and the research question. In some domains, it is common to remove outliers as they often occur due to a malfunctioning process. In other fields, outliers are kept because they contain valuable information. It also happens that analyses are performed twice, once with and once without outliers to evaluate their impact on the conclusions. If results change drastically due to some influential values, this should caution the researcher to make overambitious claims.</li>
<li>Whether the tests you are going to apply are robust to the presence of outliers or not. For instance, the slope of a simple <a href="/blog/multiple-linear-regression-made-simple/">linear regression</a> may significantly varies with just one outlier, whereas non-parametric tests such as the <a href="/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/">Wilcoxon test</a> are usually robust to outliers.</li>
<li>How distant are the outliers from other observations? Some observations considered as outliers (according to the techniques presented below) are actually not really extreme compared to all other observations, while other potential outliers may be really distant from the rest of the observations.</li>
</ol>
<p>The dataset <code>mpg</code> from the <code>{ggplot2}</code> package will be used to illustrate the different approaches of outliers detection in R, and in particular we will focus on the variable <code>hwy</code> (highway miles per gallon). We will also use some simulated data for the presentation of the outlier tests.</p>
</div>
<div id="descriptive-statistics" class="section level1">
<h1>Descriptive statistics</h1>
<p>Several methods using descriptive statistics exist. We present the most common ones below.</p>
<div id="minimum-and-maximum" class="section level2">
<h2>Minimum and maximum</h2>
<p>The first step to detect outliers in R is to start with some <a href="/blog/descriptive-statistics-in-r/">descriptive statistics</a>, and in particular with the <a href="/blog/descriptive-statistics-in-r/#minimum-and-maximum">minimum and maximum</a>.</p>
<p>In R, this can easily be done with the <code>summary()</code> function:</p>
<pre class="r"><code>dat &lt;- ggplot2::mpg
summary(dat$hwy)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   12.00   18.00   24.00   23.44   27.00   44.00</code></pre>
<p>where the minimum and maximum are respectively the first and last values in the output above.</p>
<p>Alternatively, they can also be computed with the <code>min()</code> and <code>max()</code>, or <code>range()</code> functions:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<pre class="r"><code>min(dat$hwy)</code></pre>
<pre><code>## [1] 12</code></pre>
<pre class="r"><code>max(dat$hwy)</code></pre>
<pre><code>## [1] 44</code></pre>
<pre class="r"><code>range(dat$hwy)</code></pre>
<pre><code>## [1] 12 44</code></pre>
<p>Some clear encoding mistake like a weight of 786 kg (1733 pounds) for a human will already be easily detected by this very simple technique.</p>
</div>
<div id="histogram" class="section level2">
<h2>Histogram</h2>
<p>Another basic way to detect outliers is to draw a <a href="/blog/descriptive-statistics-in-r/#histogram">histogram</a> of the data.</p>
<p>Using R base (with the number of bins corresponding to the square root of the number of observations in order to have more bins than the default option):</p>
<pre class="r"><code>hist(dat$hwy,
  xlab = &quot;hwy&quot;,
  main = &quot;Histogram of hwy&quot;,
  breaks = sqrt(length(dat$hwy)) # set number of bins
)</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>or using <code>ggplot2</code> (learn how to create plots with this package via the <a href="/blog/rstudio-addins-or-how-to-make-your-coding-life-easier/#esquisse"><code>esquisse</code> addin</a> or via this <a href="/blog/graphics-in-r-with-ggplot2/">tutorial</a>):</p>
<pre class="r"><code>library(ggplot2)

ggplot(dat) +
  aes(x = hwy) +
  geom_histogram(
    bins = round(sqrt(length(dat$hwy))), # set number of bins
    fill = &quot;steelblue&quot;, color = &quot;black&quot;
  ) +
  theme_minimal()</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>From the histograms, we see that there seems to be a couple of observations larger than all other observations (see the bars on the right side of the plot).</p>
</div>
<div id="boxplot" class="section level2">
<h2>Boxplot</h2>
<p>In addition to histograms, <a href="/blog/descriptive-statistics-in-r/#boxplot">boxplots</a> are also useful to detect potential outliers.</p>
<p>Using R base:</p>
<pre class="r"><code>boxplot(dat$hwy,
  ylab = &quot;hwy&quot;
)</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>or using <code>ggplot2</code>:</p>
<pre class="r"><code>ggplot(dat) +
  aes(x = &quot;&quot;, y = hwy) +
  geom_boxplot(fill = &quot;steelblue&quot;) +
  theme_minimal()</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>A boxplot helps to visualize a quantitative variable by displaying five common location summary (minimum, median, first and third quartiles and maximum) and any observation that was classified as a suspected outlier using the <a href="/blog/descriptive-statistics-in-r/#interquartile-range">interquartile range (IQR)</a> criterion.</p>
<p>The IQR criterion means that all observations above <span class="math inline">\(q_{0.75} + 1.5 \cdot IQR\)</span> or below <span class="math inline">\(q_{0.25} - 1.5 \cdot IQR\)</span> (where <span class="math inline">\(q_{0.25}\)</span> and <span class="math inline">\(q_{0.75}\)</span> correspond to first and third quartile respectively, and IQR is the difference between the third and first quartile) are considered as potential outliers by R.</p>
<p>In other words, all observations outside of the following interval will be considered as potential outliers:</p>
<p><span class="math display">\[I = [q_{0.25} - 1.5 \cdot IQR; q_{0.75} + 1.5 \cdot IQR]\]</span></p>
<p>Observations considered as potential outliers by the IQR criterion are displayed as points in the boxplot. Based on this criterion, there are 2 potential outliers (see the 2 points above the vertical line, at the top of the boxplot).</p>
<p>Remember that it is not because an observation is considered as a potential outlier by the IQR criterion that you should remove it. Removing or keeping an outlier depends on:</p>
<ol style="list-style-type: decimal">
<li>the context of your analysis,</li>
<li>whether the tests you are going to perform on the dataset are robust to outliers or not, and</li>
<li>how extreme is the outlier (so how far is the outlier from other observations).</li>
</ol>
<p>It is also possible to extract the values of the potential outliers based on the IQR criterion thanks to the <code>boxplot.stats()$out</code> function:</p>
<pre class="r"><code>boxplot.stats(dat$hwy)$out</code></pre>
<pre><code>## [1] 44 44 41</code></pre>
<p>As you can see, there are actually 3 points considered as potential outliers: 2 observations with a value of 44 and 1 observation with a value of 41.</p>
<p>Thanks to the <code>which()</code> function it is possible to extract the row number corresponding to these outliers:</p>
<pre class="r"><code>out &lt;- boxplot.stats(dat$hwy)$out
out_ind &lt;- which(dat$hwy %in% c(out))
out_ind</code></pre>
<pre><code>## [1] 213 222 223</code></pre>
<p>With this information you can now easily go back to the specific rows in the dataset to verify them, or print all variables for these outliers:</p>
<pre class="r"><code>dat[out_ind, ]</code></pre>
<pre><code>## # A tibble: 3 × 11
##   manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class
##   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;
## 1 volkswagen   jetta        1.9  1999     4 manua… f        33    44 d     comp…
## 2 volkswagen   new beetle   1.9  1999     4 manua… f        35    44 d     subc…
## 3 volkswagen   new beetle   1.9  1999     4 auto(… f        29    41 d     subc…</code></pre>
<p>It is also possible to print the values of the outliers directly on the boxplot with the <code>mtext()</code> function:</p>
<pre class="r"><code>boxplot(dat$hwy,
  ylab = &quot;hwy&quot;,
  main = &quot;Boxplot of highway miles per gallon&quot;
)
mtext(paste(&quot;Outliers: &quot;, paste(out, collapse = &quot;, &quot;)))</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="percentiles" class="section level2">
<h2>Percentiles</h2>
<p>This method of outliers detection is based on the <a href="/blog/descriptive-statistics-by-hand/#a-note-on-deciles-and-percentiles">percentiles</a>.</p>
<p>With the percentiles method, all observations that lie outside the interval formed by the 2.5 and 97.5 percentiles will be considered as potential outliers. Other percentiles such as the 1 and 99, or the 5 and 95 percentiles can also be considered to construct the interval.</p>
<p>The values of the lower and upper percentiles (and thus the lower and upper limits of the interval) can be computed with the <code>quantile()</code> function:</p>
<pre class="r"><code>lower_bound &lt;- quantile(dat$hwy, 0.025)
lower_bound</code></pre>
<pre><code>## 2.5% 
##   14</code></pre>
<pre class="r"><code>upper_bound &lt;- quantile(dat$hwy, 0.975)
upper_bound</code></pre>
<pre><code>##  97.5% 
## 35.175</code></pre>
<p>According to this method, all observations below 14 and above 35.175 will be considered as potential outliers. The row numbers of the observations outside of the interval can then be extracted with the <code>which()</code> function:</p>
<pre class="r"><code>outlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound)
outlier_ind</code></pre>
<pre><code>##  [1]  55  60  66  70 106 107 127 197 213 222 223</code></pre>
<p>Then their values of highway miles per gallon can be printed:</p>
<pre class="r"><code>dat[outlier_ind, &quot;hwy&quot;]</code></pre>
<pre><code>## # A tibble: 11 × 1
##      hwy
##    &lt;int&gt;
##  1    12
##  2    12
##  3    12
##  4    12
##  5    36
##  6    36
##  7    12
##  8    37
##  9    44
## 10    44
## 11    41</code></pre>
<p>Alternatively, all variables for these outliers can be printed:</p>
<pre class="r"><code>dat[outlier_ind, ]</code></pre>
<pre><code>## # A tibble: 11 × 11
##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class
##    &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;
##  1 dodge        dakota pi…   4.7  2008     8 auto… 4         9    12 e     pick…
##  2 dodge        durango 4…   4.7  2008     8 auto… 4         9    12 e     suv  
##  3 dodge        ram 1500 …   4.7  2008     8 auto… 4         9    12 e     pick…
##  4 dodge        ram 1500 …   4.7  2008     8 manu… 4         9    12 e     pick…
##  5 honda        civic        1.8  2008     4 auto… f        25    36 r     subc…
##  6 honda        civic        1.8  2008     4 auto… f        24    36 c     subc…
##  7 jeep         grand che…   4.7  2008     8 auto… 4         9    12 e     suv  
##  8 toyota       corolla      1.8  2008     4 manu… f        28    37 r     comp…
##  9 volkswagen   jetta        1.9  1999     4 manu… f        33    44 d     comp…
## 10 volkswagen   new beetle   1.9  1999     4 manu… f        35    44 d     subc…
## 11 volkswagen   new beetle   1.9  1999     4 auto… f        29    41 d     subc…</code></pre>
<p>There are 11 potential outliers according to the percentiles method. To reduce this number, you can set the percentiles to 1 and 99:</p>
<pre class="r"><code>lower_bound &lt;- quantile(dat$hwy, 0.01)
upper_bound &lt;- quantile(dat$hwy, 0.99)

outlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound)

dat[outlier_ind, ]</code></pre>
<pre><code>## # A tibble: 3 × 11
##   manufacturer model      displ  year   cyl trans  drv     cty   hwy fl    class
##   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;
## 1 volkswagen   jetta        1.9  1999     4 manua… f        33    44 d     comp…
## 2 volkswagen   new beetle   1.9  1999     4 manua… f        35    44 d     subc…
## 3 volkswagen   new beetle   1.9  1999     4 auto(… f        29    41 d     subc…</code></pre>
<p>Setting the percentiles to 1 and 99 gives the same potential outliers as with the IQR criterion.</p>
</div>
<div id="z-scores" class="section level2">
<h2>Z-scores</h2>
<p>If your data come from a <a href="/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/">normal distribution</a>, you can use the z-scores, defined as</p>
<p><span class="math display">\[z_i = \frac{x_i - \overline{X}}{s_X}\]</span></p>
<p>where <span class="math inline">\(\overline{X}\)</span> is the mean and <span class="math inline">\(s_X\)</span> is the standard deviation of the random variable <span class="math inline">\(X\)</span>.</p>
<p>This is referred as scaling, which can be done with the <code>scale()</code> function in R.</p>
<p>According to this method, any z-score:</p>
<ul>
<li>&lt; -2 or &gt; 2 is considered as rare</li>
<li>&lt; -3 or &gt; 3 is considered as extremely rare</li>
</ul>
<p>Other authors also use a z-score below -3.29 or above 3.29 to detect outliers. This value of 3.29 comes from the fact that 1 observation out of 1000 is out of this interval if the data follow a normal distribution.</p>
<p>In our case:</p>
<pre class="r"><code>dat$z_hwy &lt;- scale(dat$hwy)

hist(dat$z_hwy)</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>summary(dat$z_hwy)</code></pre>
<pre><code>##        V1          
##  Min.   :-1.92122  
##  1st Qu.:-0.91360  
##  Median : 0.09402  
##  Mean   : 0.00000  
##  3rd Qu.: 0.59782  
##  Max.   : 3.45274</code></pre>
<p>We see that there are some observations above 3.29, but none below -3.29.</p>
<p>To identify the line in the dataset of these observations:</p>
<pre class="r"><code>which(dat$z_hwy &gt; 3.29)</code></pre>
<pre><code>## [1] 213 222</code></pre>
<p>We see that observations 213 and 222 can be considered as outliers according to this method.</p>
</div>
</div>
<div id="hampel-filter" class="section level1">
<h1>Hampel filter</h1>
<p>Another method, known as Hampel filter, consists of considering as outliers the values outside the interval (<span class="math inline">\(I\)</span>) formed by the median, plus or minus 3 median absolute deviations (<span class="math inline">\(MAD\)</span>):<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p><span class="math display">\[I = [median - 3 \cdot MAD; median + 3 \cdot MAD]\]</span></p>
<p>where <span class="math inline">\(MAD\)</span> is the median absolute deviation and is defined as the median of the absolute deviations from the data’s median <span class="math inline">\(\tilde{X} = median(X)\)</span>:</p>
<p><span class="math display">\[MAD = median(|X_i - \tilde{X}|)\]</span></p>
<p>For this method we first set the interval limits thanks to the <code>median()</code> and <code>mad()</code> functions:<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<pre class="r"><code>lower_bound &lt;- median(dat$hwy) - 3 * mad(dat$hwy, constant = 1)
lower_bound</code></pre>
<pre><code>## [1] 9</code></pre>
<pre class="r"><code>upper_bound &lt;- median(dat$hwy) + 3 * mad(dat$hwy, constant = 1)
upper_bound</code></pre>
<pre><code>## [1] 39</code></pre>
<p>According to this method, all observations below 9 and above 39 will be considered as potential outliers. The row numbers of the observations outside of the interval can then be extracted with the <code>which()</code> function:</p>
<pre class="r"><code>outlier_ind &lt;- which(dat$hwy &lt; lower_bound | dat$hwy &gt; upper_bound)
outlier_ind</code></pre>
<pre><code>## [1] 213 222 223</code></pre>
<p>According to the Hampel filter, there are 3 outliers for the <code>hwy</code> variable.</p>
</div>
<div id="statistical-tests" class="section level1">
<h1>Statistical tests</h1>
<p>In this section, we present 3 <a href="/blog/hypothesis-test-by-hand/">hypothesis tests</a> to detect outliers:</p>
<ol style="list-style-type: decimal">
<li>Grubbs’s test</li>
<li>Dixon’s test</li>
<li>Rosner’s test</li>
</ol>
<p>These 3 statistical tests are part of more formal techniques of outliers detection as they all involve the computation of a test statistic that is compared to tabulated critical values (that are based on the sample size and the desired confidence level).</p>
<p>Note that the 3 tests are appropriate only when the data, without any outliers, are <strong>approximately normally distributed</strong>. It is recommended to check normality visually, with a <a href="/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/#qq-plot">QQ-plot</a> and/or a histogram for instance. Although it can also be checked with a formal test for normality (such as the <a href="/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/#normality-test">Shapiro-Wilk test</a> for example), the presence of one or more outliers may cause the normality test to reject normality when it is in fact a reasonable assumption for applying one of the 3 outlier tests mentioned above.</p>
<p>We check the normality of our data thanks to a QQ-plot:</p>
<pre class="r"><code>library(car)
qqPlot(dat$hwy)</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-20-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre><code>## [1] 213 222</code></pre>
<p>Too many points deviate from the Henry’s line, so based on the QQ-plot we conclude that our data do not follow a normal distribution. Therefore, we should not use one of the outlier test on these data.</p>
<p>For the sake of completeness, here is an example of data that could be used with the three outlier tests, together with the QQ-plot of these data:</p>
<pre class="r"><code>dat_tests &lt;- c(rnorm(50), 5)

qqPlot(dat_tests)</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-21-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre><code>## [1] 51 18</code></pre>
<pre class="r"><code>hist(dat_tests)</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-21-2.png" width="100%" style="display: block; margin: auto;" /></p>
<p>For the illustration of the three outlier tests in the next sections, we thus use the simulated data (<code>dat_tests</code>) instead of the data used so far (<code>dat$hwy</code>).</p>
<div id="grubbss-test" class="section level2">
<h2>Grubbs’s test</h2>
<p>The Grubbs test allows to detect whether the highest or lowest value in a dataset is an outlier.</p>
<p>The Grubbs test detects one outlier at a time (highest or lowest value), so the null and alternative hypotheses are as follows:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The <em>highest</em> value is <strong>not</strong> an outlier</li>
<li><span class="math inline">\(H_1\)</span>: The <em>highest</em> value is an outlier</li>
</ul>
<p>if we want to test the highest value, or:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The <em>lowest</em> value is <strong>not</strong> an outlier</li>
<li><span class="math inline">\(H_1\)</span>: The <em>lowest</em> value is an outlier</li>
</ul>
<p>if we want to test the lowest value.</p>
<p>As for any <a href="/blog/what-statistical-test-should-i-do/">statistical test</a>, if the <strong><a href="/blog/student-s-t-test-in-r-and-by-hand-how-to-compare-two-groups-under-different-scenarios/#a-note-on-p-value-and-significance-level-alpha"><em>p</em>-value</a> is less</strong> than the chosen <strong>significance threshold</strong> (generally <span class="math inline">\(\alpha = 0.05\)</span>) then the null hypothesis is rejected and we will conclude that the <strong>lowest/highest value is an outlier</strong>.</p>
<p>On the contrary, if the <strong><em>p</em>-value is greater or equal</strong> than the significance level, the null hypothesis is not rejected, and we will conclude that, based on the data, we do not reject the hypothesis that the <strong>lowest/highest value is not an outlier</strong>.</p>
<p>Note that the Grubbs test is not appropriate for sample size of 6 or less (<span class="math inline">\(n \le 6\)</span>).</p>
<p>To perform the Grubbs test in R, we use the <code>grubbs.test()</code> function from the <code>{outliers}</code> package:</p>
<pre class="r"><code># install.packages(&quot;outliers&quot;)
library(outliers)

# Grubbs test
test &lt;- grubbs.test(dat_tests)
test</code></pre>
<pre><code>## 
## 	Grubbs test for one outlier
## 
## data:  dat_tests
## G = 3.68326, U = 0.72325, p-value = 0.001873
## alternative hypothesis: highest value 5 is an outlier</code></pre>
<p>The <em>p</em>-value is 0.002. At the 5% significance level, we reject the hypothesis that the <em>highest</em> value 5 is <strong>not</strong> an outlier. In other words, based on this test, we conclude that the highest value 5 is an outlier.</p>
<p>By default, the test is performed on the highest value (as shown in the R output: <code>alternative hypothesis: highest value 5 is an outlier</code>). If you want to do the test for the lowest value, simply add the argument <code>opposite = TRUE</code> in the <code>grubbs.test()</code> function:</p>
<pre class="r"><code>test &lt;- grubbs.test(dat_tests, opposite = TRUE)
test</code></pre>
<pre><code>## 
## 	Grubbs test for one outlier
## 
## data:  dat_tests
## G = 2.02893, U = 0.91602, p-value = 0.9981
## alternative hypothesis: lowest value -2.65645542090478 is an outlier</code></pre>
<p>The R output indicates that the test is now performed on the lowest value (see <code>alternative hypothesis: lowest value -2.6564554 is an outlier</code>).</p>
<p>The <em>p</em>-value is 0.998. At the 5% significance level, we do not reject the hypothesis that the <em>lowest</em> value -2.66 is <strong>not</strong> an outlier. In other words, we cannot conlude that the lowest value -2.66 is an outlier.</p>
</div>
<div id="dixons-test" class="section level2">
<h2>Dixon’s test</h2>
<p>Similar to the Grubbs test, Dixon test is used to test whether a single low or high value is an outlier. So if more than one outliers is suspected, the test has to be performed on these suspected outliers individually.</p>
<p>Note that Dixon test is most useful for small sample size (usually <span class="math inline">\(n \le 25\)</span>).</p>
<p>To perform the Dixon’s test in R, we use the <code>dixon.test()</code> function from the <code>{outliers}</code> package.</p>
<p>For this illustration, as the Dixon test can only be done on small samples, we take a <a href="/blog/data-manipulation-in-r/#subset-a-data-frame">subset</a> of our simulated data which consists of the 20 first observations and the outlier. Note that R will throw an <a href="/blog/top-10-errors-in-r/">error</a> and accepts only a dataset consisting of 3 to 30 observations for this test.</p>
<pre class="r"><code># subset of simulated data
subdat &lt;- c(dat_tests[1:20], max(dat_tests))

# Dixon test
test &lt;- dixon.test(subdat)
test</code></pre>
<pre><code>## 
## 	Dixon test for outliers
## 
## data:  subdat
## Q = 0.46668, p-value = 0.06429
## alternative hypothesis: highest value 5 is an outlier</code></pre>
<p>Results of the Dixon test show that we cannot conclude that the highest value 5 is an outlier (<em>p</em>-value = 0.064).</p>
<p>To test for the lowest value, simply add the <code>opposite = TRUE</code> argument to the <code>dixon.test()</code> function:</p>
<pre class="r"><code>test &lt;- dixon.test(subdat,
  opposite = TRUE
)
test</code></pre>
<pre><code>## 
## 	Dixon test for outliers
## 
## data:  subdat
## Q = 0.27115, p-value = 0.6872
## alternative hypothesis: lowest value -2.65645542090478 is an outlier</code></pre>
<p>Results of the test show that we cannot conclude that the lowest value -2.66 is an outlier (<em>p</em>-value = 0.687).</p>
<p>It is a good practice to always check the results of the statistical test for outliers against the boxplot to make sure we tested <strong>all</strong> potential outliers:</p>
<pre class="r"><code>out &lt;- boxplot.stats(subdat)$out

boxplot(subdat)
mtext(paste(&quot;Outlier: &quot;, paste(out, collapse = &quot;, &quot;)))</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-26-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>From the boxplot, we see that the Dixon test has been applied to all potential outliers.</p>
<p>If you need to perform the test again without the highest or lowest value, this can be done by finding the row number of the maximum or minimum value, excluding this row number from the dataset and then finally apply the Dixon test on this new dataset. We illustrate the process as if we needed to perform the test again, but this time on the data excluding the highest value:</p>
<pre class="r"><code># find and exclude highest value
remove_ind &lt;- which.max(subdat)
subsubdat &lt;- subdat[-remove_ind]

# Dixon test on dataset without the maximum
test &lt;- dixon.test(subsubdat)
test</code></pre>
<pre><code>## 
## 	Dixon test for outliers
## 
## data:  subsubdat
## Q = 0.30413, p-value = 0.5547
## alternative hypothesis: lowest value -2.65645542090478 is an outlier</code></pre>
<p>Results show that we cannot conclude that the lowest value -2.66 is an outlier (<em>p</em>-value = 0.555).</p>
</div>
<div id="rosners-test" class="section level2">
<h2>Rosner’s test</h2>
<p>Rosner’s test for outliers has the advantages that:</p>
<ol style="list-style-type: decimal">
<li>it is used to <strong>detect several outliers at once</strong> (unlike Grubbs and Dixon test which must be performed iteratively to screen for multiple outliers), and</li>
<li>it is designed to avoid the problem of masking, where an outlier that is close in value to another outlier can go undetected.</li>
</ol>
<p>Unlike Dixon test, note that Rosner test is most appropriate when the sample size is large (<span class="math inline">\(n \ge 20\)</span>). We therefore use again the similated dataset (i.e., <code>dat_tests</code>), which includes 51 observations.</p>
<p>To perform the Rosner test, we use the <code>rosnerTest()</code> function from the <code>{EnvStats}</code> package. This function requires at least 2 arguments: the data and the number of suspected outliers <code>k</code> (with <code>k = 3</code> as the default number of suspected outliers).</p>
<p>For this example, we set the number of suspected outliers to be equal to 1, as suggested by the number of potential outliers outlined in the boxplot.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<pre class="r"><code>library(EnvStats)

# Rosner test
test &lt;- rosnerTest(dat_tests,
  k = 1
)
test</code></pre>
<pre><code>## 
## Results of Outlier Test
## -------------------------
## 
## Test Method:                     Rosner&#39;s Test for Outliers
## 
## Hypothesized Distribution:       Normal
## 
## Data:                            dat_tests
## 
## Sample Size:                     51
## 
## Test Statistic:                  R.1 = 3.683258
## 
## Test Statistic Parameter:        k = 1
## 
## Alternative Hypothesis:          Up to 1 observations are not
##                                  from the same Distribution.
## 
## Type I Error:                    5%
## 
## Number of Outliers Detected:     1
## 
##   i     Mean.i     SD.i Value Obs.Num    R.i+1 lambda.i+1 Outlier
## 1 0 0.06306688 1.340371     5      51 3.683258   3.136165    TRUE</code></pre>
<p>The interesting results are provided in the <code>$all.stats</code> table:</p>
<pre class="r"><code>test$all.stats</code></pre>
<pre><code>##   i     Mean.i     SD.i Value Obs.Num    R.i+1 lambda.i+1 Outlier
## 1 0 0.06306688 1.340371     5      51 3.683258   3.136165    TRUE</code></pre>
<p>Based on the results of the Rosner test, we see that there is only one outlier (as the table contains 1 line), and that it is the observation 51 (see <code>Obs.Num</code>) with a value of 5 (see <code>Value</code>). This finding aligns with the Grubbs test presented above, which also detected the value 5 as an outlier.</p>
</div>
</div>
<div id="additional-remarks" class="section level1">
<h1>Additional remarks</h1>
<p>You will find many other methods to detect outliers:</p>
<ol style="list-style-type: decimal">
<li>in the <code>{outliers}</code> packages,</li>
<li>via the <code>lofactor()</code> function from the <code>{DMwR}</code> package: Local Outlier Factor (LOF) is an algorithm used to identify outliers by comparing the local density of a point with that of its neighbors,</li>
<li>the <code>outlierTest()</code> from the <code>{car}</code> package gives the most extreme observation based on the given model and allows to test whether it is an outlier,</li>
<li>in the <code>{OutlierDetection}</code> package, and</li>
<li>with the <code>aq.plot()</code> function from the <code>{mvoutlier}</code> package (Thanks KTR for the suggestion.).</li>
</ol>
<p>We present these methods with the initial dataset <code>mpg</code>, using the <code>cyl</code> and <code>hwy</code> variables:</p>
<pre class="r"><code>library(mvoutlier)

Y &lt;- as.matrix(ggplot2::mpg[, c(&quot;cyl&quot;, &quot;hwy&quot;)])
res &lt;- aq.plot(Y)</code></pre>
<p><img src="/blog/2020-08-11-outliers-detection-in-r_files/figure-html/unnamed-chunk-30-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note also that some transformations may “naturally” eliminate outliers. The natural log or square root of a value reduces the variation caused by extreme values, so in some cases applying these transformations will help in making potential outliers less extreme.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Thanks for reading.</p>
<p>I hope this article helped you to detect outliers in R via several descriptive statistics (including minimum, maximum, histogram, boxplot and percentiles) or thanks to more formal techniques of outliers detection (including Hampel filter, Grubbs, Dixon and Rosner tests).</p>
<p>It is now your turn to try to detect outliers in your data, and decide how to treat them (i.e., keeping, removing or imputing them) before conducting your analyses.</p>
<p>As always, if you have a question or a suggestion related to the topic covered in this article, please add it as a comment so other readers can benefit from the discussion.</p>
<p><em>(Note that this article is available for download on my <a href="https://statsandr.gumroad.com/">Gumroad page</a>.)</em></p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-enderlein1987hawkins" class="csl-entry">
Enderlein, G. 1987. <span>“Hawkins, DM: Identification of Outliers. Chapman and Hall, London–New York 1980, 188 s.,<span>£</span> 14, 50.”</span> <em>Biometrical Journal</em> 29 (2): 198–98.
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>Thanks Felix Kluxen for the valuable suggestion.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Thanks to Victor for pointing out the <code>range()</code> function.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>The default is 3 (according to Pearson’s rule), but another value is also possible.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>The constant in the <code>mad()</code> function is 1.4826 by default, so it has to be set to 1 to find the median absolute deviation. See <code>help(mad)</code> for more details. Thanks to Elisei for pointing this out to me.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>In order to avoid flawed conclusions, it is important to pre-screen the data (graphically with a boxplot for example) to make the selection of the number of potential outliers as accurate as possible prior to running Rosner’s test.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
